# LangChain + Dify ä¼ä¸šçº§çŸ¥è¯†ä¸­å°æ¶æ„è®¾è®¡æ–‡æ¡£

## ğŸ—ï¸ ç³»ç»Ÿæ¶æ„æ¦‚è¿°

### æ•´ä½“æ¶æ„å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          æ•°æ®ä¸­å°å‰ç«¯ï¼ˆç®¡ç†ç•Œé¢ï¼‰              â”‚
â”‚  â€¢ å¤šçŸ¥è¯†åº“åˆ‡æ¢ä¸ç®¡ç†                         â”‚
â”‚  â€¢ æ–‡æ¡£æ‰¹é‡ä¸Šä¼                                â”‚
â”‚  â€¢ å‘é‡æ•°æ®ç›‘æ§                               â”‚
â”‚  â€¢ æ£€ç´¢æ•ˆæœæµ‹è¯•                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚ HTTP API
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          æ•°æ®ä¸­å°åç«¯ï¼ˆFastAPIï¼‰              â”‚
â”‚  â€¢ çŸ¥è¯†åº“ CRUD æ¥å£                          â”‚
â”‚  â€¢ æ–‡æ¡£ä¸Šä¼ ä¸å¤„ç†è°ƒåº¦                         â”‚
â”‚  â€¢ ç»Ÿä¸€æ£€ç´¢æ¥å£ï¼ˆä¾› Dify è°ƒç”¨ï¼‰               â”‚
â”‚  â€¢ ä»»åŠ¡é˜Ÿåˆ—ä¸çŠ¶æ€ç®¡ç†                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          LangChain å¤„ç†å±‚                    â”‚
â”‚  â€¢ Unstructured å¤šæ ¼å¼æ–‡æ¡£è§£æ                â”‚
â”‚  â€¢ RecursiveCharacterTextSplitter æ™ºèƒ½åˆ†å—   â”‚
â”‚  â€¢ Embedding å‘é‡åŒ–ï¼ˆOpenAI/æœ¬åœ°æ¨¡å‹ï¼‰        â”‚
â”‚  â€¢ Metadata è‡ªåŠ¨æå–ä¸ç®¡ç†                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                    â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       å‘é‡æ•°æ®åº“ï¼ˆæŒä¹…åŒ–å­˜å‚¨ï¼‰                 â”‚
â”‚  â€¢ å°è§„æ¨¡ï¼šChromaDB (PersistentClient)       â”‚
â”‚  â€¢ å¤§è§„æ¨¡ï¼šMilvus / Qdrant                   â”‚
â”‚  â€¢ æŒ‰çŸ¥è¯†åº“éš”ç¦»ï¼š./dbs/{kb_name}/            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

           â†‘ æ£€ç´¢ API
           â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       Dify        â”‚
â”‚  â€¢ é€šè¿‡ API å·¥å…·   â”‚
â”‚    è°ƒç”¨æ•°æ®ä¸­å°     â”‚
â”‚  â€¢ æ•´åˆæ£€ç´¢ç»“æœ     â”‚
â”‚  â€¢ ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### æ ¸å¿ƒè®¾è®¡åŸåˆ™

1. **çŸ¥è¯†åº“éš”ç¦»**ï¼šæ¯ä¸ªé¡¹ç›®/ç³»ç»Ÿç‹¬ç«‹çŸ¥è¯†åº“ï¼Œäº’ä¸å¹²æ‰°
2. **æ•°æ®æŒä¹…åŒ–**ï¼šå‘é‡æ•°æ®å­˜å‚¨åœ¨ç£ç›˜ï¼Œæ”¯æŒå¤‡ä»½ä¸è¿ç§»
3. **è§£è€¦æ¶æ„**ï¼šæ•°æ®ä¸­å°ä½œä¸ºä¸­é—´å±‚ï¼ŒDify å’Œ LangChain é€šè¿‡æ ‡å‡† API é€šä¿¡
4. **å¯æ‰©å±•æ€§**ï¼šæ”¯æŒä»å•æœºåˆ°åˆ†å¸ƒå¼çš„å¹³æ»‘å‡çº§

------

## ğŸ§© æ”¯æŒçš„æ–‡æ¡£ç±»å‹

LangChain + Unstructured æ”¯æŒä¸»æµåŠå…¬æ ¼å¼ï¼š

| æ–‡æ¡£ç±»å‹        | æ”¯æŒç¨‹åº¦             | è¯´æ˜                         |
| --------------- | -------------------- | ---------------------------- |
| .txt            | âœ… å®Œå…¨æ”¯æŒ           | æœ€æ¨èï¼Œè§£æç®€å•ç¨³å®š         |
| .mdï¼ˆMarkdownï¼‰ | âœ… å®Œå…¨æ”¯æŒ           | èƒ½ä¿ç•™æ ‡é¢˜å±‚çº§ç»“æ„           |
| .doc / .docx    | âœ… å®Œå…¨æ”¯æŒ           | Word æ–‡æ¡£å¯ä»¥è§£ææ®µè½å’Œæ ‡é¢˜  |
| .pdf            | âœ… æ”¯æŒï¼ˆå»ºè®®æ–‡å­—ç‰ˆï¼‰ | å›¾ç‰‡å‹ PDF éœ€ OCR æ‰èƒ½è¯†åˆ«   |
| .html / .htm    | âœ… å®Œå…¨æ”¯æŒ           | é€‚åˆå­˜ç½‘é¡µå†…å®¹               |
| .pptx           | âœ… æ”¯æŒ               | ä¼šæå–å¹»ç¯ç‰‡æ–‡å­—å†…å®¹         |
| .csv / .xlsx    | âš ï¸ å¯é€‰æ”¯æŒ           | å†…å®¹æ˜¯ç»“æ„åŒ–è¡¨æ ¼ï¼Œè¯­ä¹‰ä»·å€¼ä½ |
| .png / .jpg     | âš ï¸ éœ€ OCR             | éœ€å®‰è£… Tesseract ç­‰ä¾èµ–      |

### æ¨èçš„æ–‡æ¡£ç±»å‹ä½¿ç”¨åœºæ™¯

| æ–‡æ¡£åœºæ™¯ | æ¨èæ ¼å¼    | åŸå›                        |
| -------- | ----------- | -------------------------- |
| éœ€æ±‚æ–‡æ¡£ | .docx, .md  | ç« èŠ‚ç»“æ„æ¸…æ™°ï¼Œæ˜“äºåˆ†å—     |
| è®¾è®¡æ–‡æ¡£ | .docx, .pdf | æ¨¡å—åŒ–æ˜æ˜¾ï¼Œchunk æ•ˆæœæœ€å¥½ |
| è¿ç»´æ–‡æ¡£ | .md, .txt   | æ“ä½œæ­¥éª¤æ˜ç¡®ï¼Œé€‚åˆæ£€ç´¢     |
| æŠ€æœ¯æ‰‹å†Œ | .pdf, .html | å†…å®¹å®Œæ•´ï¼Œé€‚åˆé•¿æœŸå½’æ¡£     |

------

## ğŸ§  æ–‡æ¡£å†…å®¹æ ¼å¼è¦æ±‚

çŸ¥è¯†ä¸­å°çš„è¯­ä¹‰åŒ¹é…ä¾èµ– embeddingï¼Œembedding åªèƒ½ç†è§£æ–‡å­—å†…å®¹ã€‚

### æ–‡æ¡£ç¼–å†™æœ€ä½³å®è·µ

| å»ºè®®é¡¹             | åŸå›                                        | ç¤ºä¾‹                                          |
| ------------------ | ------------------------------------------ | --------------------------------------------- |
| ä½¿ç”¨æ¸…æ™°çš„ç« èŠ‚å±‚çº§ | chunk åˆ†æ®µæ—¶å¯ä»¥æŒ‰ç« èŠ‚åˆ‡ï¼Œæå‡è¯­ä¹‰ç‹¬ç«‹æ€§   | `1. æ¦‚è¿° / 1.1 æ¨¡å—åŠŸèƒ½ / 1.1.1 ç™»å½•æ¨¡å—`     |
| æ§åˆ¶æ®µè½é•¿åº¦       | æ¯æ®µ 5~10 è¡Œï¼Œå¤ªé•¿éš¾ä»¥åˆ†å—ï¼Œå¤ªçŸ­è¯­ä¹‰ä¸å®Œæ•´ | ä¸€ä¸ªåŠŸèƒ½ç‚¹ä¸€æ®µï¼Œä¸è¶…è¿‡ 200 å­—                 |
| ç»Ÿä¸€ä¸­è‹±æ–‡ç¬¦å·     | é¿å…åˆ†å¥é”™è¯¯                               | ç»Ÿä¸€ä½¿ç”¨ä¸­æ–‡æ ‡ç‚¹æˆ–è‹±æ–‡æ ‡ç‚¹                    |
| è¡¨æ ¼é…æ–‡å­—è¯´æ˜     | embedding å¯èƒ½è¯†åˆ«ä¸åˆ°è¡¨æ ¼å«ä¹‰             | "ä¸‹è¡¨å±•ç¤ºäº†ä¸‰ç§è®¤è¯æ–¹å¼çš„å¯¹æ¯”ï¼š..."           |
| å›¾ç‰‡é…æ–‡å­—æè¿°     | å›¾ç‰‡ä¿¡æ¯ä¸ä¼šè¢«ç›´æ¥è¯†åˆ«è¿›å‘é‡               | "æ¶æ„å›¾å¦‚ä¸‹ï¼ˆåŒ…å«å‰ç«¯ã€ç½‘å…³ã€åç«¯ä¸‰å±‚ï¼‰ï¼š..." |
| æ–‡æ¡£å…ƒä¿¡æ¯å®Œæ•´     | è‡ªåŠ¨æå– metadataï¼Œæ–¹ä¾¿ç‰ˆæœ¬ç®¡ç†            | æ–‡æ¡£å¼€å¤´å†™æ˜ï¼šç³»ç»Ÿåã€ç‰ˆæœ¬å·ã€ä½œè€…ã€æ—¥æœŸ      |

### Chunk åˆ†å‰²ç­–ç•¥

æ¯ä¸ª chunk æœ€å¥½æ§åˆ¶åœ¨ï¼š**500 ~ 1500 å­—**ï¼ˆæˆ– 200 ~ 600 tokensï¼‰

**ç¤ºä¾‹**ï¼šä¸€ä»½ã€ŠCRM ç³»ç»Ÿè®¾è®¡æ–‡æ¡£ã€‹

```
1. ç³»ç»Ÿæ¦‚è¿°                    â† chunk 1 (800 å­—)
2. åŠŸèƒ½æ¨¡å—è®¾è®¡  
   - 2.1 ç™»å½•æ¨¡å—              â† chunk 2 (600 å­—)
   - 2.2 å®¢æˆ·ç®¡ç†              â† chunk 3 (1200 å­—)
   - 2.3 æŠ¥è¡¨åˆ†æ              â† chunk 4 (900 å­—)
3. æ¥å£è¯´æ˜                    â† chunk 5 (1100 å­—)
4. éƒ¨ç½²æ–¹æ¡ˆ                    â† chunk 6 (700 å­—)
```

------

## ğŸ—‚ï¸ å¤šçŸ¥è¯†åº“è®¾è®¡

### ç›®å½•ç»“æ„è®¾è®¡

```
knowledge_platform/
â”œâ”€â”€ knowledge_bases/              # çŸ¥è¯†åº“æ ¹ç›®å½•
â”‚   â”œâ”€â”€ crm_system/              # CRM ç³»ç»ŸçŸ¥è¯†åº“
â”‚   â”‚   â”œâ”€â”€ docs/                # åŸå§‹æ–‡æ¡£å­˜æ”¾
â”‚   â”‚   â”‚   â”œâ”€â”€ requirements/
â”‚   â”‚   â”‚   â”œâ”€â”€ design/
â”‚   â”‚   â”‚   â””â”€â”€ operations/
â”‚   â”‚   â”œâ”€â”€ db/                  # å‘é‡æ•°æ®åº“ï¼ˆæŒä¹…åŒ–ï¼‰
â”‚   â”‚   â””â”€â”€ config.json          # çŸ¥è¯†åº“é…ç½®
â”‚   â”œâ”€â”€ erp_system/
â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â”œâ”€â”€ db/
â”‚   â”‚   â””â”€â”€ config.json
â”‚   â””â”€â”€ ops_manual/
â”‚       â”œâ”€â”€ docs/
â”‚       â”œâ”€â”€ db/
â”‚       â””â”€â”€ config.json
â”œâ”€â”€ data_platform/               # æ•°æ®ä¸­å°ä»£ç 
â”‚   â”œâ”€â”€ backend/                 # FastAPI åç«¯
â”‚   â”œâ”€â”€ frontend/                # React/Vue å‰ç«¯
â”‚   â””â”€â”€ config/                  # å…¨å±€é…ç½®
â””â”€â”€ logs/                        # æ—¥å¿—ç›®å½•
```

### çŸ¥è¯†åº“é…ç½®æ–‡ä»¶ç¤ºä¾‹

**æ–‡ä»¶è·¯å¾„**ï¼š`knowledge_bases/crm_system/config.json`

```json
{
  "kb_id": "crm_system",
  "kb_name": "CRMç³»ç»ŸçŸ¥è¯†åº“",
  "description": "åŒ…å«CRMç³»ç»Ÿçš„éœ€æ±‚ã€è®¾è®¡ã€è¿ç»´æ–‡æ¡£",
  "created_at": "2025-10-30",
  "embedding_model": "text-embedding-3-small",
  "chunk_size": 1000,
  "chunk_overlap": 200,
  "db_type": "chromadb",
  "db_path": "./knowledge_bases/crm_system/db",
  "allowed_doc_types": [".docx", ".pdf", ".md", ".txt"],
  "auto_extract_metadata": true,
  "metadata_fields": ["system", "doc_type", "version", "module"]
}
```

### å¤šçŸ¥è¯†åº“ç®¡ç† API

```python
# åˆ›å»ºçŸ¥è¯†åº“
POST /api/kb/create
{
  "kb_id": "erp_system",
  "kb_name": "ERPç³»ç»ŸçŸ¥è¯†åº“",
  "embedding_model": "text-embedding-3-small"
}

# åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“
GET /api/kb/list
# è¿”å›ï¼š
[
  {"kb_id": "crm_system", "kb_name": "CRMç³»ç»ŸçŸ¥è¯†åº“", "doc_count": 15},
  {"kb_id": "erp_system", "kb_name": "ERPç³»ç»ŸçŸ¥è¯†åº“", "doc_count": 8}
]

# åˆ é™¤çŸ¥è¯†åº“ï¼ˆè°¨æ…æ“ä½œï¼‰
DELETE /api/kb/{kb_id}
```

------

## ğŸ’¾ å‘é‡æ•°æ®åº“æŒä¹…åŒ–æ–¹æ¡ˆ

### ChromaDB æŒä¹…åŒ–é…ç½®ï¼ˆæ¨èå°è§„æ¨¡ä½¿ç”¨ï¼‰

```python
import chromadb
from chromadb.config import Settings

# âŒ é”™è¯¯ï¼šçº¯å†…å­˜æ¨¡å¼ï¼ˆé‡å¯ä¸¢å¤±ï¼‰
client = chromadb.Client()

# âœ… æ­£ç¡®ï¼šæŒä¹…åŒ–åˆ°ç£ç›˜
client = chromadb.PersistentClient(
    path="./knowledge_bases/crm_system/db",
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True
    )
)

# åˆ›å»ºæˆ–è·å– collection
collection = client.get_or_create_collection(
    name="crm_documents",
    metadata={"kb_id": "crm_system"}
)
```

### æ•°æ®åº“é€‰å‹å»ºè®®

| æ•°æ®è§„æ¨¡      | æ¨èæ–¹æ¡ˆ | éƒ¨ç½²æ–¹å¼     | ç‰¹ç‚¹             |
| ------------- | -------- | ------------ | ---------------- |
| < 10 ä¸‡æ¡     | ChromaDB | å•æœºæ–‡ä»¶å­˜å‚¨ | è½»é‡çº§ï¼Œé›¶é…ç½®   |
| 10 ~ 100 ä¸‡æ¡ | Qdrant   | Docker å®¹å™¨  | é«˜æ€§èƒ½ï¼Œæ”¯æŒè¿‡æ»¤ |
| > 100 ä¸‡æ¡    | Milvus   | K8s é›†ç¾¤     | åˆ†å¸ƒå¼ï¼Œä¼ä¸šçº§   |

### æ•°æ®åº“ç»´æŠ¤ç­–ç•¥

#### 1. å®šæœŸå¤‡ä»½

```bash
#!/bin/bash
# backup_dbs.sh

DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_DIR="./backups/chroma_$DATE"

# å¤‡ä»½æ‰€æœ‰çŸ¥è¯†åº“
mkdir -p $BACKUP_DIR
cp -r ./knowledge_bases/*/db $BACKUP_DIR/

# å‹ç¼©
tar -czf $BACKUP_DIR.tar.gz $BACKUP_DIR
rm -rf $BACKUP_DIR

echo "Backup completed: $BACKUP_DIR.tar.gz"
```

#### 2. ç‰ˆæœ¬æ¸…ç†è„šæœ¬

```python
def clean_old_versions(kb_id: str, doc_name: str, keep_latest: int = 3):
    """
    æ¸…ç†åŒä¸€æ–‡æ¡£çš„æ—§ç‰ˆæœ¬ï¼Œä¿ç•™æœ€æ–°çš„ N ä¸ªç‰ˆæœ¬
    """
    collection = get_collection(kb_id)
    
    # æŸ¥è¯¢è¯¥æ–‡æ¡£çš„æ‰€æœ‰ç‰ˆæœ¬
    results = collection.get(
        where={"doc_name": doc_name},
        include=["metadatas"]
    )
    
    # æŒ‰ç‰ˆæœ¬å·æ’åº
    versions = sorted(results['metadatas'], 
                     key=lambda x: x.get('version', '0.0'), 
                     reverse=True)
    
    # åˆ é™¤æ—§ç‰ˆæœ¬
    old_versions = versions[keep_latest:]
    for v in old_versions:
        collection.delete(where={"version": v['version'], "doc_name": doc_name})
    
    print(f"Cleaned {len(old_versions)} old versions of {doc_name}")
```

#### 3. æ•°æ®è¿ç§»

```python
# è¿ç§»åˆ°æ–°ç¯å¢ƒ
def migrate_kb(source_path: str, target_path: str, kb_id: str):
    """
    è¿ç§»çŸ¥è¯†åº“åˆ°æ–°ç¯å¢ƒ
    """
    import shutil
    
    source_db = f"{source_path}/{kb_id}/db"
    target_db = f"{target_path}/{kb_id}/db"
    
    # å¤åˆ¶æ•°æ®åº“æ–‡ä»¶
    shutil.copytree(source_db, target_db)
    
    # éªŒè¯æ•°æ®å®Œæ•´æ€§
    client = chromadb.PersistentClient(path=target_db)
    collection = client.get_or_create_collection(f"{kb_id}_documents")
    count = collection.count()
    
    print(f"Migration completed. Total documents: {count}")
```

#### 4. ç›‘æ§ä¸å‘Šè­¦

```python
import os

def monitor_db_size(kb_id: str, threshold_gb: float = 10.0):
    """
    ç›‘æ§æ•°æ®åº“ç£ç›˜å ç”¨
    """
    db_path = f"./knowledge_bases/{kb_id}/db"
    
    total_size = 0
    for dirpath, dirnames, filenames in os.walk(db_path):
        for f in filenames:
            fp = os.path.join(dirpath, f)
            total_size += os.path.getsize(fp)
    
    size_gb = total_size / (1024**3)
    
    if size_gb > threshold_gb:
        print(f"âš ï¸ Warning: {kb_id} database size {size_gb:.2f}GB exceeds threshold!")
        # å‘é€å‘Šè­¦é€šçŸ¥...
    
    return size_gb
```

------

## ğŸ›ï¸ æ•°æ®ä¸­å°æ¶æ„è®¾è®¡

### åç«¯ API è®¾è®¡ï¼ˆFastAPIï¼‰

#### å®Œæ•´ API åˆ—è¡¨

```python
# ==================== çŸ¥è¯†åº“ç®¡ç† ====================
POST   /api/kb/create                    # åˆ›å»ºçŸ¥è¯†åº“
GET    /api/kb/list                      # è·å–çŸ¥è¯†åº“åˆ—è¡¨
GET    /api/kb/{kb_id}/info              # è·å–çŸ¥è¯†åº“è¯¦æƒ…
PUT    /api/kb/{kb_id}/update            # æ›´æ–°çŸ¥è¯†åº“é…ç½®
DELETE /api/kb/{kb_id}                   # åˆ é™¤çŸ¥è¯†åº“

# ==================== æ–‡æ¡£ç®¡ç† ====================
POST   /api/kb/{kb_id}/documents/upload  # ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰
GET    /api/kb/{kb_id}/documents         # è·å–æ–‡æ¡£åˆ—è¡¨
GET    /api/kb/{kb_id}/documents/{doc_id} # è·å–æ–‡æ¡£è¯¦æƒ…
DELETE /api/kb/{kb_id}/documents/{doc_id} # åˆ é™¤æ–‡æ¡£
PUT    /api/kb/{kb_id}/documents/{doc_id}/reprocess # é‡æ–°å¤„ç†æ–‡æ¡£

# ==================== æ£€ç´¢æ¥å£ï¼ˆä¾› Dify è°ƒç”¨ï¼‰ ====================
POST   /api/retrieve                     # è¯­ä¹‰æ£€ç´¢
POST   /api/retrieve/multi               # å¤šçŸ¥è¯†åº“è”åˆæ£€ç´¢

# ==================== ç›‘æ§ä¸ç»´æŠ¤ ====================
GET    /api/system/stats                 # ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
GET    /api/kb/{kb_id}/stats             # çŸ¥è¯†åº“ç»Ÿè®¡
POST   /api/maintenance/backup           # è§¦å‘å¤‡ä»½
POST   /api/maintenance/cleanup          # æ¸…ç†æ—§ç‰ˆæœ¬
```

#### æ ¸å¿ƒæ¥å£å®ç°ç¤ºä¾‹

**1. æ–‡æ¡£ä¸Šä¼ æ¥å£**

```python
from fastapi import FastAPI, UploadFile, File, BackgroundTasks
from typing import List
import uuid

app = FastAPI()

@app.post("/api/kb/{kb_id}/documents/upload")
async def upload_documents(
    kb_id: str,
    files: List[UploadFile] = File(...),
    background_tasks: BackgroundTasks = None
):
    """
    ä¸Šä¼ æ–‡æ¡£åˆ°æŒ‡å®šçŸ¥è¯†åº“
    æ”¯æŒæ‰¹é‡ä¸Šä¼ ï¼Œå¼‚æ­¥å¤„ç†
    """
    results = []
    
    for file in files:
        # ç”Ÿæˆæ–‡æ¡£ ID
        doc_id = str(uuid.uuid4())
        
        # ä¿å­˜æ–‡ä»¶
        file_path = f"./knowledge_bases/{kb_id}/docs/{file.filename}"
        with open(file_path, "wb") as f:
            content = await file.read()
            f.write(content)
        
        # æ·»åŠ åˆ°åå°ä»»åŠ¡é˜Ÿåˆ—
        background_tasks.add_task(
            process_document,
            kb_id=kb_id,
            doc_id=doc_id,
            file_path=file_path
        )
        
        results.append({
            "doc_id": doc_id,
            "filename": file.filename,
            "status": "processing"
        })
    
    return {"success": True, "documents": results}
```

**2. æ£€ç´¢æ¥å£ï¼ˆä¾› Dify è°ƒç”¨ï¼‰**

```python
from pydantic import BaseModel
from typing import Optional, Dict, List

class RetrieveRequest(BaseModel):
    query: str
    kb_id: str
    top_k: int = 5
    filters: Optional[Dict] = None
    min_score: float = 0.6

@app.post("/api/retrieve")
async def retrieve(request: RetrieveRequest):
    """
    è¯­ä¹‰æ£€ç´¢æ¥å£
    Dify é€šè¿‡æ­¤æ¥å£è·å–ç›¸å…³çŸ¥è¯†
    """
    # åŠ è½½çŸ¥è¯†åº“
    collection = get_collection(request.kb_id)
    
    # å‘é‡åŒ–æŸ¥è¯¢
    query_embedding = get_embedding(request.query)
    
    # æ£€ç´¢
    results = collection.query(
        query_embeddings=[query_embedding],
        n_results=request.top_k,
        where=request.filters  # æŒ‰ metadata è¿‡æ»¤
    )
    
    # æ ¼å¼åŒ–è¿”å›
    chunks = []
    for i, doc in enumerate(results['documents'][0]):
        score = results['distances'][0][i]
        
        if score >= request.min_score:
            chunks.append({
                "content": doc,
                "metadata": results['metadatas'][0][i],
                "score": score
            })
    
    return {
        "success": True,
        "query": request.query,
        "kb_id": request.kb_id,
        "chunks": chunks,
        "total": len(chunks)
    }
```

**3. æ–‡æ¡£å¤„ç†é€»è¾‘ï¼ˆLangChain å±‚ï¼‰**

```python
from langchain.document_loaders import UnstructuredFileLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
import json

def process_document(kb_id: str, doc_id: str, file_path: str):
    """
    å¤„ç†ä¸Šä¼ çš„æ–‡æ¡£
    1. è§£ææ–‡æ¡£
    2. åˆ†å—
    3. å‘é‡åŒ–
    4. å­˜å…¥æ•°æ®åº“
    """
    try:
        # 1. è¯»å–é…ç½®
        with open(f"./knowledge_bases/{kb_id}/config.json") as f:
            config = json.load(f)
        
        # 2. åŠ è½½æ–‡æ¡£
        loader = UnstructuredFileLoader(file_path)
        documents = loader.load()
        
        # 3. æå–å…ƒä¿¡æ¯ï¼ˆä»æ–‡ä»¶åæˆ–å†…å®¹ï¼‰
        metadata = extract_metadata(file_path, documents[0].page_content)
        
        # 4. åˆ†å—
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config['chunk_size'],
            chunk_overlap=config['chunk_overlap'],
            separators=["\n\n", "\n", "ã€‚", "!", "?", "ï¼›", "â€¦â€¦", "â€¦", " "]
        )
        chunks = text_splitter.split_documents(documents)
        
        # 5. å‘é‡åŒ–
        embeddings = OpenAIEmbeddings(model=config['embedding_model'])
        
        # 6. å­˜å…¥æ•°æ®åº“
        collection = get_collection(kb_id)
        
        for i, chunk in enumerate(chunks):
            chunk_metadata = {
                **metadata,
                "doc_id": doc_id,
                "chunk_index": i,
                "source": file_path
            }
            
            embedding = embeddings.embed_query(chunk.page_content)
            
            collection.add(
                embeddings=[embedding],
                documents=[chunk.page_content],
                metadatas=[chunk_metadata],
                ids=[f"{doc_id}_chunk_{i}"]
            )
        
        # 7. æ›´æ–°æ–‡æ¡£çŠ¶æ€
        update_document_status(kb_id, doc_id, "completed", len(chunks))
        
        print(f"âœ… Document {doc_id} processed successfully: {len(chunks)} chunks")
        
    except Exception as e:
        update_document_status(kb_id, doc_id, "failed", 0, str(e))
        print(f"âŒ Error processing document {doc_id}: {e}")

def extract_metadata(file_path: str, content: str) -> dict:
    """
    ä»æ–‡ä»¶åå’Œå†…å®¹ä¸­æå–å…ƒä¿¡æ¯
    """
    import re
    from pathlib import Path
    
    filename = Path(file_path).stem
    
    metadata = {
        "filename": filename,
        "doc_type": "unknown",
        "version": "1.0",
        "system": "unknown"
    }
    
    # ä»æ–‡ä»¶åæå–ï¼ˆä¾‹å¦‚ï¼šCRM_è®¾è®¡æ–‡æ¡£_v3.0.docxï¼‰
    match = re.search(r'([A-Z]+)_(.+?)_v([\d.]+)', filename)
    if match:
        metadata['system'] = match.group(1)
        metadata['doc_type'] = match.group(2)
        metadata['version'] = match.group(3)
    
    # ä»å†…å®¹æå–ï¼ˆæŸ¥æ‰¾"ç³»ç»Ÿï¼š"ã€"ç‰ˆæœ¬ï¼š"ç­‰å…³é”®è¯ï¼‰
    if 'éœ€æ±‚æ–‡æ¡£' in content[:500]:
        metadata['doc_type'] = 'éœ€æ±‚æ–‡æ¡£'
    elif 'è®¾è®¡æ–‡æ¡£' in content[:500]:
        metadata['doc_type'] = 'è®¾è®¡æ–‡æ¡£'
    elif 'è¿ç»´' in content[:500]:
        metadata['doc_type'] = 'è¿ç»´æ–‡æ¡£'
    
    return metadata
```

### å‰ç«¯ç®¡ç†ç•Œé¢è®¾è®¡

#### æŠ€æœ¯æ ˆæ¨è

```
å‰ç«¯æ¡†æ¶ï¼šReact 18 + TypeScript
UI ç»„ä»¶åº“ï¼šAnt Design 5.x
çŠ¶æ€ç®¡ç†ï¼šZustand / Redux Toolkit
HTTP å®¢æˆ·ç«¯ï¼šAxios
å›¾è¡¨å±•ç¤ºï¼šECharts / Recharts
```

#### æ ¸å¿ƒåŠŸèƒ½æ¨¡å—

**1. çŸ¥è¯†åº“ç®¡ç†é¡µé¢**

```
åŠŸèƒ½ï¼š
âœ… åˆ›å»º/åˆ é™¤çŸ¥è¯†åº“
âœ… åˆ‡æ¢å½“å‰çŸ¥è¯†åº“
âœ… æŸ¥çœ‹çŸ¥è¯†åº“ç»Ÿè®¡ï¼ˆæ–‡æ¡£æ•°ã€chunk æ•°ã€æ€»å¤§å°ï¼‰
âœ… é…ç½®çŸ¥è¯†åº“å‚æ•°ï¼ˆchunk_sizeã€embedding_modelï¼‰
```

**2. æ–‡æ¡£ç®¡ç†é¡µé¢**

```
åŠŸèƒ½ï¼š
âœ… æ‹–æ‹½ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰
âœ… æ–‡æ¡£åˆ—è¡¨å±•ç¤ºï¼ˆæ–‡ä»¶åã€å¤§å°ã€çŠ¶æ€ã€ä¸Šä¼ æ—¶é—´ï¼‰
âœ… æ–‡æ¡£çŠ¶æ€ï¼šprocessingï¼ˆå¤„ç†ä¸­ï¼‰ã€completedï¼ˆå·²å®Œæˆï¼‰ã€failedï¼ˆå¤±è´¥ï¼‰
âœ… æŸ¥çœ‹æ–‡æ¡£è¯¦æƒ…ï¼ˆchunk åˆ—è¡¨ã€metadataï¼‰
âœ… é‡æ–°å¤„ç†å¤±è´¥çš„æ–‡æ¡£
âœ… åˆ é™¤æ–‡æ¡£
```

**3. æ£€ç´¢æµ‹è¯•é¡µé¢**

```
åŠŸèƒ½ï¼š
âœ… è¾“å…¥æµ‹è¯•é—®é¢˜
âœ… é€‰æ‹©çŸ¥è¯†åº“
âœ… è®¾ç½®æ£€ç´¢å‚æ•°ï¼ˆtop_kã€è¿‡æ»¤æ¡ä»¶ï¼‰
âœ… æŸ¥çœ‹å¬å›çš„ chunks
âœ… æ˜¾ç¤ºç›¸ä¼¼åº¦åˆ†æ•°
âœ… é«˜äº®åŒ¹é…å…³é”®è¯
```

**4. ç³»ç»Ÿç›‘æ§é¡µé¢**

```
åŠŸèƒ½ï¼š
âœ… æ•°æ®åº“ç£ç›˜å ç”¨
âœ… API è°ƒç”¨ç»Ÿè®¡
âœ… æ–‡æ¡£å¤„ç†é€Ÿåº¦
âœ… é”™è¯¯æ—¥å¿—æŸ¥çœ‹
âœ… è§¦å‘æ‰‹åŠ¨å¤‡ä»½
```

------

## ğŸš€ å®Œæ•´å®æ–½æ­¥éª¤

### ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒå‡†å¤‡

```bash
# 1. åˆ›å»ºé¡¹ç›®ç›®å½•
mkdir knowledge_platform && cd knowledge_platform

# 2. åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.10 -m venv venv
source venv/bin/activate  # Windows: venv\Scripts\activate

# 3. å®‰è£…ä¾èµ–
pip install fastapi uvicorn langchain langchain-community \
            openai chromadb unstructured watchdog \
            python-multipart pydantic pillow

# 4. åˆ›å»ºç›®å½•ç»“æ„
mkdir -p knowledge_bases data_platform/{backend,frontend} logs
```

### ç¬¬äºŒé˜¶æ®µï¼šæ­å»ºæ•°æ®ä¸­å°åç«¯

**é¡¹ç›®ç»“æ„**ï¼š

```
data_platform/backend/
â”œâ”€â”€ main.py                    # FastAPI ä¸»å…¥å£
â”œâ”€â”€ api/
â”‚   â”œâ”€â”€ kb_management.py       # çŸ¥è¯†åº“ç®¡ç† API
â”‚   â”œâ”€â”€ document_management.py # æ–‡æ¡£ç®¡ç† API
â”‚   â”œâ”€â”€ retrieve.py            # æ£€ç´¢ API
â”‚   â””â”€â”€ monitoring.py          # ç›‘æ§ API
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ langchain_processor.py # LangChain å¤„ç†é€»è¾‘
â”‚   â”œâ”€â”€ db_manager.py          # æ•°æ®åº“ç®¡ç†
â”‚   â””â”€â”€ metadata_extractor.py # å…ƒä¿¡æ¯æå–
â”œâ”€â”€ models/
â”‚   â””â”€â”€ schemas.py             # Pydantic æ•°æ®æ¨¡å‹
â””â”€â”€ config/
    â””â”€â”€ settings.py            # é…ç½®æ–‡ä»¶
```

**å¯åŠ¨åç«¯**ï¼š

```bash
cd data_platform/backend
uvicorn main:app --host 0.0.0.0 --port 8000 --reload
```

### ç¬¬ä¸‰é˜¶æ®µï¼šå¯¹æ¥ Dify

**åœ¨ Dify ä¸­åˆ›å»ºè‡ªå®šä¹‰å·¥å…·**ï¼š

```yaml
å·¥å…·åç§°: knowledge_platform_retrieve
æè¿°: ä»çŸ¥è¯†ä¸­å°æ£€ç´¢ç›¸å…³å†…å®¹
API é…ç½®:
  - Method: POST
  - URL: http://your-server:8000/api/retrieve
  - Headers:
      Content-Type: application/json
  - Body:
      {
        "query": "{{query}}",
        "kb_id": "{{kb_id}}",
        "top_k": 5
      }
è¾“å‡ºå˜é‡:
  - chunks (array): æ£€ç´¢åˆ°çš„çŸ¥è¯†ç‰‡æ®µ
  - total (number): ç»“æœæ•°é‡
```

**åœ¨ Dify Workflow ä¸­ä½¿ç”¨**ï¼š

```
1. æ¥æ”¶ç”¨æˆ·é—®é¢˜
2. è°ƒç”¨ knowledge_platform_retrieve å·¥å…·
   - query: {{ç”¨æˆ·è¾“å…¥}}
   - kb_id: "crm_system"
3. æå– chunks[].content åˆå¹¶ä¸ºä¸Šä¸‹æ–‡
4. è°ƒç”¨ LLM èŠ‚ç‚¹
   - System Prompt: "ä½ æ˜¯ä¸€ä¸ªæŠ€æœ¯åŠ©æ‰‹ï¼Œè¯·æ ¹æ®ä»¥ä¸‹çŸ¥è¯†å›ç­”é—®é¢˜"
   - Context: {{æ£€ç´¢ç»“æœ}}
   - User Query: {{ç”¨æˆ·è¾“å…¥}}
5. è¿”å›ç­”æ¡ˆ
```

### ç¬¬å››é˜¶æ®µï¼šå¼€å‘å‰ç«¯ç®¡ç†ç•Œé¢ï¼ˆå¯é€‰ï¼‰

```bash
# ä½¿ç”¨ Vite + React åˆ›å»ºé¡¹ç›®
cd data_platform/frontend
npm create vite@latest . -- --template react-ts
npm install antd axios zustand recharts

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
npm run dev
```

### ç¬¬äº”é˜¶æ®µï¼šéƒ¨ç½²ä¸Šçº¿

**Docker éƒ¨ç½²ï¼ˆæ¨èï¼‰**ï¼š

```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨æœåŠ¡
CMD ["uvicorn", "data_platform.backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
# docker-compose.yml
version: '3.8'

services:
  knowledge-platform:
    build: .
    ports:
      - "8000:8000"
    volumes:
      - ./knowledge_bases:/app/knowledge_bases
      - ./logs:/app/logs
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
    restart: unless-stopped
```

------

## ğŸ¯ è¿›é˜¶åŠŸèƒ½æ‰©å±•

### 1. è‡ªåŠ¨åŒ–æ–‡æ¡£ç›‘å¬

```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import time

class DocumentWatcher(FileSystemEventHandler):
    """
    ç›‘å¬æ–‡æ¡£æ–‡ä»¶å¤¹ï¼Œè‡ªåŠ¨è§¦å‘å¤„ç†
    """
    def __init__(self, kb_id: str):
        self.kb_id = kb_id
        self.watch_path = f"./knowledge_bases/{kb_id}/docs"
    
    def on_created(self, event):
        if event.is_directory:
            return
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        allowed_extensions = ['.txt', '.md', '.docx', '.pdf', '.html']
        if not any(event.src_path.endswith(ext) for ext in allowed_extensions):
            return
        
        print(f"ğŸ“„ New document detected: {event.src_path}")
        
        # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆ
        time.sleep(2)
        
        # è§¦å‘å¤„ç†
        doc_id = str(uuid.uuid4())
        process_document(self.kb_id, doc_id, event.src_path)
    
    def on_modified(self, event):
        if event.is_directory:
            return
        
        print(f"ğŸ“ Document modified: {event.src_path}")
        # å¯ä»¥é€‰æ‹©é‡æ–°å¤„ç†æ–‡æ¡£

def start_watcher(kb_id: str):
    """
    å¯åŠ¨æ–‡ä»¶ç›‘å¬å™¨
    """
    event_handler = DocumentWatcher(kb_id)
    observer = Observer()
    observer.schedule(
        event_handler, 
        f"./knowledge_bases/{kb_id}/docs", 
        recursive=True
    )
    observer.start()
    
    print(f"ğŸ‘€ Watching knowledge base: {kb_id}")
    
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        observer.stop()
    observer.join()

# ä½¿ç”¨ç¤ºä¾‹
if __name__ == "__main__":
    start_watcher("crm_system")
```

### 2. ç‰ˆæœ¬ç®¡ç†ä¸å¯¹æ¯”

```python
def compare_versions(kb_id: str, doc_name: str, version1: str, version2: str):
    """
    å¯¹æ¯”åŒä¸€æ–‡æ¡£çš„ä¸¤ä¸ªç‰ˆæœ¬å·®å¼‚
    """
    collection = get_collection(kb_id)
    
    # è·å–ä¸¤ä¸ªç‰ˆæœ¬çš„å†…å®¹
    v1_chunks = collection.get(
        where={"doc_name": doc_name, "version": version1}
    )
    v2_chunks = collection.get(
        where={"doc_name": doc_name, "version": version2}
    )
    
    # ä½¿ç”¨ difflib å¯¹æ¯”
    import difflib
    
    v1_text = "\n".join(v1_chunks['documents'])
    v2_text = "\n".join(v2_chunks['documents'])
    
    diff = difflib.unified_diff(
        v1_text.splitlines(),
        v2_text.splitlines(),
        lineterm='',
        fromfile=f'{doc_name} v{version1}',
        tofile=f'{doc_name} v{version2}'
    )
    
    return list(diff)

def get_version_history(kb_id: str, doc_name: str):
    """
    è·å–æ–‡æ¡£çš„ç‰ˆæœ¬å†å²
    """
    collection = get_collection(kb_id)
    
    results = collection.get(
        where={"doc_name": doc_name},
        include=["metadatas"]
    )
    
    versions = {}
    for metadata in results['metadatas']:
        version = metadata.get('version', '1.0')
        if version not in versions:
            versions[version] = {
                "version": version,
                "created_at": metadata.get('created_at'),
                "author": metadata.get('author'),
                "chunk_count": 0
            }
        versions[version]['chunk_count'] += 1
    
    return sorted(versions.values(), key=lambda x: x['version'], reverse=True)
```

### 3. æ™ºèƒ½å…ƒä¿¡æ¯æå–

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain.llms import OpenAI

def extract_metadata_with_llm(content: str) -> dict:
    """
    ä½¿ç”¨ LLM æ™ºèƒ½æå–æ–‡æ¡£å…ƒä¿¡æ¯
    """
    prompt = PromptTemplate(
        input_variables=["content"],
        template="""
        è¯·ä»ä»¥ä¸‹æ–‡æ¡£å†…å®¹ä¸­æå–å…³é”®å…ƒä¿¡æ¯ï¼Œä»¥ JSON æ ¼å¼è¿”å›ï¼š
        
        æ–‡æ¡£å†…å®¹ï¼š
        {content}
        
        éœ€è¦æå–çš„ä¿¡æ¯ï¼š
        - system: ç³»ç»Ÿåç§°ï¼ˆå¦‚ CRMã€ERPï¼‰
        - doc_type: æ–‡æ¡£ç±»å‹ï¼ˆéœ€æ±‚æ–‡æ¡£/è®¾è®¡æ–‡æ¡£/è¿ç»´æ–‡æ¡£/APIæ–‡æ¡£ï¼‰
        - version: ç‰ˆæœ¬å·
        - module: ä¸»è¦æ¶‰åŠçš„æ¨¡å—
        - keywords: 5ä¸ªå…³é”®è¯
        
        åªè¿”å› JSONï¼Œä¸è¦å…¶ä»–å†…å®¹ï¼š
        """
    )
    
    llm = OpenAI(temperature=0)
    chain = LLMChain(llm=llm, prompt=prompt)
    
    result = chain.run(content=content[:3000])  # åªå–å‰3000å­—
    
    try:
        metadata = json.loads(result)
        return metadata
    except:
        return {
            "system": "unknown",
            "doc_type": "unknown",
            "version": "1.0",
            "module": "general",
            "keywords": []
        }
```

### 4. å¤šçŸ¥è¯†åº“è”åˆæ£€ç´¢

```python
@app.post("/api/retrieve/multi")
async def multi_kb_retrieve(
    query: str,
    kb_ids: List[str],
    top_k: int = 5,
    merge_strategy: str = "score"  # score / round_robin / kb_priority
):
    """
    è·¨å¤šä¸ªçŸ¥è¯†åº“æ£€ç´¢
    é€‚ç”¨äºï¼šéœ€è¦ç»¼åˆå¤šä¸ªç³»ç»Ÿçš„çŸ¥è¯†å›ç­”é—®é¢˜
    """
    all_results = []
    
    # ä»æ¯ä¸ªçŸ¥è¯†åº“æ£€ç´¢
    for kb_id in kb_ids:
        collection = get_collection(kb_id)
        query_embedding = get_embedding(query)
        
        results = collection.query(
            query_embeddings=[query_embedding],
            n_results=top_k
        )
        
        # æ·»åŠ æ¥æºæ ‡è¯†
        for i, doc in enumerate(results['documents'][0]):
            all_results.append({
                "content": doc,
                "metadata": {
                    **results['metadatas'][0][i],
                    "source_kb": kb_id
                },
                "score": results['distances'][0][i]
            })
    
    # åˆå¹¶ç­–ç•¥
    if merge_strategy == "score":
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        all_results.sort(key=lambda x: x['score'], reverse=True)
        final_results = all_results[:top_k]
    
    elif merge_strategy == "round_robin":
        # è½®æµå–æ¯ä¸ªçŸ¥è¯†åº“çš„ç»“æœ
        final_results = []
        kb_indices = {kb_id: 0 for kb_id in kb_ids}
        
        while len(final_results) < top_k:
            for kb_id in kb_ids:
                kb_results = [r for r in all_results if r['metadata']['source_kb'] == kb_id]
                idx = kb_indices[kb_id]
                if idx < len(kb_results):
                    final_results.append(kb_results[idx])
                    kb_indices[kb_id] += 1
    
    elif merge_strategy == "kb_priority":
        # æŒ‰çŸ¥è¯†åº“ä¼˜å…ˆçº§é¡ºåº
        final_results = []
        for kb_id in kb_ids:
            kb_results = [r for r in all_results if r['metadata']['source_kb'] == kb_id]
            final_results.extend(kb_results[:top_k])
            if len(final_results) >= top_k:
                break
        final_results = final_results[:top_k]
    
    return {
        "success": True,
        "query": query,
        "kb_ids": kb_ids,
        "chunks": final_results,
        "total": len(final_results)
    }
```

### 5. æ£€ç´¢ç»“æœé‡æ’åºï¼ˆRerankï¼‰

```python
from sentence_transformers import CrossEncoder

class RerankerService:
    """
    ä½¿ç”¨äº¤å‰ç¼–ç å™¨å¯¹æ£€ç´¢ç»“æœé‡æ’åº
    æå‡æ£€ç´¢å‡†ç¡®æ€§
    """
    def __init__(self):
        self.model = CrossEncoder('cross-encoder/ms-marco-MiniLM-L-6-v2')
    
    def rerank(self, query: str, chunks: List[dict], top_k: int = 5) -> List[dict]:
        """
        å¯¹åˆæ­¥æ£€ç´¢ç»“æœé‡æ’åº
        """
        # å‡†å¤‡è¾“å…¥å¯¹
        pairs = [[query, chunk['content']] for chunk in chunks]
        
        # è®¡ç®—ç›¸å…³æ€§åˆ†æ•°
        scores = self.model.predict(pairs)
        
        # æ·»åŠ é‡æ’åºåˆ†æ•°
        for i, chunk in enumerate(chunks):
            chunk['rerank_score'] = float(scores[i])
        
        # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
        reranked = sorted(chunks, key=lambda x: x['rerank_score'], reverse=True)
        
        return reranked[:top_k]

# åœ¨æ£€ç´¢æ¥å£ä¸­ä½¿ç”¨
reranker = RerankerService()

@app.post("/api/retrieve/rerank")
async def retrieve_with_rerank(request: RetrieveRequest):
    """
    å¸¦é‡æ’åºçš„æ£€ç´¢
    """
    # åˆæ­¥æ£€ç´¢ï¼ˆå– top_k * 2ï¼‰
    initial_results = await retrieve(
        RetrieveRequest(
            query=request.query,
            kb_id=request.kb_id,
            top_k=request.top_k * 2
        )
    )
    
    # é‡æ’åº
    reranked = reranker.rerank(
        request.query,
        initial_results['chunks'],
        request.top_k
    )
    
    return {
        "success": True,
        "query": request.query,
        "chunks": reranked,
        "total": len(reranked)
    }
```

### 6. å¢é‡æ›´æ–°ä¸å·®å¼‚æ£€æµ‹

```python
def incremental_update(kb_id: str, doc_id: str, new_file_path: str):
    """
    å¢é‡æ›´æ–°æ–‡æ¡£
    åªå¤„ç†å˜åŒ–çš„éƒ¨åˆ†
    """
    collection = get_collection(kb_id)
    
    # è·å–æ—§æ–‡æ¡£çš„ chunks
    old_chunks = collection.get(
        where={"doc_id": doc_id},
        include=["documents", "metadatas"]
    )
    
    # è§£ææ–°æ–‡æ¡£
    loader = UnstructuredFileLoader(new_file_path)
    new_documents = loader.load()
    
    # åˆ†å—
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
    new_chunks = text_splitter.split_documents(new_documents)
    
    # å¯¹æ¯”å·®å¼‚
    old_texts = set(old_chunks['documents'])
    new_texts = [chunk.page_content for chunk in new_chunks]
    
    # æ‰¾å‡ºéœ€è¦æ·»åŠ çš„æ–° chunks
    to_add = [text for text in new_texts if text not in old_texts]
    
    # æ‰¾å‡ºéœ€è¦åˆ é™¤çš„æ—§ chunks
    to_delete = [text for text in old_texts if text not in new_texts]
    
    # æ‰§è¡Œåˆ é™¤
    for text in to_delete:
        collection.delete(where={"doc_id": doc_id, "content": text})
    
    # æ‰§è¡Œæ·»åŠ 
    embeddings = OpenAIEmbeddings()
    for i, text in enumerate(to_add):
        embedding = embeddings.embed_query(text)
        collection.add(
            embeddings=[embedding],
            documents=[text],
            metadatas=[{
                "doc_id": doc_id,
                "chunk_index": len(old_chunks['documents']) + i,
                "updated_at": datetime.now().isoformat()
            }],
            ids=[f"{doc_id}_chunk_{len(old_chunks['documents']) + i}"]
        )
    
    print(f"âœ… Incremental update: +{len(to_add)} chunks, -{len(to_delete)} chunks")
```

### 7. çŸ¥è¯†å›¾è°±æ„å»ºï¼ˆé«˜çº§ï¼‰

```python
from langchain.chains import GraphQAChain
import networkx as nx

class KnowledgeGraphBuilder:
    """
    ä»æ–‡æ¡£ä¸­æå–å®ä½“å’Œå…³ç³»ï¼Œæ„å»ºçŸ¥è¯†å›¾è°±
    """
    def __init__(self):
        self.graph = nx.DiGraph()
    
    def extract_entities_and_relations(self, text: str):
        """
        ä½¿ç”¨ NER å’Œå…³ç³»æŠ½å–æå–çŸ¥è¯†ä¸‰å…ƒç»„
        """
        # ä½¿ç”¨ spaCy æˆ– LLM æå–å®ä½“
        prompt = f"""
        ä»ä»¥ä¸‹æ–‡æœ¬ä¸­æå–çŸ¥è¯†ä¸‰å…ƒç»„ï¼ˆä¸»ä½“-å…³ç³»-å®¢ä½“ï¼‰ï¼š
        
        æ–‡æœ¬ï¼š{text}
        
        è¿”å›æ ¼å¼ï¼š
        ä¸»ä½“|å…³ç³»|å®¢ä½“
        ä¸»ä½“|å…³ç³»|å®¢ä½“
        ...
        """
        
        # è¿™é‡Œå¯ä»¥æ¥å…¥ GPT-4 æˆ–æœ¬åœ° NER æ¨¡å‹
        # ç¤ºä¾‹è¿”å›
        return [
            ("CRMç³»ç»Ÿ", "åŒ…å«", "ç™»å½•æ¨¡å—"),
            ("ç™»å½•æ¨¡å—", "æ”¯æŒ", "è´¦å·å¯†ç è®¤è¯"),
            ("ç™»å½•æ¨¡å—", "æ”¯æŒ", "æ‰‹æœºéªŒè¯ç "),
        ]
    
    def build_graph(self, kb_id: str):
        """
        ä¸ºæ•´ä¸ªçŸ¥è¯†åº“æ„å»ºå›¾è°±
        """
        collection = get_collection(kb_id)
        all_docs = collection.get()
        
        for doc in all_docs['documents']:
            triples = self.extract_entities_and_relations(doc)
            
            for subject, relation, obj in triples:
                self.graph.add_edge(subject, obj, relation=relation)
        
        return self.graph
    
    def query_graph(self, query: str):
        """
        åŸºäºå›¾è°±çš„æŸ¥è¯¢
        """
        # ä½¿ç”¨ Cypher æˆ– GraphQL æŸ¥è¯¢
        # ç¤ºä¾‹ï¼šæ‰¾åˆ° CRM ç³»ç»Ÿçš„æ‰€æœ‰æ¨¡å—
        # MATCH (s:System {name: "CRMç³»ç»Ÿ"})-[:åŒ…å«]->(m:Module)
        # RETURN m.name
        pass
```

### 8. è‡ªåŠ¨åŒ–æµ‹è¯•ä¸è´¨é‡è¯„ä¼°

```python
class KnowledgeQualityEvaluator:
    """
    è¯„ä¼°çŸ¥è¯†åº“è´¨é‡
    """
    def evaluate_coverage(self, kb_id: str, test_questions: List[str]) -> dict:
        """
        æµ‹è¯•çŸ¥è¯†åº“è¦†ç›–ç‡
        """
        results = {
            "total_questions": len(test_questions),
            "answered": 0,
            "no_answer": 0,
            "low_confidence": 0
        }
        
        for question in test_questions:
            response = retrieve(RetrieveRequest(
                query=question,
                kb_id=kb_id,
                top_k=3
            ))
            
            if len(response['chunks']) == 0:
                results['no_answer'] += 1
            elif response['chunks'][0]['score'] < 0.7:
                results['low_confidence'] += 1
            else:
                results['answered'] += 1
        
        results['coverage_rate'] = results['answered'] / results['total_questions']
        
        return results
    
    def evaluate_chunk_quality(self, kb_id: str) -> dict:
        """
        è¯„ä¼° chunk è´¨é‡
        """
        collection = get_collection(kb_id)
        all_chunks = collection.get()
        
        stats = {
            "total_chunks": len(all_chunks['documents']),
            "avg_length": 0,
            "too_short": 0,  # < 100 å­—
            "too_long": 0,   # > 2000 å­—
            "optimal": 0     # 500-1500 å­—
        }
        
        lengths = []
        for doc in all_chunks['documents']:
            length = len(doc)
            lengths.append(length)
            
            if length < 100:
                stats['too_short'] += 1
            elif length > 2000:
                stats['too_long'] += 1
            else:
                stats['optimal'] += 1
        
        stats['avg_length'] = sum(lengths) / len(lengths)
        stats['optimal_rate'] = stats['optimal'] / stats['total_chunks']
        
        return stats
```

------

## ğŸ“Š ç³»ç»Ÿç›‘æ§ä¸å‘Šè­¦

### ç›‘æ§æŒ‡æ ‡è®¾è®¡

```python
from datetime import datetime, timedelta
import psutil

class SystemMonitor:
    """
    ç³»ç»Ÿç›‘æ§æœåŠ¡
    """
    def get_system_stats(self):
        """
        è·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
        """
        return {
            "cpu_percent": psutil.cpu_percent(interval=1),
            "memory_percent": psutil.virtual_memory().percent,
            "disk_usage": psutil.disk_usage('/').percent,
            "timestamp": datetime.now().isoformat()
        }
    
    def get_kb_stats(self, kb_id: str):
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        """
        collection = get_collection(kb_id)
        
        # æ•°æ®åº“å¤§å°
        db_path = f"./knowledge_bases/{kb_id}/db"
        db_size = sum(
            os.path.getsize(os.path.join(dirpath, filename))
            for dirpath, _, filenames in os.walk(db_path)
            for filename in filenames
        ) / (1024**3)  # GB
        
        # chunk ç»Ÿè®¡
        total_chunks = collection.count()
        
        # æ–‡æ¡£ç»Ÿè®¡
        all_metadata = collection.get(include=["metadatas"])
        unique_docs = len(set(m['doc_id'] for m in all_metadata['metadatas']))
        
        return {
            "kb_id": kb_id,
            "total_chunks": total_chunks,
            "total_documents": unique_docs,
            "db_size_gb": round(db_size, 2),
            "avg_chunks_per_doc": round(total_chunks / unique_docs, 1) if unique_docs > 0 else 0
        }
    
    def get_api_stats(self, time_range: int = 24):
        """
        è·å– API è°ƒç”¨ç»Ÿè®¡ï¼ˆæœ€è¿‘ N å°æ—¶ï¼‰
        """
        # è¿™é‡Œéœ€è¦ä»æ—¥å¿—æˆ–æ•°æ®åº“è¯»å–
        # ç¤ºä¾‹æ•°æ®
        return {
            "total_calls": 1523,
            "retrieve_calls": 1200,
            "upload_calls": 150,
            "avg_response_time_ms": 245,
            "error_rate": 0.02
        }

# åœ¨ FastAPI ä¸­ä½¿ç”¨
monitor = SystemMonitor()

@app.get("/api/system/stats")
async def get_system_stats():
    return monitor.get_system_stats()

@app.get("/api/kb/{kb_id}/stats")
async def get_kb_stats(kb_id: str):
    return monitor.get_kb_stats(kb_id)
```

### å‘Šè­¦é…ç½®

```python
class AlertService:
    """
    å‘Šè­¦æœåŠ¡
    """
    def __init__(self, webhook_url: str):
        self.webhook_url = webhook_url
    
    def check_and_alert(self):
        """
        æ£€æŸ¥å„é¡¹æŒ‡æ ‡å¹¶å‘é€å‘Šè­¦
        """
        alerts = []
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        disk_usage = psutil.disk_usage('/').percent
        if disk_usage > 85:
            alerts.append({
                "level": "critical",
                "message": f"ç£ç›˜ä½¿ç”¨ç‡è¿‡é«˜: {disk_usage}%"
            })
        
        # æ£€æŸ¥æ•°æ®åº“å¤§å°
        for kb_id in list_all_kb_ids():
            size_gb = monitor_db_size(kb_id)
            if size_gb > 50:
                alerts.append({
                    "level": "warning",
                    "message": f"çŸ¥è¯†åº“ {kb_id} å¤§å°è¶…è¿‡ 50GB: {size_gb}GB"
                })
        
        # å‘é€å‘Šè­¦
        if alerts:
            self.send_alerts(alerts)
    
    def send_alerts(self, alerts: List[dict]):
        """
        å‘é€å‘Šè­¦åˆ°ä¼ä¸šå¾®ä¿¡/é’‰é’‰/é‚®ä»¶
        """
        import requests
        
        for alert in alerts:
            requests.post(self.webhook_url, json={
                "msgtype": "text",
                "text": {
                    "content": f"[{alert['level'].upper()}] {alert['message']}"
                }
            })
```

------

## ğŸ”’ å®‰å…¨ä¸æƒé™æ§åˆ¶

### API è®¤è¯

```python
from fastapi import Depends, HTTPException, status
from fastapi.security import HTTPBearer, HTTPAuthorizationCredentials
import jwt

security = HTTPBearer()

def verify_token(credentials: HTTPAuthorizationCredentials = Depends(security)):
    """
    éªŒè¯ JWT Token
    """
    try:
        payload = jwt.decode(
            credentials.credentials,
            SECRET_KEY,
            algorithms=["HS256"]
        )
        return payload
    except jwt.ExpiredSignatureError:
        raise HTTPException(status_code=401, detail="Token expired")
    except jwt.InvalidTokenError:
        raise HTTPException(status_code=401, detail="Invalid token")

# åœ¨ API ä¸­ä½¿ç”¨
@app.post("/api/kb/{kb_id}/documents/upload")
async def upload_documents(
    kb_id: str,
    files: List[UploadFile],
    user: dict = Depends(verify_token)
):
    # æ£€æŸ¥ç”¨æˆ·æƒé™
    if not has_permission(user['user_id'], kb_id, 'write'):
        raise HTTPException(status_code=403, detail="No permission")
    
    # å¤„ç†ä¸Šä¼ ...
```

### çŸ¥è¯†åº“æƒé™ç®¡ç†

```python
class PermissionManager:
    """
    æƒé™ç®¡ç†
    """
    def __init__(self):
        self.permissions = {}  # {user_id: {kb_id: [permissions]}}
    
    def grant_permission(self, user_id: str, kb_id: str, permission: str):
        """
        æˆäºˆæƒé™: read / write / admin
        """
        if user_id not in self.permissions:
            self.permissions[user_id] = {}
        
        if kb_id not in self.permissions[user_id]:
            self.permissions[user_id][kb_id] = []
        
        if permission not in self.permissions[user_id][kb_id]:
            self.permissions[user_id][kb_id].append(permission)
    
    def has_permission(self, user_id: str, kb_id: str, permission: str) -> bool:
        """
        æ£€æŸ¥æƒé™
        """
        if user_id not in self.permissions:
            return False
        
        if kb_id not in self.permissions[user_id]:
            return False
        
        return permission in self.permissions[user_id][kb_id] or \
               'admin' in self.permissions[user_id][kb_id]
```

------

## ğŸ“ æœ€ä½³å®è·µå»ºè®®

### æ–‡æ¡£å‡†å¤‡å»ºè®®

1. **ç»Ÿä¸€å‘½åè§„èŒƒ**

   ```
   æ ¼å¼ï¼š{ç³»ç»Ÿå}_{æ–‡æ¡£ç±»å‹}_{æ¨¡å—å}_v{ç‰ˆæœ¬å·}.{æ‰©å±•å}
   ç¤ºä¾‹ï¼šCRM_è®¾è®¡æ–‡æ¡£_ç™»å½•æ¨¡å—_v3.0.docx
   ```

2. **ç« èŠ‚ç»“æ„æ¨¡æ¿**

   ```markdown
   # {ç³»ç»Ÿå} - {æ–‡æ¡£ç±»å‹}
   
   **ç‰ˆæœ¬**: v3.0
   **ä½œè€…**: å¼ ä¸‰
   **æ—¥æœŸ**: 2025-10-30
   
   ## 1. æ¦‚è¿°
   ï¼ˆç³»ç»ŸèƒŒæ™¯ã€ç›®æ ‡ï¼‰
   
   ## 2. åŠŸèƒ½æ¨¡å—
   ### 2.1 æ¨¡å—A
   #### 2.1.1 åŠŸèƒ½è¯´æ˜
   #### 2.1.2 æŠ€æœ¯å®ç°
   
   ### 2.2 æ¨¡å—B
   ...
   ```

3. **è¡¨æ ¼ä¸å›¾ç‰‡å¤„ç†**

   - è¡¨æ ¼å‰ååŠ æ–‡å­—è¯´æ˜
   - å›¾ç‰‡é…æ–‡å­—æè¿°
   - æ¶æ„å›¾ç”¨ Mermaid æˆ–æ–‡å­—è¡¥å……

### æ£€ç´¢ä¼˜åŒ–ç­–ç•¥

1. **åˆç†è®¾ç½® top_k**

   - ä¸€èˆ¬é—®é¢˜ï¼štop_k = 3-5
   - å¤æ‚é—®é¢˜ï¼štop_k = 8-10
   - éœ€è¦å…¨é¢äº†è§£ï¼štop_k = 15-20

2. **ä½¿ç”¨ metadata è¿‡æ»¤**

   ```python
   # åªæ£€ç´¢è®¾è®¡æ–‡æ¡£
   filters = {"doc_type": "è®¾è®¡æ–‡æ¡£"}
   
   # åªæ£€ç´¢æœ€æ–°ç‰ˆæœ¬
   filters = {"version": "v3.0"}
   
   # ç»„åˆæ¡ä»¶
   filters = {
       "doc_type": "è®¾è®¡æ–‡æ¡£",
       "module": "ç™»å½•",
       "version": "v3.0"
   }
   ```

3. **å¯ç”¨é‡æ’åº**

   - å¯¹äºé‡è¦æŸ¥è¯¢ï¼Œä½¿ç”¨ Rerank æå‡å‡†ç¡®æ€§
   - åˆæ£€ç´¢å– top_k * 2ï¼Œé‡æ’åºåå– top_k

### è¿ç»´å»ºè®®

1. **å®šæœŸå¤‡ä»½**

   ```bash
   # æ¯å¤©å‡Œæ™¨ 2 ç‚¹æ‰§è¡Œ
   0 2 * * * /path/to/backup_dbs.sh
   ```

2. **ç›‘æ§å…³é”®æŒ‡æ ‡**

   - ç£ç›˜ç©ºé—´ > 80% å‘Šè­¦
   - API å“åº”æ—¶é—´ > 1s å‘Šè­¦
   - é”™è¯¯ç‡ > 5% å‘Šè­¦

3. **ç‰ˆæœ¬æ¸…ç†ç­–ç•¥**

   - ä¿ç•™æœ€è¿‘ 3 ä¸ªç‰ˆæœ¬
   - æ¯æœˆæ¸…ç†ä¸€æ¬¡æ—§ç‰ˆæœ¬
   - é‡è¦ç‰ˆæœ¬æ‰‹åŠ¨æ ‡è®°ä¿ç•™

------

## ğŸ“š é™„å½•

### å¸¸è§é—®é¢˜FAQ

**Q1: å¦‚ä½•åˆ‡æ¢ embedding æ¨¡å‹ï¼Ÿ**

A: ä¿®æ”¹çŸ¥è¯†åº“çš„ `config.json`ï¼š

```json
{
  "embedding_model": "text-embedding-3-large"  // æˆ–æœ¬åœ°æ¨¡å‹
}
```

**Q2: å¦‚ä½•å¤„ç†è¶…å¤§æ–‡æ¡£ï¼ˆ> 100MBï¼‰ï¼Ÿ**

A:

- æ–¹æ¡ˆ1ï¼šæ‰‹åŠ¨æ‹†åˆ†æ–‡æ¡£
- æ–¹æ¡ˆ2ï¼šå¢åŠ  chunk_sizeï¼Œå‡å°‘ chunk æ•°é‡
- æ–¹æ¡ˆ3ï¼šä½¿ç”¨æµå¼å¤„ç†

**Q3: æ£€ç´¢æ•ˆæœä¸å¥½æ€ä¹ˆåŠï¼Ÿ**

A:

1. æ£€æŸ¥æ–‡æ¡£è´¨é‡ï¼ˆæ˜¯å¦æœ‰ç»“æ„ï¼‰
2. è°ƒæ•´ chunk_size å’Œ overlap
3. å¯ç”¨ Rerank
4. ä½¿ç”¨æ›´å¥½çš„ embedding æ¨¡å‹

**Q4: å¦‚ä½•æ”¯æŒå¤šè¯­è¨€ï¼Ÿ**

A: ä½¿ç”¨å¤šè¯­è¨€ embedding æ¨¡å‹ï¼Œå¦‚ï¼š

- `text-embedding-3-small` (OpenAIï¼Œæ”¯æŒå¤šè¯­è¨€)
- `multilingual-e5-large` (å¼€æº)

------

## âœ… æ€»ç»“

é€šè¿‡ä»¥ä¸Šè®¾è®¡ï¼Œä½ å°†æ‹¥æœ‰ä¸€ä¸ªï¼š

âœ… **å¤šçŸ¥è¯†åº“éš”ç¦»** - ä¸åŒé¡¹ç›®äº’ä¸å¹²æ‰° âœ… **æ•°æ®æŒä¹…åŒ–** - å‘é‡æ•°æ®å®‰å…¨å­˜å‚¨ âœ… **è§£è€¦æ¶æ„** - æ•°æ®ä¸­å°ä½œä¸ºä¸­é—´å±‚ âœ… **è‡ªåŠ¨åŒ–å¤„ç†** - æ–‡æ¡£ä¸Šä¼ å³å…¥åº“ âœ… **çµæ´»æ£€ç´¢** - æ”¯æŒè¿‡æ»¤ã€é‡æ’åºã€å¤šåº“è”åˆ âœ… **å¯è§†åŒ–ç®¡ç†** - å‰ç«¯ç•Œé¢ç›‘æ§ä¸€åˆ‡ âœ… **ä¼ä¸šçº§åŠŸèƒ½** - ç‰ˆæœ¬ç®¡ç†ã€æƒé™æ§åˆ¶ã€ç›‘æ§å‘Šè­¦

è¿™æ˜¯ä¸€ä¸ªå®Œæ•´çš„ä¼ä¸šçº§çŸ¥è¯†ä¸­å°è§£å†³æ–¹æ¡ˆï¼ğŸ‰1