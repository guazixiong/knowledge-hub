# ä¼ä¸šçº§çŸ¥è¯†ä¸­å°å®Œæ•´å®æ–½æŒ‡å—

> **ç›®æ ‡**ï¼šä»é›¶æ­å»º LangChain + Dify ä¼ä¸šçº§çŸ¥è¯†ä¸­å° **é¢„è®¡æ—¶é—´**ï¼š2-3 å‘¨ï¼ˆæ ¹æ®åŠŸèƒ½å®Œæ•´åº¦ï¼‰ **æŠ€æœ¯æ ˆ**ï¼šPython 3.10+ / FastAPI / LangChain / ChromaDB / React

------

## ğŸ“‹ æ€»ä½“å®æ–½è·¯çº¿

```
ç¬¬ä¸€å‘¨ï¼šæ ¸å¿ƒåç«¯å¼€å‘
â”œâ”€â”€ Day 1-2: ç¯å¢ƒæ­å»º + æ•°æ®åº“è®¾è®¡
â”œâ”€â”€ Day 3-4: æ–‡æ¡£å¤„ç†å¼•æ“ï¼ˆLangChain å±‚ï¼‰
â””â”€â”€ Day 5-7: æ•°æ®ä¸­å°åç«¯ API å¼€å‘

ç¬¬äºŒå‘¨ï¼šå¯¹æ¥ä¸ä¼˜åŒ–
â”œâ”€â”€ Day 8-9: Dify å¯¹æ¥ + æ£€ç´¢æ¥å£ä¼˜åŒ–
â”œâ”€â”€ Day 10-11: å¤šçŸ¥è¯†åº“ç®¡ç† + ç‰ˆæœ¬æ§åˆ¶
â””â”€â”€ Day 12-14: ç›‘æ§ã€æ—¥å¿—ã€å¤‡ä»½åŠŸèƒ½

ç¬¬ä¸‰å‘¨ï¼ˆå¯é€‰ï¼‰ï¼šå‰ç«¯å¼€å‘
â”œâ”€â”€ Day 15-17: ç®¡ç†ç•Œé¢å¼€å‘
â”œâ”€â”€ Day 18-19: éƒ¨ç½²é…ç½®ï¼ˆDockerï¼‰
â””â”€â”€ Day 20-21: æµ‹è¯• + æ–‡æ¡£ç¼–å†™
```

------

## ğŸš€ ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒæ­å»ºä¸é¡¹ç›®åˆå§‹åŒ–

### Step 1: åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æ„ï¼ˆ15åˆ†é’Ÿï¼‰

```bash
# åˆ›å»ºé¡¹ç›®æ ¹ç›®å½•
mkdir knowledge_platform && cd knowledge_platform

# åˆ›å»ºç›®å½•ç»“æ„
mkdir -p {knowledge_bases,data_platform/{backend,frontend},logs,backups,tests}

# åˆ›å»ºåç«¯å­ç›®å½•
cd data_platform/backend
mkdir -p {api,services,models,config,utils}
cd ../..

# æŸ¥çœ‹ç»“æ„
tree -L 3
```

**é¢„æœŸç›®å½•ç»“æ„**ï¼š

```
knowledge_platform/
â”œâ”€â”€ knowledge_bases/          # çŸ¥è¯†åº“å­˜æ”¾ï¼ˆè¿è¡Œæ—¶åˆ›å»ºï¼‰
â”œâ”€â”€ data_platform/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ api/             # API è·¯ç”±
â”‚   â”‚   â”œâ”€â”€ services/        # ä¸šåŠ¡é€»è¾‘
â”‚   â”‚   â”œâ”€â”€ models/          # æ•°æ®æ¨¡å‹
â”‚   â”‚   â”œâ”€â”€ config/          # é…ç½®æ–‡ä»¶
â”‚   â”‚   â”œâ”€â”€ utils/           # å·¥å…·å‡½æ•°
â”‚   â”‚   â””â”€â”€ main.py          # FastAPI å…¥å£
â”‚   â””â”€â”€ frontend/            # å‰ç«¯ä»£ç ï¼ˆç¬¬ä¸‰å‘¨å¼€å‘ï¼‰
â”œâ”€â”€ logs/                    # æ—¥å¿—ç›®å½•
â”œâ”€â”€ backups/                 # å¤‡ä»½ç›®å½•
â”œâ”€â”€ tests/                   # æµ‹è¯•ä»£ç 
â”œâ”€â”€ requirements.txt         # Python ä¾èµ–
â”œâ”€â”€ .env                     # ç¯å¢ƒå˜é‡
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

### Step 2: åˆ›å»º Python è™šæ‹Ÿç¯å¢ƒï¼ˆ5åˆ†é’Ÿï¼‰

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3.10 -m venv venv

# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# éªŒè¯ Python ç‰ˆæœ¬
python --version  # åº”æ˜¾ç¤º Python 3.10.x
```

### Step 3: å®‰è£…æ ¸å¿ƒä¾èµ–ï¼ˆ10åˆ†é’Ÿï¼‰

åˆ›å»º `requirements.txt`ï¼š

```txt
# FastAPI æ¡†æ¶
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6

# LangChain ç”Ÿæ€
langchain==0.1.0
langchain-community==0.0.10
langchain-openai==0.0.2

# æ–‡æ¡£å¤„ç†
unstructured==0.11.0
python-magic==0.4.27
pillow==10.1.0
pypdf==3.17.1
python-docx==1.1.0
openpyxl==3.1.2

# å‘é‡æ•°æ®åº“
chromadb==0.4.18

# å·¥å…·åº“
pydantic==2.5.0
pydantic-settings==2.1.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0
watchdog==3.0.0
psutil==5.9.6

# å¯é€‰ï¼šå…¶ä»– embedding æ¨¡å‹
sentence-transformers==2.2.2
```

å®‰è£…ä¾èµ–ï¼š

```bash
pip install -r requirements.txt

# éªŒè¯å®‰è£…
python -c "import fastapi; import langchain; import chromadb; print('âœ… æ‰€æœ‰ä¾èµ–å®‰è£…æˆåŠŸ')"
```

### Step 4: åˆ›å»ºç¯å¢ƒå˜é‡æ–‡ä»¶ï¼ˆ5åˆ†é’Ÿï¼‰

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```bash
# OpenAI API é…ç½®
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_API_BASE=https://api.openai.com/v1  # æˆ–å›½å†…ä»£ç†åœ°å€

# æ•°æ®åº“é…ç½®
CHROMA_DB_PATH=./knowledge_bases

# æœåŠ¡é…ç½®
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=True

# JWT è®¤è¯ï¼ˆå¯é€‰ï¼‰
SECRET_KEY=your-secret-key-here
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=1440

# æ—¥å¿—é…ç½®
LOG_LEVEL=INFO
LOG_FILE=./logs/app.log
```

åˆ›å»º `.gitignore`ï¼š

```bash
# Python
__pycache__/
*.py[cod]
*$py.class
venv/
.env

# æ•°æ®æ–‡ä»¶
knowledge_bases/
logs/
backups/
*.db
*.sqlite

# IDE
.vscode/
.idea/
*.swp
```

### Step 5: åˆ›å»ºé…ç½®ç®¡ç†æ¨¡å—ï¼ˆ20åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/config/settings.py`ï¼š

```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """å…¨å±€é…ç½®"""
    
    # API é…ç½®
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    DEBUG: bool = True
    
    # OpenAI é…ç½®
    OPENAI_API_KEY: str
    OPENAI_API_BASE: Optional[str] = None
    OPENAI_EMBEDDING_MODEL: str = "text-embedding-3-small"
    
    # æ•°æ®åº“é…ç½®
    CHROMA_DB_PATH: str = "./knowledge_bases"
    
    # è®¤è¯é…ç½®
    SECRET_KEY: str = "change-me-in-production"
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 1440
    
    # æ–‡æ¡£å¤„ç†é…ç½®
    DEFAULT_CHUNK_SIZE: int = 1000
    DEFAULT_CHUNK_OVERLAP: int = 200
    ALLOWED_FILE_TYPES: list = [".txt", ".md", ".docx", ".pdf", ".html"]
    MAX_FILE_SIZE_MB: int = 50
    
    # æ—¥å¿—é…ç½®
    LOG_LEVEL: str = "INFO"
    LOG_FILE: str = "./logs/app.log"
    
    class Config:
        env_file = ".env"

# å…¨å±€é…ç½®å®ä¾‹
settings = Settings()
```

**æµ‹è¯•é…ç½®**ï¼š

```bash
python -c "from data_platform.backend.config.settings import settings; print(settings.OPENAI_API_KEY[:10] + '...')"
```

------

## ğŸ§± ç¬¬äºŒé˜¶æ®µï¼šæ ¸å¿ƒæ•°æ®æ¨¡å‹è®¾è®¡

### Step 6: åˆ›å»º Pydantic æ•°æ®æ¨¡å‹ï¼ˆ30åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/models/schemas.py`ï¼š

```python
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
from datetime import datetime
from enum import Enum

# ==================== çŸ¥è¯†åº“ç›¸å…³ ====================
class KnowledgeBaseCreate(BaseModel):
    """åˆ›å»ºçŸ¥è¯†åº“è¯·æ±‚"""
    kb_id: str = Field(..., description="çŸ¥è¯†åº“å”¯ä¸€æ ‡è¯†")
    kb_name: str = Field(..., description="çŸ¥è¯†åº“åç§°")
    description: Optional[str] = None
    embedding_model: str = "text-embedding-3-small"
    chunk_size: int = 1000
    chunk_overlap: int = 200

class KnowledgeBaseInfo(BaseModel):
    """çŸ¥è¯†åº“ä¿¡æ¯"""
    kb_id: str
    kb_name: str
    description: Optional[str]
    created_at: str
    doc_count: int = 0
    chunk_count: int = 0
    db_size_mb: float = 0.0

class KnowledgeBaseConfig(BaseModel):
    """çŸ¥è¯†åº“é…ç½®"""
    kb_id: str
    kb_name: str
    description: str
    created_at: str
    embedding_model: str
    chunk_size: int
    chunk_overlap: int
    db_type: str = "chromadb"
    db_path: str
    allowed_doc_types: List[str]

# ==================== æ–‡æ¡£ç›¸å…³ ====================
class DocumentStatus(str, Enum):
    """æ–‡æ¡£å¤„ç†çŠ¶æ€"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class DocumentInfo(BaseModel):
    """æ–‡æ¡£ä¿¡æ¯"""
    doc_id: str
    kb_id: str
    filename: str
    file_size: int
    status: DocumentStatus
    chunk_count: int = 0
    created_at: str
    processed_at: Optional[str] = None
    error_message: Optional[str] = None

class DocumentMetadata(BaseModel):
    """æ–‡æ¡£å…ƒä¿¡æ¯"""
    doc_id: str
    filename: str
    system: str = "unknown"
    doc_type: str = "unknown"
    version: str = "1.0"
    module: str = "general"
    author: Optional[str] = None
    created_at: str

# ==================== æ£€ç´¢ç›¸å…³ ====================
class RetrieveRequest(BaseModel):
    """æ£€ç´¢è¯·æ±‚"""
    query: str = Field(..., description="æŸ¥è¯¢é—®é¢˜")
    kb_id: str = Field(..., description="çŸ¥è¯†åº“ID")
    top_k: int = Field(5, ge=1, le=20, description="è¿”å›ç»“æœæ•°é‡")
    min_score: float = Field(0.6, ge=0.0, le=1.0, description="æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼")
    filters: Optional[Dict] = Field(None, description="å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶")

class ChunkResult(BaseModel):
    """å•ä¸ªæ£€ç´¢ç»“æœ"""
    content: str
    metadata: Dict
    score: float

class RetrieveResponse(BaseModel):
    """æ£€ç´¢å“åº”"""
    success: bool
    query: str
    kb_id: str
    chunks: List[ChunkResult]
    total: int

# ==================== ç³»ç»Ÿç›¸å…³ ====================
class SystemStats(BaseModel):
    """ç³»ç»Ÿç»Ÿè®¡"""
    cpu_percent: float
    memory_percent: float
    disk_percent: float
    total_kb: int
    total_docs: int
    total_chunks: int
    timestamp: str

class APIResponse(BaseModel):
    """é€šç”¨ API å“åº”"""
    success: bool
    message: str
    data: Optional[Dict] = None
```

------

## ğŸ”§ ç¬¬ä¸‰é˜¶æ®µï¼šæ•°æ®åº“ç®¡ç†å±‚

### Step 7: åˆ›å»ºæ•°æ®åº“ç®¡ç†æœåŠ¡ï¼ˆ45åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/services/db_manager.py`ï¼š

```python
import chromadb
from chromadb.config import Settings as ChromaSettings
from typing import Optional, List, Dict
import os
import json
from datetime import datetime

from ..config.settings import settings

class DatabaseManager:
    """å‘é‡æ•°æ®åº“ç®¡ç†å™¨"""
    
    def __init__(self):
        self.base_path = settings.CHROMA_DB_PATH
        self._clients = {}  # ç¼“å­˜ä¸åŒçŸ¥è¯†åº“çš„ client
    
    def get_client(self, kb_id: str) -> chromadb.ClientAPI:
        """
        è·å–æŒ‡å®šçŸ¥è¯†åº“çš„ ChromaDB å®¢æˆ·ç«¯
        """
        if kb_id in self._clients:
            return self._clients[kb_id]
        
        db_path = os.path.join(self.base_path, kb_id, "db")
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(db_path, exist_ok=True)
        
        # åˆ›å»ºæŒä¹…åŒ–å®¢æˆ·ç«¯
        client = chromadb.PersistentClient(
            path=db_path,
            settings=ChromaSettings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        self._clients[kb_id] = client
        return client
    
    def get_collection(self, kb_id: str, collection_name: Optional[str] = None):
        """
        è·å–æˆ–åˆ›å»º collection
        """
        client = self.get_client(kb_id)
        
        if collection_name is None:
            collection_name = f"{kb_id}_documents"
        
        collection = client.get_or_create_collection(
            name=collection_name,
            metadata={"kb_id": kb_id}
        )
        
        return collection
    
    def create_kb(self, kb_config: dict) -> bool:
        """
        åˆ›å»ºæ–°çŸ¥è¯†åº“
        """
        kb_id = kb_config['kb_id']
        kb_path = os.path.join(self.base_path, kb_id)
        
        # åˆ›å»ºç›®å½•ç»“æ„
        os.makedirs(os.path.join(kb_path, "docs"), exist_ok=True)
        os.makedirs(os.path.join(kb_path, "db"), exist_ok=True)
        
        # ä¿å­˜é…ç½®æ–‡ä»¶
        config_path = os.path.join(kb_path, "config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(kb_config, f, ensure_ascii=False, indent=2)
        
        # åˆå§‹åŒ– collection
        self.get_collection(kb_id)
        
        return True
    
    def delete_kb(self, kb_id: str) -> bool:
        """
        åˆ é™¤çŸ¥è¯†åº“
        """
        import shutil
        
        kb_path = os.path.join(self.base_path, kb_id)
        
        if os.path.exists(kb_path):
            # æ¸…ç†å®¢æˆ·ç«¯ç¼“å­˜
            if kb_id in self._clients:
                del self._clients[kb_id]
            
            # åˆ é™¤ç›®å½•
            shutil.rmtree(kb_path)
            return True
        
        return False
    
    def list_kb(self) -> List[Dict]:
        """
        åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“
        """
        if not os.path.exists(self.base_path):
            return []
        
        kb_list = []
        
        for kb_id in os.listdir(self.base_path):
            kb_path = os.path.join(self.base_path, kb_id)
            config_path = os.path.join(kb_path, "config.json")
            
            if os.path.isdir(kb_path) and os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                # è·å–ç»Ÿè®¡ä¿¡æ¯
                try:
                    collection = self.get_collection(kb_id)
                    chunk_count = collection.count()
                except:
                    chunk_count = 0
                
                kb_list.append({
                    "kb_id": kb_id,
                    "kb_name": config.get('kb_name', kb_id),
                    "description": config.get('description', ''),
                    "chunk_count": chunk_count,
                    "created_at": config.get('created_at', '')
                })
        
        return kb_list
    
    def get_kb_config(self, kb_id: str) -> Optional[Dict]:
        """
        è·å–çŸ¥è¯†åº“é…ç½®
        """
        config_path = os.path.join(self.base_path, kb_id, "config.json")
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        return None
    
    def get_kb_stats(self, kb_id: str) -> Dict:
        """
        è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯
        """
        collection = self.get_collection(kb_id)
        
        # chunk æ•°é‡
        chunk_count = collection.count()
        
        # æ–‡æ¡£æ•°é‡ï¼ˆé€šè¿‡ doc_id å»é‡ï¼‰
        if chunk_count > 0:
            results = collection.get(include=["metadatas"])
            doc_ids = set(m.get('doc_id', '') for m in results['metadatas'])
            doc_count = len(doc_ids)
        else:
            doc_count = 0
        
        # æ•°æ®åº“å¤§å°
        db_path = os.path.join(self.base_path, kb_id, "db")
        db_size = self._get_dir_size(db_path) / (1024 * 1024)  # MB
        
        return {
            "kb_id": kb_id,
            "chunk_count": chunk_count,
            "doc_count": doc_count,
            "db_size_mb": round(db_size, 2)
        }
    
    def _get_dir_size(self, path: str) -> int:
        """
        è·å–ç›®å½•å¤§å°ï¼ˆå­—èŠ‚ï¼‰
        """
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                total_size += os.path.getsize(filepath)
        return total_size

# å…¨å±€æ•°æ®åº“ç®¡ç†å™¨å®ä¾‹
db_manager = DatabaseManager()
```

**æµ‹è¯•æ•°æ®åº“ç®¡ç†å™¨**ï¼š

åˆ›å»º `tests/test_db_manager.py`ï¼š

```python
from data_platform.backend.services.db_manager import db_manager
from datetime import datetime

def test_create_kb():
    """æµ‹è¯•åˆ›å»ºçŸ¥è¯†åº“"""
    kb_config = {
        "kb_id": "test_kb",
        "kb_name": "æµ‹è¯•çŸ¥è¯†åº“",
        "description": "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•çŸ¥è¯†åº“",
        "created_at": datetime.now().isoformat(),
        "embedding_model": "text-embedding-3-small",
        "chunk_size": 1000,
        "chunk_overlap": 200,
        "db_type": "chromadb",
        "db_path": "./knowledge_bases/test_kb/db",
        "allowed_doc_types": [".txt", ".md", ".docx", ".pdf"]
    }
    
    result = db_manager.create_kb(kb_config)
    print(f"âœ… åˆ›å»ºçŸ¥è¯†åº“: {result}")
    
    # åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“
    kb_list = db_manager.list_kb()
    print(f"ğŸ“š çŸ¥è¯†åº“åˆ—è¡¨: {kb_list}")
    
    # è·å–ç»Ÿè®¡ä¿¡æ¯
    stats = db_manager.get_kb_stats("test_kb")
    print(f"ğŸ“Š ç»Ÿè®¡ä¿¡æ¯: {stats}")

if __name__ == "__main__":
    test_create_kb()
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python -m tests.test_db_manager
```

------

## ğŸ“„ ç¬¬å››é˜¶æ®µï¼šæ–‡æ¡£å¤„ç†å¼•æ“ï¼ˆLangChain å±‚ï¼‰

### Step 8: åˆ›å»ºæ–‡æ¡£å¤„ç†æœåŠ¡ï¼ˆ60åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/services/document_processor.py`ï¼š

```python
from langchain.document_loaders import (
    TextLoader,
    UnstructuredMarkdownLoader,
    UnstructuredWordDocumentLoader,
    PyPDFLoader,
    UnstructuredHTMLLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from typing import List, Dict, Optional
import os
import re
from pathlib import Path
from datetime import datetime
import uuid

from ..config.settings import settings
from .db_manager import db_manager

class DocumentProcessor:
    """æ–‡æ¡£å¤„ç†å¼•æ“"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            model=settings.OPENAI_EMBEDDING_MODEL,
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_API_BASE
        )
    
    def process_document(
        self, 
        kb_id: str, 
        file_path: str,
        doc_id: Optional[str] = None
    ) -> Dict:
        """
        å¤„ç†å•ä¸ªæ–‡æ¡£
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            file_path: æ–‡ä»¶è·¯å¾„
            doc_id: æ–‡æ¡£IDï¼ˆå¯é€‰ï¼Œä¸æä¾›åˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰
        
        Returns:
            å¤„ç†ç»“æœå­—å…¸
        """
        try:
            # ç”Ÿæˆæ–‡æ¡£ID
            if doc_id is None:
                doc_id = str(uuid.uuid4())
            
            # 1. åŠ è½½çŸ¥è¯†åº“é…ç½®
            kb_config = db_manager.get_kb_config(kb_id)
            if not kb_config:
                raise Exception(f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
            
            # 2. åŠ è½½æ–‡æ¡£
            documents = self._load_document(file_path)
            
            if not documents:
                raise Exception("æ–‡æ¡£åŠ è½½å¤±è´¥æˆ–å†…å®¹ä¸ºç©º")
            
            # 3. æå–å…ƒä¿¡æ¯
            metadata = self._extract_metadata(file_path, documents[0].page_content)
            metadata['doc_id'] = doc_id
            metadata['kb_id'] = kb_id
            metadata['source'] = file_path
            metadata['processed_at'] = datetime.now().isoformat()
            
            # 4. æ–‡æ¡£åˆ†å—
            chunks = self._split_documents(
                documents,
                chunk_size=kb_config.get('chunk_size', 1000),
                chunk_overlap=kb_config.get('chunk_overlap', 200)
            )
            
            # 5. å‘é‡åŒ–å¹¶å­˜å‚¨
            self._store_chunks(kb_id, chunks, metadata)
            
            return {
                "success": True,
                "doc_id": doc_id,
                "filename": Path(file_path).name,
                "chunk_count": len(chunks),
                "metadata": metadata
            }
        
        except Exception as e:
            return {
                "success": False,
                "doc_id": doc_id,
                "error": str(e)
            }
    
    def _load_document(self, file_path: str) -> List:
        """
        æ ¹æ®æ–‡ä»¶ç±»å‹åŠ è½½æ–‡æ¡£
        """
        ext = Path(file_path).suffix.lower()
        
        loaders = {
            '.txt': TextLoader,
            '.md': UnstructuredMarkdownLoader,
            '.docx': UnstructuredWordDocumentLoader,
            '.pdf': PyPDFLoader,
            '.html': UnstructuredHTMLLoader,
            '.htm': UnstructuredHTMLLoader
        }
        
        loader_class = loaders.get(ext)
        
        if not loader_class:
            raise Exception(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {ext}")
        
        try:
            loader = loader_class(file_path)
            documents = loader.load()
            return documents
        except Exception as e:
            raise Exception(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {str(e)}")
    
    def _split_documents(
        self, 
        documents: List, 
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ) -> List:
        """
        æ–‡æ¡£åˆ†å—
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\n",  # æ®µè½
                "\n",    # è¡Œ
                "ã€‚",    # ä¸­æ–‡å¥å·
                "ï¼",    # æ„Ÿå¹å·
                "ï¼Ÿ",    # é—®å·
                "ï¼›",    # åˆ†å·
                ".",     # è‹±æ–‡å¥å·
                " ",     # ç©ºæ ¼
                ""       # å­—ç¬¦
            ],
            length_function=len
        )
        
        chunks = text_splitter.split_documents(documents)
        return chunks
    
    def _extract_metadata(self, file_path: str, content: str) -> Dict:
        """
        ä»æ–‡ä»¶åå’Œå†…å®¹ä¸­æå–å…ƒä¿¡æ¯
        """
        filename = Path(file_path).stem
        
        metadata = {
            "filename": Path(file_path).name,
            "doc_type": "unknown",
            "version": "1.0",
            "system": "unknown",
            "module": "general",
            "author": None
        }
        
        # ä»æ–‡ä»¶åæå–ï¼ˆä¾‹å¦‚ï¼šCRM_è®¾è®¡æ–‡æ¡£_ç™»å½•æ¨¡å—_v3.0.docxï¼‰
        pattern = r'([A-Z]+)_(.+?)_(.+?)_v([\d.]+)'
        match = re.search(pattern, filename)
        
        if match:
            metadata['system'] = match.group(1)
            metadata['doc_type'] = match.group(2)
            metadata['module'] = match.group(3)
            metadata['version'] = match.group(4)
        else:
            # ç®€å•æ¨¡å¼ï¼šç³»ç»Ÿ_æ–‡æ¡£ç±»å‹_ç‰ˆæœ¬
            simple_pattern = r'([A-Z]+)_(.+?)_v([\d.]+)'
            simple_match = re.search(simple_pattern, filename)
            if simple_match:
                metadata['system'] = simple_match.group(1)
                metadata['doc_type'] = simple_match.group(2)
                metadata['version'] = simple_match.group(3)
        
        # ä»å†…å®¹æå–æ–‡æ¡£ç±»å‹
        content_preview = content[:1000]
        
        doc_type_keywords = {
            'éœ€æ±‚æ–‡æ¡£': ['éœ€æ±‚', 'åŠŸèƒ½éœ€æ±‚', 'ç”¨æˆ·éœ€æ±‚', 'ä¸šåŠ¡éœ€æ±‚'],
            'è®¾è®¡æ–‡æ¡£': ['è®¾è®¡', 'æ¶æ„è®¾è®¡', 'è¯¦ç»†è®¾è®¡', 'æ¦‚è¦è®¾è®¡'],
            'è¿ç»´æ–‡æ¡£': ['è¿ç»´', 'éƒ¨ç½²', 'é…ç½®', 'ç›‘æ§', 'æ•…éšœ'],
            'APIæ–‡æ¡£': ['API', 'æ¥å£', 'endpoint', 'è¯·æ±‚', 'å“åº”']
        }
        
        for doc_type, keywords in doc_type_keywords.items():
            if any(kw in content_preview for kw in keywords):
                metadata['doc_type'] = doc_type
                break
        
        return metadata
    
    def _store_chunks(self, kb_id: str, chunks: List, base_metadata: Dict):
        """
        å‘é‡åŒ–å¹¶å­˜å‚¨åˆ°æ•°æ®åº“
        """
        collection = db_manager.get_collection(kb_id)
        
        for i, chunk in enumerate(chunks):
            # åˆå¹¶å…ƒä¿¡æ¯
            chunk_metadata = {
                **base_metadata,
                "chunk_index": i,
                "chunk_total": len(chunks)
            }
            
            # å‘é‡åŒ–
            embedding = self.embeddings.embed_query(chunk.page_content)
            
            # ç”Ÿæˆå”¯ä¸€ ID
            chunk_id = f"{base_metadata['doc_id']}_chunk_{i}"
            
            # å­˜å‚¨
            collection.add(
                embeddings=[embedding],
                documents=[chunk.page_content],
                metadatas=[chunk_metadata],
                ids=[chunk_id]
            )
    
    def delete_document(self, kb_id: str, doc_id: str) -> bool:
        """
        åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰ chunks
        """
        try:
            collection = db_manager.get_collection(kb_id)
            
            # æŸ¥æ‰¾è¯¥æ–‡æ¡£çš„æ‰€æœ‰ chunks
            results = collection.get(
                where={"doc_id": doc_id}
            )
            
            if results['ids']:
                collection.delete(ids=results['ids'])
                return True
            
            return False
        
        except Exception as e:
            print(f"åˆ é™¤æ–‡æ¡£å¤±è´¥: {e}")
            return False

# å…¨å±€æ–‡æ¡£å¤„ç†å™¨å®ä¾‹
doc_processor = DocumentProcessor()
```

**æµ‹è¯•æ–‡æ¡£å¤„ç†**ï¼š

åˆ›å»º `tests/test_document_processor.py`ï¼š

```python
from data_platform.backend.services.document_processor import doc_processor
from data_platform.backend.services.db_manager import db_manager

def test_process_document():
    """æµ‹è¯•æ–‡æ¡£å¤„ç†"""
    
    # åˆ›å»ºæµ‹è¯•æ–‡æ¡£
    test_file = "./test_doc.txt"
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write("""
# CRM ç³»ç»Ÿç™»å½•æ¨¡å—è®¾è®¡æ–‡æ¡£ v2.0

## 1. æ¦‚è¿°
æœ¬æ–‡æ¡£æè¿° CRM ç³»ç»Ÿç™»å½•æ¨¡å—çš„è¯¦ç»†è®¾è®¡ã€‚

## 2. åŠŸèƒ½éœ€æ±‚
### 2.1 ç”¨æˆ·åå¯†ç ç™»å½•
ç”¨æˆ·å¯ä»¥ä½¿ç”¨ç”¨æˆ·åå’Œå¯†ç è¿›è¡Œç™»å½•ã€‚

### 2.2 æ‰‹æœºéªŒè¯ç ç™»å½•
ç”¨æˆ·å¯ä»¥ä½¿ç”¨æ‰‹æœºå·æ¥æ”¶éªŒè¯ç ç™»å½•ã€‚

## 3. æŠ€æœ¯å®ç°
### 3.1 å¯†ç åŠ å¯†
ä½¿ç”¨ bcrypt ç®—æ³•åŠ å¯†å­˜å‚¨å¯†ç ã€‚

### 3.2 Token ç”Ÿæˆ
ä½¿ç”¨ JWT ç”Ÿæˆè®¿é—®ä»¤ç‰Œã€‚
        """)
    
    # å¤„ç†æ–‡æ¡£
    result = doc_processor.process_document(
        kb_id="test_kb",
        file_path=test_file
    )
    
    print(f"âœ… å¤„ç†ç»“æœ: {result}")
    
    # æŸ¥çœ‹ç»Ÿè®¡
    stats = db_manager.get_kb_stats("test_kb")
    print(f"ğŸ“Š çŸ¥è¯†åº“ç»Ÿè®¡: {stats}")
    
    # æ¸…ç†æµ‹è¯•æ–‡ä»¶
    import os
    os.remove(test_file)

if __name__ == "__main__":
    test_process_document()
```

------

### ğŸ’¡ ç¬¬ä¸€é˜¶æ®µæ€»ç»“æ£€æŸ¥ç‚¹

åˆ°è¿™é‡Œï¼Œä½ åº”è¯¥å·²ç»å®Œæˆï¼š

- âœ… é¡¹ç›®ç›®å½•ç»“æ„
- âœ… Python ç¯å¢ƒä¸ä¾èµ–å®‰è£…
- âœ… é…ç½®ç®¡ç†æ¨¡å—
- âœ… æ•°æ®æ¨¡å‹å®šä¹‰
- âœ… æ•°æ®åº“ç®¡ç†æœåŠ¡
- âœ… æ–‡æ¡£å¤„ç†å¼•æ“

**éªŒè¯æµ‹è¯•**ï¼š

```bash
# è¿è¡Œæ‰€æœ‰æµ‹è¯•
python -m tests.test_db_manager
python -m tests.test_document_processor

# åº”è¯¥çœ‹åˆ°ï¼š
# âœ… åˆ›å»ºçŸ¥è¯†åº“æˆåŠŸ
# âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ
# ğŸ“Š chunk æ•°é‡ > 0
```

------

## ğŸŒ ç¬¬äº”é˜¶æ®µï¼šæ•°æ®ä¸­å° API å¼€å‘

### Step 9: åˆ›å»ºæ£€ç´¢æœåŠ¡ï¼ˆ40åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/services/retriever.py`ï¼š

```python
from typing import List, Dict, Optional
from langchain.embeddings import OpenAIEmbeddings

from ..config.settings import settings
from .db_manager import db_manager

class RetrieverService:
    """æ£€ç´¢æœåŠ¡"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            model=settings.OPENAI_EMBEDDING_MODEL,
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_API_BASE
        )
    
    def retrieve(
        self,
        kb_id: str,
        query: str,
        top_k: int = 5,
        min_score: float = 0.6,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """
        è¯­ä¹‰æ£€ç´¢
        
        Args:
            kb_id: çŸ¥è¯†åº“ID
            query: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            min_score: æœ€ä½ç›¸ä¼¼åº¦é˜ˆå€¼
            filters: å…ƒæ•°æ®è¿‡æ»¤æ¡ä»¶
        
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        try:
            # è·å– collection
            collection = db_manager.get_collection(kb_id)
            
            # å‘é‡åŒ–æŸ¥è¯¢
            query_embedding = self.embeddings.embed_query(query)
            
            # æ£€ç´¢
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k * 2,  # å¤šå–ä¸€äº›ï¼Œåé¢è¿‡æ»¤
                where=filters,
                include=["documents", "metadatas", "distances"]
            )
            
            # æ ¼å¼åŒ–ç»“æœ
            chunks = []
            
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    # ChromaDB è¿”å›çš„æ˜¯è·ç¦»ï¼ˆè¶Šå°è¶Šç›¸ä¼¼ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºç›¸ä¼¼åº¦åˆ†æ•°
                    distance = results['distances'][0][i]
                    score = 1 / (1 + distance)  # è½¬æ¢ä¸º 0-1 ä¹‹é—´çš„åˆ†æ•°
                    
                    if score >= min_score:
                        chunks.append({
                            "content": doc,
                            "metadata": results['metadatas'][0][i],
                            "score": round(score, 4)
                        })
            
            # æŒ‰åˆ†æ•°æ’åºå¹¶é™åˆ¶æ•°é‡
            chunks.sort(key=lambda x: x['score'], reverse=True)
            
            return chunks[:top_k]
        
        except Exception as e:
            print(f"æ£€ç´¢å¤±è´¥: {e}")
            return []
    
    def multi_kb_retrieve(
        self,
        kb_ids: List[str],
        query: str,
        top_k: int = 5,
        merge_strategy: str = "score"
    ) -> List[Dict]:
        """
        å¤šçŸ¥è¯†åº“è”åˆæ£€ç´¢
        
        Args:
            kb_ids: çŸ¥è¯†åº“IDåˆ—è¡¨
            query: æŸ¥è¯¢é—®é¢˜
            top_k: è¿”å›ç»“æœæ•°é‡
            merge_strategy: åˆå¹¶ç­–ç•¥ (score/round_robin/kb_priority)
        
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        all_results = []
        
        # ä»æ¯ä¸ªçŸ¥è¯†åº“æ£€ç´¢
        for kb_id in kb_ids:
            kb_results = self.retrieve(
                kb_id=kb_id,
                query=query,
                top_k=top_k
            )
            
            # æ·»åŠ æ¥æºæ ‡è¯†
            for result in kb_results:
                result['metadata']['source_kb'] = kb_id
                all_results.append(result)
        
        # åˆå¹¶ç­–ç•¥
        if merge_strategy == "score":
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            all_results.sort(key=lambda x: x['score'], reverse=True)
            return all_results[:top_k]
        
        elif merge_strategy == "round_robin":
            # è½®æµå–æ¯ä¸ªçŸ¥è¯†åº“çš„ç»“æœ
            final_results = []
            kb_indices = {kb_id: 0 for kb_id in kb_ids}
            
            while len(final_results) < top_k and any(kb_indices.values()):
                for kb_id in kb_ids:
                    kb_results = [r for r in all_results if r['metadata']['source_kb'] == kb_id]
                    idx = kb_indices[kb_id]
                    if idx < len(kb_results):
                        final_results.append(kb_results[idx])
                        kb_indices[kb_id] += 1
                        if len(final_results) >= top_k:
                            break
            
            return final_results
        
        else:  # kb_priority
            # æŒ‰çŸ¥è¯†åº“é¡ºåºä¼˜å…ˆ
            final_results = []
            for kb_id in kb_ids:
                kb_results = [r for r in all_results if r['metadata']['source_kb'] == kb_id]
                final_results.extend(kb_results)
                if len(final_results) >= top_k:
                    break
            
            return final_results[:top_k]

# å…¨å±€æ£€ç´¢æœåŠ¡å®ä¾‹
retriever = RetrieverService()
```

### Step 10: åˆ›å»º API è·¯ç”± - çŸ¥è¯†åº“ç®¡ç†ï¼ˆ30åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/api/kb_management.py`ï¼š

```python
from fastapi import APIRouter, HTTPException, status
from typing import List

from ..models.schemas import (
    KnowledgeBaseCreate,
    KnowledgeBaseInfo,
    APIResponse
)
from ..services.db_manager import db_manager
from datetime import datetime

router = APIRouter(prefix="/api/kb", tags=["çŸ¥è¯†åº“ç®¡ç†"])

@router.post("/create", response_model=APIResponse)
async def create_kb(kb_data: KnowledgeBaseCreate):
    """åˆ›å»ºæ–°çŸ¥è¯†åº“"""
    try:
        # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
        existing = db_manager.get_kb_config(kb_data.kb_id)
        if existing:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"çŸ¥è¯†åº“ {kb_data.kb_id} å·²å­˜åœ¨"
            )
        
        # æ„å»ºé…ç½®
        kb_config = {
            "kb_id": kb_data.kb_id,
            "kb_name": kb_data.kb_name,
            "description": kb_data.description or "",
            "created_at": datetime.now().isoformat(),
            "embedding_model": kb_data.embedding_model,
            "chunk_size": kb_data.chunk_size,
            "chunk_overlap": kb_data.chunk_overlap,
            "db_type": "chromadb",
            "db_path": f"./knowledge_bases/{kb_data.kb_id}/db",
            "allowed_doc_types": [".txt", ".md", ".docx", ".pdf", ".html"]
        }
        
        # åˆ›å»ºçŸ¥è¯†åº“
        success = db_manager.create_kb(kb_config)
        
        if success:
            return APIResponse(
                success=True,
                message=f"çŸ¥è¯†åº“ {kb_data.kb_name} åˆ›å»ºæˆåŠŸ",
                data={"kb_id": kb_data.kb_id}
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="çŸ¥è¯†åº“åˆ›å»ºå¤±è´¥"
            )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/list", response_model=List[KnowledgeBaseInfo])
async def list_kb():
    """è·å–æ‰€æœ‰çŸ¥è¯†åº“åˆ—è¡¨"""
    try:
        kb_list = db_manager.list_kb()
        
        result = []
        for kb in kb_list:
            stats = db_manager.get_kb_stats(kb['kb_id'])
            
            result.append(KnowledgeBaseInfo(
                kb_id=kb['kb_id'],
                kb_name=kb['kb_name'],
                description=kb.get('description', ''),
                created_at=kb.get('created_at', ''),
                doc_count=stats.get('doc_count', 0),
                chunk_count=stats.get('chunk_count', 0),
                db_size_mb=stats.get('db_size_mb', 0.0)
            ))
        
        return result
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/{kb_id}/info", response_model=KnowledgeBaseInfo)
async def get_kb_info(kb_id: str):
    """è·å–æŒ‡å®šçŸ¥è¯†åº“è¯¦æƒ…"""
    try:
        config = db_manager.get_kb_config(kb_id)
        if not config:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨"
            )
        
        stats = db_manager.get_kb_stats(kb_id)
        
        return KnowledgeBaseInfo(
            kb_id=config['kb_id'],
            kb_name=config['kb_name'],
            description=config.get('description', ''),
            created_at=config.get('created_at', ''),
            doc_count=stats.get('doc_count', 0),
            chunk_count=stats.get('chunk_count', 0),
            db_size_mb=stats.get('db_size_mb', 0.0)
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.delete("/{kb_id}", response_model=APIResponse)
async def delete_kb(kb_id: str):
    """åˆ é™¤çŸ¥è¯†åº“"""
    try:
        success = db_manager.delete_kb(kb_id)
        
        if success:
            return APIResponse(
                success=True,
                message=f"çŸ¥è¯†åº“ {kb_id} åˆ é™¤æˆåŠŸ"
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨"
            )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/{kb_id}/stats")
async def get_kb_stats(kb_id: str):
    """è·å–çŸ¥è¯†åº“ç»Ÿè®¡ä¿¡æ¯"""
    try:
        stats = db_manager.get_kb_stats(kb_id)
        return stats
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )
```

### Step 11: åˆ›å»º API è·¯ç”± - æ–‡æ¡£ç®¡ç†ï¼ˆ40åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/api/document_management.py`ï¼š

```python
from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from typing import List
import os
import shutil
import uuid

from ..models.schemas import APIResponse, DocumentInfo, DocumentStatus
from ..services.document_processor import doc_processor
from ..services.db_manager import db_manager
from ..config.settings import settings
from datetime import datetime

router = APIRouter(prefix="/api/kb/{kb_id}/documents", tags=["æ–‡æ¡£ç®¡ç†"])

# å­˜å‚¨æ–‡æ¡£å¤„ç†çŠ¶æ€
document_status = {}

@router.post("/upload", response_model=APIResponse)
async def upload_documents(
    kb_id: str,
    files: List[UploadFile] = File(...),
    background_tasks: BackgroundTasks = None
):
    """
    ä¸Šä¼ æ–‡æ¡£åˆ°æŒ‡å®šçŸ¥è¯†åº“
    æ”¯æŒæ‰¹é‡ä¸Šä¼ ï¼Œå¼‚æ­¥å¤„ç†
    """
    try:
        # éªŒè¯çŸ¥è¯†åº“æ˜¯å¦å­˜åœ¨
        config = db_manager.get_kb_config(kb_id)
        if not config:
            raise HTTPException(status_code=404, detail=f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨")
        
        results = []
        
        for file in files:
            # éªŒè¯æ–‡ä»¶ç±»å‹
            file_ext = os.path.splitext(file.filename)[1].lower()
            if file_ext not in settings.ALLOWED_FILE_TYPES:
                results.append({
                    "filename": file.filename,
                    "status": "failed",
                    "error": f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_ext}"
                })
                continue
            
            # éªŒè¯æ–‡ä»¶å¤§å°
            file.file.seek(0, 2)  # ç§»åŠ¨åˆ°æ–‡ä»¶æœ«å°¾
            file_size = file.file.tell()
            file.file.seek(0)  # é‡ç½®åˆ°å¼€å¤´
            
            if file_size > settings.MAX_FILE_SIZE_MB * 1024 * 1024:
                results.append({
                    "filename": file.filename,
                    "status": "failed",
                    "error": f"æ–‡ä»¶è¿‡å¤§ï¼Œè¶…è¿‡ {settings.MAX_FILE_SIZE_MB}MB"
                })
                continue
            
            # ç”Ÿæˆæ–‡æ¡£ ID
            doc_id = str(uuid.uuid4())
            
            # ä¿å­˜æ–‡ä»¶
            docs_dir = os.path.join(settings.CHROMA_DB_PATH, kb_id, "docs")
            os.makedirs(docs_dir, exist_ok=True)
            
            file_path = os.path.join(docs_dir, file.filename)
            
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            
            # åˆå§‹åŒ–çŠ¶æ€
            document_status[doc_id] = {
                "doc_id": doc_id,
                "kb_id": kb_id,
                "filename": file.filename,
                "file_size": file_size,
                "status": DocumentStatus.PENDING,
                "created_at": datetime.now().isoformat()
            }
            
            # æ·»åŠ åˆ°åå°ä»»åŠ¡
            background_tasks.add_task(
                process_document_task,
                kb_id=kb_id,
                doc_id=doc_id,
                file_path=file_path
            )
            
            results.append({
                "doc_id": doc_id,
                "filename": file.filename,
                "status": "processing"
            })
        
        return APIResponse(
            success=True,
            message=f"æˆåŠŸä¸Šä¼  {len(results)} ä¸ªæ–‡ä»¶",
            data={"documents": results}
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def process_document_task(kb_id: str, doc_id: str, file_path: str):
    """
    åå°å¤„ç†æ–‡æ¡£ä»»åŠ¡
    """
    try:
        # æ›´æ–°çŠ¶æ€
        document_status[doc_id]["status"] = DocumentStatus.PROCESSING
        
        # å¤„ç†æ–‡æ¡£
        result = doc_processor.process_document(kb_id, file_path, doc_id)
        
        if result["success"]:
            document_status[doc_id].update({
                "status": DocumentStatus.COMPLETED,
                "chunk_count": result["chunk_count"],
                "processed_at": datetime.now().isoformat(),
                "metadata": result.get("metadata", {})
            })
        else:
            document_status[doc_id].update({
                "status": DocumentStatus.FAILED,
                "error_message": result.get("error", "æœªçŸ¥é”™è¯¯")
            })
    
    except Exception as e:
        document_status[doc_id].update({
            "status": DocumentStatus.FAILED,
            "error_message": str(e)
        })

@router.get("", response_model=List[DocumentInfo])
async def list_documents(kb_id: str):
    """è·å–çŸ¥è¯†åº“çš„æ–‡æ¡£åˆ—è¡¨"""
    try:
        # ä»çŠ¶æ€å­—å…¸è·å–
        kb_docs = [
            DocumentInfo(**doc) 
            for doc in document_status.values() 
            if doc['kb_id'] == kb_id
        ]
        
        return kb_docs
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{doc_id}", response_model=DocumentInfo)
async def get_document_info(kb_id: str, doc_id: str):
    """è·å–æ–‡æ¡£è¯¦æƒ…"""
    try:
        if doc_id not in document_status:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        doc = document_status[doc_id]
        
        if doc['kb_id'] != kb_id:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸åœ¨è¯¥çŸ¥è¯†åº“ä¸­")
        
        return DocumentInfo(**doc)
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/{doc_id}", response_model=APIResponse)
async def delete_document(kb_id: str, doc_id: str):
    """åˆ é™¤æ–‡æ¡£"""
    try:
        # ä»å‘é‡æ•°æ®åº“åˆ é™¤
        success = doc_processor.delete_document(kb_id, doc_id)
        
        if success:
            # åˆ é™¤çŠ¶æ€è®°å½•
            if doc_id in document_status:
                filename = document_status[doc_id]['filename']
                del document_status[doc_id]
                
                # åˆ é™¤åŸå§‹æ–‡ä»¶
                file_path = os.path.join(
                    settings.CHROMA_DB_PATH, 
                    kb_id, 
                    "docs", 
                    filename
                )
                if os.path.exists(file_path):
                    os.remove(file_path)
            
            return APIResponse(
                success=True,
                message=f"æ–‡æ¡£ {doc_id} åˆ é™¤æˆåŠŸ"
            )
        else:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/{doc_id}/reprocess", response_model=APIResponse)
async def reprocess_document(
    kb_id: str, 
    doc_id: str,
    background_tasks: BackgroundTasks
):
    """é‡æ–°å¤„ç†æ–‡æ¡£"""
    try:
        if doc_id not in document_status:
            raise HTTPException(status_code=404, detail="æ–‡æ¡£ä¸å­˜åœ¨")
        
        doc = document_status[doc_id]
        
        # å…ˆåˆ é™¤æ—§æ•°æ®
        doc_processor.delete_document(kb_id, doc_id)
        
        # é‡æ–°å¤„ç†
        file_path = os.path.join(
            settings.CHROMA_DB_PATH,
            kb_id,
            "docs",
            doc['filename']
        )
        
        background_tasks.add_task(
            process_document_task,
            kb_id=kb_id,
            doc_id=doc_id,
            file_path=file_path
        )
        
        return APIResponse(
            success=True,
            message="æ–‡æ¡£å·²åŠ å…¥é‡æ–°å¤„ç†é˜Ÿåˆ—"
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Step 12: åˆ›å»º API è·¯ç”± - æ£€ç´¢æ¥å£ï¼ˆ30åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/api/retrieve.py`ï¼š

```python
from fastapi import APIRouter, HTTPException
from typing import List

from ..models.schemas import RetrieveRequest, RetrieveResponse, ChunkResult
from ..services.retriever import retriever

router = APIRouter(prefix="/api", tags=["æ£€ç´¢"])

@router.post("/retrieve", response_model=RetrieveResponse)
async def retrieve_knowledge(request: RetrieveRequest):
    """
    è¯­ä¹‰æ£€ç´¢æ¥å£ï¼ˆä¾› Dify è°ƒç”¨ï¼‰
    """
    try:
        chunks = retriever.retrieve(
            kb_id=request.kb_id,
            query=request.query,
            top_k=request.top_k,
            min_score=request.min_score,
            filters=request.filters
        )
        
        chunk_results = [ChunkResult(**chunk) for chunk in chunks]
        
        return RetrieveResponse(
            success=True,
            query=request.query,
            kb_id=request.kb_id,
            chunks=chunk_results,
            total=len(chunk_results)
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/retrieve/multi")
async def multi_kb_retrieve(
    query: str,
    kb_ids: List[str],
    top_k: int = 5,
    merge_strategy: str = "score"
):
    """
    å¤šçŸ¥è¯†åº“è”åˆæ£€ç´¢
    """
    try:
        chunks = retriever.multi_kb_retrieve(
            kb_ids=kb_ids,
            query=query,
            top_k=top_k,
            merge_strategy=merge_strategy
        )
        
        return {
            "success": True,
            "query": query,
            "kb_ids": kb_ids,
            "chunks": chunks,
            "total": len(chunks)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Step 13: åˆ›å»º FastAPI ä¸»å…¥å£ï¼ˆ20åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/main.py`ï¼š

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging
from datetime import datetime

from .config.settings import settings
from .api import kb_management, document_management, retrieve

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=settings.LOG_LEVEL,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(settings.LOG_FILE),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# åˆ›å»º FastAPI åº”ç”¨
app = FastAPI(
    title="çŸ¥è¯†ä¸­å° API",
    description="LangChain + Dify ä¼ä¸šçº§çŸ¥è¯†ä¸­å°",
    version="1.0.0"
)

# é…ç½® CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # ç”Ÿäº§ç¯å¢ƒåº”è¯¥é™åˆ¶å…·ä½“åŸŸå
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# æ³¨å†Œè·¯ç”±
app.include_router(kb_management.router)
app.include_router(document_management.router)
app.include_router(retrieve.router)

@app.get("/")
async def root():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "running",
        "service": "çŸ¥è¯†ä¸­å° API",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

@app.on_event("startup")
async def startup_event():
    """å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ çŸ¥è¯†ä¸­å° API å¯åŠ¨æˆåŠŸ")
    logger.info(f"ğŸ“ API åœ°å€: http://{settings.API_HOST}:{settings.API_PORT}")
    logger.info(f"ğŸ“š API æ–‡æ¡£: http://{settings.API_HOST}:{settings.API_PORT}/docs")

@app.on_event("shutdown")
async def shutdown_event():
    """å…³é—­äº‹ä»¶"""
    logger.info("ğŸ‘‹ çŸ¥è¯†ä¸­å° API å…³é—­")

if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host=settings.API_HOST,
        port=settings.API_PORT,
        reload=settings.DEBUG
    )
```

### Step 14: å¯åŠ¨å¹¶æµ‹è¯•åç«¯ APIï¼ˆ30åˆ†é’Ÿï¼‰

**å¯åŠ¨æœåŠ¡**ï¼š

```bash
cd data_platform/backend
python main.py

# æˆ–ä½¿ç”¨ uvicorn
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**è®¿é—® API æ–‡æ¡£**ï¼š æµè§ˆå™¨æ‰“å¼€ `http://localhost:8000/docs`

**API æµ‹è¯•è„šæœ¬**ï¼š

åˆ›å»º `tests/test_api.py`ï¼š

```python
import requests
import json

BASE_URL = "http://localhost:8000"

def test_create_kb():
    """æµ‹è¯•åˆ›å»ºçŸ¥è¯†åº“"""
    url = f"{BASE_URL}/api/kb/create"
    
    data = {
        "kb_id": "test_crm",
        "kb_name": "CRMç³»ç»ŸçŸ¥è¯†åº“",
        "description": "åŒ…å«CRMç³»ç»Ÿçš„éœ€æ±‚ã€è®¾è®¡ã€è¿ç»´æ–‡æ¡£",
        "embedding_model": "text-embedding-3-small",
        "chunk_size": 1000,
        "chunk_overlap": 200
    }
    
    response = requests.post(url, json=data)
    print(f"âœ… åˆ›å»ºçŸ¥è¯†åº“: {response.json()}")
    return response.json()

def test_list_kb():
    """æµ‹è¯•è·å–çŸ¥è¯†åº“åˆ—è¡¨"""
    url = f"{BASE_URL}/api/kb/list"
    
    response = requests.get(url)
    print(f"ğŸ“š çŸ¥è¯†åº“åˆ—è¡¨: {json.dumps(response.json(), indent=2, ensure_ascii=False)}")
    return response.json()

def test_upload_document():
    """æµ‹è¯•ä¸Šä¼ æ–‡æ¡£"""
    url = f"{BASE_URL}/api/kb/test_crm/documents/upload"
    
    # åˆ›å»ºæµ‹è¯•æ–‡ä»¶
    test_content = """
    # CRMç³»ç»Ÿç™»å½•æ¨¡å—è®¾è®¡ v2.0
    
    ## åŠŸèƒ½è¯´æ˜
    ç³»ç»Ÿæ”¯æŒä¸‰ç§ç™»å½•æ–¹å¼ï¼š
    1. ç”¨æˆ·åå¯†ç ç™»å½•
    2. æ‰‹æœºéªŒè¯ç ç™»å½•  
    3. ä¼ä¸šå¾®ä¿¡æ‰«ç ç™»å½•
    """
    
    files = {
        'files': ('CRM_è®¾è®¡æ–‡æ¡£_ç™»å½•æ¨¡å—_v2.0.txt', test_content.encode('utf-8'))
    }
    
    response = requests.post(url, files=files)
    print(f"ğŸ“„ ä¸Šä¼ æ–‡æ¡£: {response.json()}")
    return response.json()

def test_retrieve():
    """æµ‹è¯•æ£€ç´¢"""
    import time
    time.sleep(3)  # ç­‰å¾…æ–‡æ¡£å¤„ç†å®Œæˆ
    
    url = f"{BASE_URL}/api/retrieve"
    
    data = {
        "query": "CRMç³»ç»Ÿæœ‰å“ªäº›ç™»å½•æ–¹å¼ï¼Ÿ",
        "kb_id": "test_crm",
        "top_k": 3
    }
    
    response = requests.post(url, json=data)
    result = response.json()
    
    print(f"\nğŸ” æ£€ç´¢ç»“æœ:")
    print(f"é—®é¢˜: {result['query']}")
    print(f"æ‰¾åˆ° {result['total']} æ¡ç›¸å…³å†…å®¹:\n")
    
    for i, chunk in enumerate(result['chunks'], 1):
        print(f"{i}. [ç›¸ä¼¼åº¦: {chunk['score']}]")
        print(f"   å†…å®¹: {chunk['content'][:100]}...")
        print(f"   å…ƒä¿¡æ¯: {chunk['metadata']}\n")
    
    return result

if __name__ == "__main__":
    print("=" * 50)
    print("å¼€å§‹æµ‹è¯•çŸ¥è¯†ä¸­å° API")
    print("=" * 50)
    
    # 1. åˆ›å»ºçŸ¥è¯†åº“
    test_create_kb()
    
    # 2. æŸ¥çœ‹çŸ¥è¯†åº“åˆ—è¡¨
    test_list_kb()
    
    # 3. ä¸Šä¼ æ–‡æ¡£
    test_upload_document()
    
    # 4. æ£€ç´¢æµ‹è¯•
    test_retrieve()
    
    print("\n" + "=" * 50)
    print("âœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼")
    print("=" * 50)
```

è¿è¡Œæµ‹è¯•ï¼š

```bash
python -m tests.test_api
```

------

## ğŸ’¡ ç¬¬äºŒé˜¶æ®µæ€»ç»“æ£€æŸ¥ç‚¹

åˆ°è¿™é‡Œï¼Œä½ çš„åç«¯ API åº”è¯¥å·²ç»å®Œå…¨å¯ç”¨ï¼š

- âœ… çŸ¥è¯†åº“ CRUD æ¥å£
- âœ… æ–‡æ¡£ä¸Šä¼ ä¸å¤„ç†
- âœ… è¯­ä¹‰æ£€ç´¢æ¥å£
- âœ… å¤šçŸ¥è¯†åº“è”åˆæ£€ç´¢

**éªŒè¯**ï¼š

1. API æ–‡æ¡£å¯è®¿é—®ï¼šhttp://localhost:8000/docs
2. æ‰€æœ‰æµ‹è¯•é€šè¿‡
3. èƒ½åˆ›å»ºçŸ¥è¯†åº“ã€ä¸Šä¼ æ–‡æ¡£ã€æ£€ç´¢å†…å®¹

------

## ğŸ”— ç¬¬å…­é˜¶æ®µï¼šDify å¯¹æ¥

### Step 15: åœ¨ Dify ä¸­é…ç½®è‡ªå®šä¹‰å·¥å…·ï¼ˆ20åˆ†é’Ÿï¼‰

**1. ç™»å½• Dify æ§åˆ¶å°**

è®¿é—®ä½ çš„ Dify å®ä¾‹ï¼ˆcloud.dify.ai æˆ–è‡ªéƒ¨ç½²åœ°å€ï¼‰

**2. åˆ›å»ºè‡ªå®šä¹‰å·¥å…·**

è¿›å…¥ã€Œå·¥å…·ã€â†’ã€Œåˆ›å»ºè‡ªå®šä¹‰å·¥å…·ã€â†’ã€ŒAPI å·¥å…·ã€

**å·¥å…·é…ç½®ï¼ˆJSON æ ¼å¼ï¼‰**ï¼š

```json
{
  "openapi": "3.0.0",
  "info": {
    "title": "çŸ¥è¯†ä¸­å°æ£€ç´¢å·¥å…·",
    "description": "ä»ä¼ä¸šçŸ¥è¯†ä¸­å°æ£€ç´¢ç›¸å…³æ–‡æ¡£å†…å®¹",
    "version": "1.0.0"
  },
  "servers": [
    {
      "url": "http://your-server-ip:8000"
    }
  ],
  "paths": {
    "/api/retrieve": {
      "post": {
        "summary": "è¯­ä¹‰æ£€ç´¢",
        "description": "æ ¹æ®é—®é¢˜æ£€ç´¢çŸ¥è¯†åº“ä¸­çš„ç›¸å…³å†…å®¹",
        "operationId": "retrieve_knowledge",
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "query": {
                    "type": "string",
                    "description": "ç”¨æˆ·çš„é—®é¢˜"
                  },
                  "kb_id": {
                    "type": "string",
                    "description": "çŸ¥è¯†åº“ID",
                    "default": "test_crm"
                  },
                  "top_k": {
                    "type": "integer",
                    "description": "è¿”å›ç»“æœæ•°é‡",
                    "default": 5
                  }
                },
                "required": ["query", "kb_id"]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "æ£€ç´¢æˆåŠŸ",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "success": {
                      "type": "boolean"
                    },
                    "chunks": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "content": {
                            "type": "string"
                          },
                          "score": {
                            "type": "number"
                          }
                        }
                      }
                    },
                    "total": {
                      "type": "integer"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

**3. åœ¨ Dify Workflow ä¸­ä½¿ç”¨å·¥å…·**

åˆ›å»ºä¸€ä¸ªæ–°çš„ Chatflow æˆ– Workflowï¼š

```
èŠ‚ç‚¹1: å¼€å§‹èŠ‚ç‚¹
  â†“ æ¥æ”¶ç”¨æˆ·è¾“å…¥
èŠ‚ç‚¹2: å·¥å…·è°ƒç”¨èŠ‚ç‚¹ï¼ˆçŸ¥è¯†ä¸­å°æ£€ç´¢ï¼‰
  - å·¥å…·: knowledge_platform_retrieve
  - å‚æ•°é…ç½®:
    * query: {{sys.query}}  (ç”¨æˆ·é—®é¢˜)
    * kb_id: test_crm
    * top_k: 5
  â†“ è¾“å‡º: chunks (æ£€ç´¢ç»“æœ)
èŠ‚ç‚¹3: ä»£ç èŠ‚ç‚¹ï¼ˆæ ¼å¼åŒ–ä¸Šä¸‹æ–‡ï¼‰
  - è¾“å…¥: {{retrieve.chunks}}
  - ä»£ç :
    ```python
    def main(chunks):
        context = "\n\n".join([
            f"[ç›¸å…³æ–‡æ¡£ {i+1}]\n{chunk['content']}"
            for i, chunk in enumerate(chunks)
        ])
        return {
            "context": context
        }
    ```
  â†“ è¾“å‡º: context
èŠ‚ç‚¹4: LLM èŠ‚ç‚¹
  - æ¨¡å‹: GPT-4 / Claude
  - System Prompt:
    "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æŠ€æœ¯åŠ©æ‰‹ã€‚è¯·æ ¹æ®ä»¥ä¸‹çŸ¥è¯†åº“å†…å®¹å›ç­”ç”¨æˆ·é—®é¢˜ã€‚
    å¦‚æœçŸ¥è¯†åº“ä¸­æ²¡æœ‰ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ˜ç¡®å‘ŠçŸ¥ç”¨æˆ·ã€‚"
  - User Prompt:
    "çŸ¥è¯†åº“å†…å®¹ï¼š
    {{code.context}}
    
    ç”¨æˆ·é—®é¢˜ï¼š{{sys.query}}
    
    è¯·åŸºäºä»¥ä¸ŠçŸ¥è¯†åº“å†…å®¹å›ç­”é—®é¢˜ï¼š"
  â†“
èŠ‚ç‚¹5: ç»“æŸèŠ‚ç‚¹
  - è¾“å‡º: {{llm.text}}
```

**4. æµ‹è¯• Workflow**

åœ¨ Dify èŠå¤©ç•Œé¢æµ‹è¯•ï¼š

```
ç”¨æˆ·: CRMç³»ç»Ÿæ”¯æŒå“ªäº›ç™»å½•æ–¹å¼ï¼Ÿ

é¢„æœŸæµç¨‹:
1. é—®é¢˜å‘é€åˆ°çŸ¥è¯†ä¸­å° API
2. è¿”å›ç›¸å…³æ–‡æ¡£ç‰‡æ®µ
3. LLM åŸºäºæ–‡æ¡£ç”Ÿæˆç­”æ¡ˆ
4. è¿”å›ç»™ç”¨æˆ·ï¼š
   "æ ¹æ®çŸ¥è¯†åº“ï¼ŒCRMç³»ç»Ÿæ”¯æŒä¸‰ç§ç™»å½•æ–¹å¼ï¼š
    1. ç”¨æˆ·åå¯†ç ç™»å½•
    2. æ‰‹æœºéªŒè¯ç ç™»å½•
    3. ä¼ä¸šå¾®ä¿¡æ‰«ç ç™»å½•"
```

### Step 16: ä¼˜åŒ–æ£€ç´¢ç»“æœå±•ç¤ºï¼ˆå¯é€‰ï¼Œ15åˆ†é’Ÿï¼‰

**åœ¨ Workflow ä¸­æ·»åŠ å¼•ç”¨å±•ç¤º**ï¼š

ä¿®æ”¹ LLM èŠ‚ç‚¹çš„è¾“å‡ºæ ¼å¼ï¼š

```python
# åœ¨ä»£ç èŠ‚ç‚¹ä¸­æ·»åŠ 
def main(chunks):
    context = "\n\n".join([
        f"[æ–‡æ¡£{i+1}] {chunk['content']}"
        for i, chunk in enumerate(chunks)
    ])
    
    # ç”Ÿæˆå¼•ç”¨ä¿¡æ¯
    references = [
        {
            "index": i+1,
            "filename": chunk['metadata'].get('filename', 'æœªçŸ¥'),
            "version": chunk['metadata'].get('version', '1.0'),
            "score": chunk['score']
        }
        for i, chunk in enumerate(chunks)
    ]
    
    return {
        "context": context,
        "references": references
    }
```

åœ¨æœ€ç»ˆå›å¤ä¸­é™„åŠ å¼•ç”¨ï¼š

```
{{llm.text}}

---
ğŸ“š å‚è€ƒæ–‡æ¡£ï¼š
{{#each code.references}}
  {{index}}. {{filename}} (v{{version}}) - ç›¸ä¼¼åº¦: {{score}}
{{/each}}
```

------

## ğŸ”§ ç¬¬ä¸ƒé˜¶æ®µï¼šé«˜çº§åŠŸèƒ½å¼€å‘

### Step 17: æ·»åŠ æ–‡æ¡£ç›‘å¬å™¨ï¼ˆè‡ªåŠ¨åŒ–å¤„ç†ï¼‰ï¼ˆ30åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/services/watcher.py`ï¼š

```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import time
import os
from pathlib import Path

from ..services.document_processor import doc_processor
from ..config.settings import settings
import logging

logger = logging.getLogger(__name__)

class DocumentWatcher(FileSystemEventHandler):
    """æ–‡æ¡£ç›‘å¬å™¨"""
    
    def __init__(self, kb_id: str):
        self.kb_id = kb_id
        self.watch_path = os.path.join(settings.CHROMA_DB_PATH, kb_id, "docs")
        self.processing = set()  # æ­£åœ¨å¤„ç†çš„æ–‡ä»¶
    
    def on_created(self, event):
        """æ–‡ä»¶åˆ›å»ºäº‹ä»¶"""
        if event.is_directory:
            return
        
        file_path = event.src_path
        
        # æ£€æŸ¥æ–‡ä»¶ç±»å‹
        if not any(file_path.endswith(ext) for ext in settings.ALLOWED_FILE_TYPES):
            return
        
        # é¿å…é‡å¤å¤„ç†
        if file_path in self.processing:
            return
        
        self.processing.add(file_path)
        
        logger.info(f"ğŸ“„ æ£€æµ‹åˆ°æ–°æ–‡æ¡£: {file_path}")
        
        # ç­‰å¾…æ–‡ä»¶å†™å…¥å®Œæˆ
        time.sleep(2)
        
        try:
            # å¤„ç†æ–‡æ¡£
            result = doc_processor.process_document(
                kb_id=self.kb_id,
                file_path=file_path
            )
            
            if result['success']:
                logger.info(f"âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ: {result['filename']}, chunks: {result['chunk_count']}")
            else:
                logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¤±è´¥: {result.get('error')}")
        
        except Exception as e:
            logger.error(f"âŒ æ–‡æ¡£å¤„ç†å¼‚å¸¸: {e}")
        
        finally:
            self.processing.remove(file_path)
    
    def on_modified(self, event):
        """æ–‡ä»¶ä¿®æ”¹äº‹ä»¶"""
        if event.is_directory:
            return
        
        logger.info(f"ğŸ“ æ–‡æ¡£å·²ä¿®æ”¹: {event.src_path}")
        # å¯ä»¥é€‰æ‹©é‡æ–°å¤„ç†æ–‡æ¡£

class WatcherManager:
    """ç›‘å¬å™¨ç®¡ç†å™¨"""
    
    def __init__(self):
        self.observers = {}  # {kb_id: Observer}
    
    def start_watcher(self, kb_id: str):
        """å¯åŠ¨æŒ‡å®šçŸ¥è¯†åº“çš„ç›‘å¬"""
        if kb_id in self.observers:
            logger.warning(f"çŸ¥è¯†åº“ {kb_id} çš„ç›‘å¬å™¨å·²åœ¨è¿è¡Œ")
            return
        
        watch_path = os.path.join(settings.CHROMA_DB_PATH, kb_id, "docs")
        
        if not os.path.exists(watch_path):
            os.makedirs(watch_path, exist_ok=True)
        
        event_handler = DocumentWatcher(kb_id)
        observer = Observer()
        observer.schedule(event_handler, watch_path, recursive=False)
        observer.start()
        
        self.observers[kb_id] = observer
        logger.info(f"ğŸ‘€ å·²å¯åŠ¨çŸ¥è¯†åº“ {kb_id} çš„æ–‡æ¡£ç›‘å¬")
    
    def stop_watcher(self, kb_id: str):
        """åœæ­¢æŒ‡å®šçŸ¥è¯†åº“çš„ç›‘å¬"""
        if kb_id not in self.observers:
            return
        
        self.observers[kb_id].stop()
        self.observers[kb_id].join()
        del self.observers[kb_id]
        
        logger.info(f"ğŸ›‘ å·²åœæ­¢çŸ¥è¯†åº“ {kb_id} çš„æ–‡æ¡£ç›‘å¬")
    
    def start_all_watchers(self):
        """å¯åŠ¨æ‰€æœ‰çŸ¥è¯†åº“çš„ç›‘å¬"""
        from .db_manager import db_manager
        
        kb_list = db_manager.list_kb()
        
        for kb in kb_list:
            self.start_watcher(kb['kb_id'])
        
        logger.info(f"ğŸ‘€ å·²å¯åŠ¨ {len(kb_list)} ä¸ªçŸ¥è¯†åº“çš„æ–‡æ¡£ç›‘å¬")
    
    def stop_all_watchers(self):
        """åœæ­¢æ‰€æœ‰ç›‘å¬"""
        for kb_id in list(self.observers.keys()):
            self.stop_watcher(kb_id)

# å…¨å±€ç›‘å¬å™¨ç®¡ç†å™¨
watcher_manager = WatcherManager()
```

**åœ¨ main.py ä¸­å¯ç”¨ç›‘å¬**ï¼š

```python
# åœ¨ main.py ä¸­æ·»åŠ 
from .services.watcher import watcher_manager

@app.on_event("startup")
async def startup_event():
    logger.info("ğŸš€ çŸ¥è¯†ä¸­å° API å¯åŠ¨æˆåŠŸ")
    
    # å¯åŠ¨æ‰€æœ‰çŸ¥è¯†åº“çš„æ–‡æ¡£ç›‘å¬
    watcher_manager.start_all_watchers()
    
    logger.info(f"ğŸ“ API åœ°å€: http://{settings.API_HOST}:{settings.API_PORT}")
    logger.info(f"ğŸ“š API æ–‡æ¡£: http://{settings.API_HOST}:{settings.API_PORT}/docs")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("ğŸ‘‹ å…³é—­æ–‡æ¡£ç›‘å¬å™¨")
    watcher_manager.stop_all_watchers()
    logger.info("ğŸ‘‹ çŸ¥è¯†ä¸­å° API å…³é—­")
```

**æµ‹è¯•è‡ªåŠ¨ç›‘å¬**ï¼š

```bash
# å¯åŠ¨æœåŠ¡
python main.py

# åœ¨å¦ä¸€ä¸ªç»ˆç«¯ï¼Œå¤åˆ¶æ–‡ä»¶åˆ° docs ç›®å½•
echo "æµ‹è¯•è‡ªåŠ¨å¤„ç†" > ./knowledge_bases/test_crm/docs/test_auto.txt

# æŸ¥çœ‹æ—¥å¿—ï¼Œåº”è¯¥çœ‹åˆ°ï¼š
# ğŸ“„ æ£€æµ‹åˆ°æ–°æ–‡æ¡£: ./knowledge_bases/test_crm/docs/test_auto.txt
# âœ… æ–‡æ¡£å¤„ç†æˆåŠŸ: test_auto.txt, chunks: 1
```

### Step 18: æ·»åŠ æ•°æ®åº“å¤‡ä»½åŠŸèƒ½ï¼ˆ20åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/services/backup.py`ï¼š

```python
import os
import shutil
import tarfile
from datetime import datetime
from pathlib import Path

from ..config.settings import settings
import logging

logger = logging.getLogger(__name__)

class BackupService:
    """å¤‡ä»½æœåŠ¡"""
    
    def __init__(self):
        self.backup_dir = "./backups"
        os.makedirs(self.backup_dir, exist_ok=True)
    
    def backup_kb(self, kb_id: str) -> dict:
        """å¤‡ä»½æŒ‡å®šçŸ¥è¯†åº“"""
        try:
            kb_path = os.path.join(settings.CHROMA_DB_PATH, kb_id)
            
            if not os.path.exists(kb_path):
                return {
                    "success": False,
                    "error": f"çŸ¥è¯†åº“ {kb_id} ä¸å­˜åœ¨"
                }
            
            # ç”Ÿæˆå¤‡ä»½æ–‡ä»¶å
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{kb_id}_{timestamp}.tar.gz"
            backup_path = os.path.join(self.backup_dir, backup_name)
            
            # åˆ›å»ºå‹ç¼©åŒ…
            with tarfile.open(backup_path, "w:gz") as tar:
                tar.add(kb_path, arcname=kb_id)
            
            # è·å–å¤‡ä»½æ–‡ä»¶å¤§å°
            backup_size = os.path.getsize(backup_path) / (1024 * 1024)  # MB
            
            logger.info(f"âœ… çŸ¥è¯†åº“ {kb_id} å¤‡ä»½æˆåŠŸ: {backup_name}")
            
            return {
                "success": True,
                "kb_id": kb_id,
                "backup_file": backup_name,
                "backup_path": backup_path,
                "size_mb": round(backup_size, 2),
                "timestamp": timestamp
            }
        
        except Exception as e:
            logger.error(f"âŒ å¤‡ä»½å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def backup_all(self) -> dict:
        """å¤‡ä»½æ‰€æœ‰çŸ¥è¯†åº“"""
        from .db_manager import db_manager
        
        kb_list = db_manager.list_kb()
        results = []
        
        for kb in kb_list:
            result = self.backup_kb(kb['kb_id'])
            results.append(result)
        
        success_count = sum(1 for r in results if r['success'])
        
        return {
            "success": True,
            "total": len(kb_list),
            "success_count": success_count,
            "results": results
        }
    
    def restore_kb(self, backup_file: str) -> dict:
        """ä»å¤‡ä»½æ¢å¤çŸ¥è¯†åº“"""
        try:
            backup_path = os.path.join(self.backup_dir, backup_file)
            
            if not os.path.exists(backup_path):
                return {
                    "success": False,
                    "error": "å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨"
                }
            
            # è§£å‹
            with tarfile.open(backup_path, "r:gz") as tar:
                tar.extractall(path=settings.CHROMA_DB_PATH)
            
            logger.info(f"âœ… ä»å¤‡ä»½æ¢å¤æˆåŠŸ: {backup_file}")
            
            return {
                "success": True,
                "backup_file": backup_file
            }
        
        except Exception as e:
            logger.error(f"âŒ æ¢å¤å¤±è´¥: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def list_backups(self) -> list:
        """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
        backups = []
        
        for filename in os.listdir(self.backup_dir):
            if filename.endswith(".tar.gz"):
                file_path = os.path.join(self.backup_dir, filename)
                file_size = os.path.getsize(file_path) / (1024 * 1024)
                
                backups.append({
                    "filename": filename,
                    "size_mb": round(file_size, 2),
                    "created_at": datetime.fromtimestamp(
                        os.path.getctime(file_path)
                    ).isoformat()
                })
        
        return sorted(backups, key=lambda x: x['created_at'], reverse=True)
    
    def cleanup_old_backups(self, keep_count: int = 10):
        """æ¸…ç†æ—§å¤‡ä»½ï¼Œä¿ç•™æœ€è¿‘ N ä¸ª"""
        backups = self.list_backups()
        
        if len(backups) <= keep_count:
            return {
                "success": True,
                "message": "å¤‡ä»½æ•°é‡æœªè¶…è¿‡é™åˆ¶",
                "deleted": 0
            }
        
        # åˆ é™¤æ—§å¤‡ä»½
        to_delete = backups[keep_count:]
        deleted_count = 0
        
        for backup in to_delete:
            file_path = os.path.join(self.backup_dir, backup['filename'])
            try:
                os.remove(file_path)
                deleted_count += 1
            except Exception as e:
                logger.error(f"åˆ é™¤å¤‡ä»½å¤±è´¥: {e}")
        
        return {
            "success": True,
            "deleted": deleted_count,
            "kept": keep_count
        }

# å…¨å±€å¤‡ä»½æœåŠ¡å®ä¾‹
backup_service = BackupService()
```

**æ·»åŠ å¤‡ä»½ API**ï¼š

åˆ›å»º `data_platform/backend/api/maintenance.py`ï¼š

```python
from fastapi import APIRouter, HTTPException

from ..models.schemas import APIResponse
from ..services.backup import backup_service

router = APIRouter(prefix="/api/maintenance", tags=["ç»´æŠ¤"])

@router.post("/backup/{kb_id}", response_model=APIResponse)
async def backup_kb(kb_id: str):
    """å¤‡ä»½æŒ‡å®šçŸ¥è¯†åº“"""
    result = backup_service.backup_kb(kb_id)
    
    if result['success']:
        return APIResponse(
            success=True,
            message="å¤‡ä»½æˆåŠŸ",
            data=result
        )
    else:
        raise HTTPException(status_code=500, detail=result['error'])

@router.post("/backup/all", response_model=APIResponse)
async def backup_all():
    """å¤‡ä»½æ‰€æœ‰çŸ¥è¯†åº“"""
    result = backup_service.backup_all()
    return APIResponse(
        success=True,
        message=f"å¤‡ä»½å®Œæˆï¼ŒæˆåŠŸ {result['success_count']}/{result['total']}",
        data=result
    )

@router.get("/backups")
async def list_backups():
    """åˆ—å‡ºæ‰€æœ‰å¤‡ä»½"""
    return backup_service.list_backups()

@router.post("/restore/{backup_file}", response_model=APIResponse)
async def restore_backup(backup_file: str):
    """ä»å¤‡ä»½æ¢å¤"""
    result = backup_service.restore_kb(backup_file)
    
    if result['success']:
        return APIResponse(
            success=True,
            message="æ¢å¤æˆåŠŸ",
            data=result
        )
    else:
        raise HTTPException(status_code=500, detail=result['error'])

@router.post("/cleanup", response_model=APIResponse)
async def cleanup_backups(keep_count: int = 10):
    """æ¸…ç†æ—§å¤‡ä»½"""
    result = backup_service.cleanup_old_backups(keep_count)
    return APIResponse(
        success=True,
        message=f"æ¸…ç†å®Œæˆï¼Œåˆ é™¤ {result['deleted']} ä¸ªå¤‡ä»½",
        data=result
    )
```

åœ¨ `main.py` ä¸­æ³¨å†Œè·¯ç”±ï¼š

```python
from .api import maintenance

app.include_router(maintenance.router)
```

### Step 19: æ·»åŠ ç³»ç»Ÿç›‘æ§ APIï¼ˆ20åˆ†é’Ÿï¼‰

åˆ›å»º `data_platform/backend/api/monitoring.py`ï¼š

```python
from fastapi import APIRouter
import psutil
from datetime import datetime

from ..services.db_manager import db_manager
from ..models.schemas import SystemStats

router = APIRouter(prefix="/api/system", tags=["ç³»ç»Ÿç›‘æ§"])

@router.get("/stats", response_model=SystemStats)
async def get_system_stats():
    """è·å–ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯"""
    
    # CPU å’Œå†…å­˜
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    # çŸ¥è¯†åº“ç»Ÿè®¡
    kb_list = db_manager.list_kb()
    total_kb = len(kb_list)
    
    total_docs = 0
    total_chunks = 0
    
    for kb in kb_list:
        stats = db_manager.get_kb_stats(kb['kb_id'])
        total_docs += stats['doc_count']
        total_chunks += stats['chunk_count']
    
    return SystemStats(
        cpu_percent=round(cpu_percent, 2),
        memory_percent=round(memory.percent, 2),
        disk_percent=round(disk.percent, 2),
        total_kb=total_kb,
        total_docs=total_docs,
        total_chunks=total_chunks,
        timestamp=datetime.now().isoformat()
    )

@router.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }
```

åœ¨ `main.py` ä¸­æ³¨å†Œï¼š

```python
from .api import monitoring

app.include_router(monitoring.router)
```

------

## ğŸ“¦ ç¬¬å…«é˜¶æ®µï¼šDocker éƒ¨ç½²

### Step 20: åˆ›å»º Dockerfileï¼ˆ15åˆ†é’Ÿï¼‰

åœ¨é¡¹ç›®æ ¹ç›®å½•åˆ›å»º `Dockerfile`ï¼š

```dockerfile
FROM python:3.10-slim

# è®¾ç½®å·¥ä½œç›®å½•
WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libmagic1 \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .

# å®‰è£… Python ä¾èµ–
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶é¡¹ç›®æ–‡ä»¶
COPY data_platform/ ./data_platform/
COPY .env .env

# åˆ›å»ºå¿…è¦çš„ç›®å½•
RUN mkdir -p knowledge_bases logs backups

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["python", "-m", "uvicorn", "data_platform.backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Step 21: åˆ›å»º docker-compose.ymlï¼ˆ15åˆ†é’Ÿï¼‰

```yaml
version: '3.8'

services:
  knowledge-platform:
    build: .
    container_name: knowledge_platform_api
    ports:
      - "8000:8000"
    volumes:
      - ./knowledge_bases:/app/knowledge_bases
      - ./logs:/app/logs
      - ./backups:/app/backups
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_API_BASE=${OPENAI_API_BASE}
      - API_HOST=0.0.0.0
      - API_PORT=8000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Step 22: éƒ¨ç½²ä¸æµ‹è¯•ï¼ˆ10åˆ†é’Ÿï¼‰

```bash
# æ„å»ºé•œåƒ
docker-compose build

# å¯åŠ¨æœåŠ¡
docker-compose up -d

# æŸ¥çœ‹æ—¥å¿—
docker-compose logs -f

# æµ‹è¯• API
curl http://localhost:8000/health

# åœæ­¢æœåŠ¡
docker-compose down
```

------

## ğŸ¨ ç¬¬ä¹é˜¶æ®µï¼ˆå¯é€‰ï¼‰ï¼šå‰ç«¯ç®¡ç†ç•Œé¢

### Step 23: åˆå§‹åŒ–å‰ç«¯é¡¹ç›®ï¼ˆ20åˆ†é’Ÿï¼‰

```bash
cd data_platform/frontend

# ä½¿ç”¨ Vite åˆ›å»º React é¡¹ç›®
npm create vite@latest . -- --template react-ts

# å®‰è£…ä¾èµ–
npm install

# å®‰è£… UI ç»„ä»¶åº“å’Œå·¥å…·
npm install antd axios zustand @ant-design/icons @ant-design/pro-components
npm install recharts dayjs

# å¯åŠ¨å¼€å‘æœåŠ¡å™¨
npm run dev
```

### Step 24: åˆ›å»ºåŸºç¡€å¸ƒå±€ï¼ˆ30åˆ†é’Ÿï¼‰

åˆ›å»º `src/App.tsx`ï¼š

```typescript
import { Layout, Menu } from 'antd';
import { DatabaseOutlined, FileTextOutlined, SearchOutlined, SettingOutlined } from '@ant-design/icons';
import { useState } from 'react';

const { Header, Sider, Content } = Layout;

function App() {
  const [collapsed, setCollapsed] = useState(false);
  const [selectedKey, setSelectedKey] = useState('kb');

  const menuItems = [
    {
      key: 'kb',
      icon: <DatabaseOutlined />,
      label: 'çŸ¥è¯†åº“ç®¡ç†'
    },
    {
      key: 'docs',
      icon: <FileTextOutlined />,
      label: 'æ–‡æ¡£ç®¡ç†'
    },
    {
      key: 'search',
      icon: <SearchOutlined />,
      label: 'æ£€ç´¢æµ‹è¯•'
    },
    {
      key: 'system',
      icon: <SettingOutlined />,
      label: 'ç³»ç»Ÿç›‘æ§'
    }
  ];

  return (
    <Layout style={{ minHeight: '100vh' }}>
      <Sider collapsible collapsed={collapsed} onCollapse={setCollapsed}>
        <div style={{
          height: 64,
          display: 'flex',
          alignItems: 'center',
          justifyContent: 'center',
          color: 'white',
          fontSize: 18,
          fontWeight: 'bold'
        }}>
          {collapsed ? 'çŸ¥è¯†ä¸­å°' : 'ä¼ä¸šçŸ¥è¯†ä¸­å°'}
        </div>
        <Menu
          theme="dark"
          selectedKeys={[selectedKey]}
          mode="inline"
          items={menuItems}
          onClick={({ key }) => setSelectedKey(key)}
        />
      </Sider>
      <Layout>
        <Header style={{ background: '#fff', padding: '0 24px' }}>
          <h2>çŸ¥è¯†ä¸­å°ç®¡ç†ç³»ç»Ÿ</h2>
        </Header>
        <Content style={{ margin: '24px 16px', padding: 24, background: '#fff' }}>
          {/* è¿™é‡Œæ ¹æ® selectedKey æ¸²æŸ“ä¸åŒçš„é¡µé¢ */}
          <div>å†…å®¹åŒºåŸŸ - {selectedKey}</div>
        </Content>
      </Layout>
    </Layout>
  );
}

export default App;
```

ç”±äºå‰ç«¯å¼€å‘å†…å®¹è¾ƒå¤šï¼Œè¿™é‡Œæä¾›æ ¸å¿ƒé¡µé¢çš„æ¡†æ¶ä»£ç ã€‚å®Œæ•´çš„å‰ç«¯å®ç°å¯ä»¥åœ¨ç¡®è®¤éœ€è¦åç»§ç»­è¡¥å……ã€‚

------

## âœ… å®Œæ•´å®æ–½æ—¶é—´è¡¨

| é˜¶æ®µ              | ä»»åŠ¡                         | é¢„è®¡æ—¶é—´ | çŠ¶æ€ |
| ----------------- | ---------------------------- | -------- | ---- |
| **ç¬¬1å‘¨**         |                              |          |      |
| Day 1             | ç¯å¢ƒæ­å»º + é¡¹ç›®åˆå§‹åŒ–        | 2å°æ—¶    | â¬œ    |
| Day 2             | æ•°æ®æ¨¡å‹ + æ•°æ®åº“ç®¡ç†        | 3å°æ—¶    | â¬œ    |
| Day 3-4           | æ–‡æ¡£å¤„ç†å¼•æ“å¼€å‘             | 6å°æ—¶    | â¬œ    |
| Day 5-7           | API å¼€å‘ï¼ˆçŸ¥è¯†åº“/æ–‡æ¡£/æ£€ç´¢ï¼‰ | 8å°æ—¶    | â¬œ    |
| **ç¬¬2å‘¨**         |                              |          |      |
| Day 8-9           | Dify å¯¹æ¥ + æµ‹è¯•             | 4å°æ—¶    | â¬œ    |
| Day 10-11         | é«˜çº§åŠŸèƒ½ï¼ˆç›‘å¬/å¤‡ä»½ï¼‰        | 5å°æ—¶    | â¬œ    |
| Day 12-14         | ç³»ç»Ÿç›‘æ§ + Docker éƒ¨ç½²       | 6å°æ—¶    | â¬œ    |
| **ç¬¬3å‘¨ï¼ˆå¯é€‰ï¼‰** |                              |          |      |
| Day 15-17         | å‰ç«¯ç•Œé¢å¼€å‘                 | 12å°æ—¶   | â¬œ    |
| Day 18-19         | è”è°ƒæµ‹è¯• + Bug ä¿®å¤          | 8å°æ—¶    | â¬œ    |
| Day 20-21         | æ–‡æ¡£ç¼–å†™ + éƒ¨ç½²ä¸Šçº¿          | 6å°æ—¶    | â¬œ    |

------

## ğŸ“ å®æ–½æ£€æŸ¥æ¸…å•

### æ ¸å¿ƒåŠŸèƒ½

- [ ] çŸ¥è¯†åº“åˆ›å»º/åˆ é™¤/åˆ—è¡¨
- [ ] æ–‡æ¡£ä¸Šä¼ /åˆ é™¤/åˆ—è¡¨
- [ ] æ–‡æ¡£è‡ªåŠ¨è§£æä¸åˆ†å—
- [ ] å‘é‡åŒ–ä¸å­˜å‚¨
- [ ] è¯­ä¹‰æ£€ç´¢æ¥å£
- [ ] Dify å·¥å…·å¯¹æ¥

### é«˜çº§åŠŸèƒ½

- [ ] æ–‡æ¡£è‡ªåŠ¨ç›‘å¬
- [ ] å¤šçŸ¥è¯†åº“è”åˆæ£€ç´¢
- [ ] æ•°æ®åº“å¤‡ä»½/æ¢å¤
- [ ] ç³»ç»Ÿç›‘æ§ API
- [ ] æ—¥å¿—è®°å½•

### éƒ¨ç½²ç›¸å…³

- [ ] Docker é•œåƒæ„å»º
- [ ] docker-compose é…ç½®
- [ ] ç¯å¢ƒå˜é‡ç®¡ç†
- [ ] å¥åº·æ£€æŸ¥

### æ–‡æ¡£ä¸æµ‹è¯•

- [ ] API æ–‡æ¡£ï¼ˆSwaggerï¼‰
- [ ] å•å…ƒæµ‹è¯•
- [ ] é›†æˆæµ‹è¯•
- [ ] éƒ¨ç½²æ–‡æ¡£
- [ ] ä½¿ç”¨æ‰‹å†Œ

------

## ğŸš¨ å¸¸è§é—®é¢˜ä¸è§£å†³æ–¹æ¡ˆ

### é—®é¢˜1: OpenAI API è¶…æ—¶

**è§£å†³**ï¼š

```python
# åœ¨ config/settings.py ä¸­æ·»åŠ 
OPENAI_TIMEOUT: int = 60
OPENAI_MAX_RETRIES: int = 3

# åœ¨åˆå§‹åŒ– embeddings æ—¶ä½¿ç”¨
embeddings = OpenAIEmbeddings(
    request_timeout=settings.OPENAI_TIMEOUT,
    max_retries=settings.OPENAI_MAX_RETRIES
)
```

### é—®é¢˜2: æ–‡æ¡£è§£æå¤±è´¥

**è§£å†³**ï¼š

```python
# æ·»åŠ å¼‚å¸¸å¤„ç†å’Œæ—¥å¿—
try:
    loader = UnstructuredFileLoader(file_path)
    documents = loader.load()
except Exception as e:
    logger.error(f"æ–‡æ¡£åŠ è½½å¤±è´¥: {file_path}, é”™è¯¯: {e}")
    # å°è¯•å¤‡ç”¨åŠ è½½å™¨
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            documents = [Document(page_content=content)]
    except:
        raise Exception("æ–‡æ¡£æ— æ³•è§£æ")
```

### é—®é¢˜3: ChromaDB æ•°æ®åº“é”å®š

**è§£å†³**ï¼š

```python
# ç¡®ä¿æ¯æ¬¡åªæœ‰ä¸€ä¸ªè¿›ç¨‹è®¿é—®æ•°æ®åº“
# ä½¿ç”¨æ–‡ä»¶é”æˆ–ä¿®æ”¹é…ç½®
client = chromadb.PersistentClient(
    path=db_path,
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True,
        is_persistent=True
    )
)
```

### é—®é¢˜4: å†…å­˜å ç”¨è¿‡é«˜

**è§£å†³**ï¼š

```python
# æ‰¹é‡å¤„ç†æ–‡æ¡£æ—¶åˆ†æ‰¹æäº¤
BATCH_SIZE = 10

for i in range(0, len(chunks), BATCH_SIZE):
    batch = chunks[i:i+BATCH_SIZE]
    # å¤„ç†æ‰¹æ¬¡
    collection.add(...)
    # æ‰‹åŠ¨è§¦å‘åƒåœ¾å›æ”¶
    import gc
    gc.collect()
```

### é—®é¢˜5: æ£€ç´¢ç»“æœä¸å‡†ç¡®

**è§£å†³**ï¼š

1. è°ƒæ•´ chunk_sizeï¼ˆè¯•è¯• 800-1200ï¼‰
2. å¢åŠ  chunk_overlapï¼ˆè¯•è¯• 150-250ï¼‰
3. ä½¿ç”¨æ›´å¥½çš„ embedding æ¨¡å‹ï¼ˆtext-embedding-3-largeï¼‰
4. å¯ç”¨é‡æ’åºï¼ˆRerankï¼‰
5. ä¼˜åŒ–æ–‡æ¡£ç»“æ„ï¼ˆæ¸…æ™°çš„ç« èŠ‚ï¼‰

------

## ğŸ“š ä¸‹ä¸€æ­¥ä¼˜åŒ–æ–¹å‘

### æ€§èƒ½ä¼˜åŒ–

1. **ç¼“å­˜æœºåˆ¶**
   - Redis ç¼“å­˜çƒ­é—¨æŸ¥è¯¢ç»“æœ
   - æœ¬åœ°ç¼“å­˜ embedding ç»“æœ
2. **å¼‚æ­¥å¤„ç†**
   - Celery ä»»åŠ¡é˜Ÿåˆ—å¤„ç†å¤§æ‰¹é‡æ–‡æ¡£
   - å¼‚æ­¥ç”Ÿæˆ embedding
3. **æ•°æ®åº“å‡çº§**
   - è¿ç§»åˆ° Milvusï¼ˆç™¾ä¸‡çº§æ•°æ®ï¼‰
   - ä½¿ç”¨ Qdrantï¼ˆé«˜æ€§èƒ½è¿‡æ»¤ï¼‰

### åŠŸèƒ½å¢å¼º

1. **æ™ºèƒ½é—®ç­”**
   - å¤šè½®å¯¹è¯ä¸Šä¸‹æ–‡ç®¡ç†
   - é—®é¢˜æ”¹å†™ä¸æ‰©å±•
   - ç­”æ¡ˆè´¨é‡è¯„ä¼°
2. **çŸ¥è¯†å›¾è°±**
   - è‡ªåŠ¨æå–å®ä½“å’Œå…³ç³»
   - å›¾è°±å¯è§†åŒ–
   - åŸºäºå›¾è°±çš„æ¨ç†
3. **ç‰ˆæœ¬ç®¡ç†**
   - æ–‡æ¡£ç‰ˆæœ¬å¯¹æ¯”
   - å˜æ›´å†å²è¿½è¸ª
   - ç‰ˆæœ¬å›æ»š
4. **æƒé™æ§åˆ¶**
   - ç”¨æˆ·è®¤è¯ï¼ˆJWTï¼‰
   - çŸ¥è¯†åº“è®¿é—®æ§åˆ¶
   - æ“ä½œå®¡è®¡æ—¥å¿—

### ç”¨æˆ·ä½“éªŒ

1. **å‰ç«¯ä¼˜åŒ–**
   - å®æ—¶çŠ¶æ€æ›´æ–°ï¼ˆWebSocketï¼‰
   - æ‰¹é‡æ“ä½œ
   - æ‹–æ‹½ä¸Šä¼ 
   - é¢„è§ˆä¸é«˜äº®
2. **æ™ºèƒ½æ¨è**
   - ç›¸å…³æ–‡æ¡£æ¨è
   - é—®é¢˜è¡¥å…¨
   - çƒ­é—¨æŸ¥è¯¢ç»Ÿè®¡

------

## ğŸ¯ å®æ–½å»ºè®®

### ç¬¬ä¸€å‘¨é‡ç‚¹

âœ… **å…ˆè·‘é€šæ ¸å¿ƒæµç¨‹**

- åˆ›å»ºçŸ¥è¯†åº“ â†’ ä¸Šä¼ æ–‡æ¡£ â†’ æ£€ç´¢æˆåŠŸ
- ä¸è¿½æ±‚å®Œç¾ï¼Œå…ˆå®ç°åŸºæœ¬åŠŸèƒ½
- æ¯å¤©æµ‹è¯•ï¼ŒåŠæ—¶å‘ç°é—®é¢˜

### ç¬¬äºŒå‘¨é‡ç‚¹

âœ… **å¯¹æ¥ Dify å¹¶ä¼˜åŒ–**

- ç¡®ä¿ Dify èƒ½æ­£å¸¸è°ƒç”¨
- æµ‹è¯•çœŸå®ä¸šåŠ¡åœºæ™¯
- æ ¹æ®åé¦ˆè°ƒæ•´ chunk ç­–ç•¥

### ç¬¬ä¸‰å‘¨ï¼ˆå¯é€‰ï¼‰

âœ… **å®Œå–„ä¸éƒ¨ç½²**

- æ·»åŠ å¿…è¦çš„é«˜çº§åŠŸèƒ½
- å‰ç«¯ç•Œé¢ï¼ˆå¯é€‰ï¼‰
- éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ

------

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### é‡åˆ°é—®é¢˜æ—¶çš„è°ƒè¯•æ­¥éª¤

1. **æ£€æŸ¥æ—¥å¿—**

   ```bash
   tail -f logs/app.log
   ```

2. **éªŒè¯é…ç½®**

   ```python
   python -c "from data_platform.backend.config.settings import settings; print(settings.dict())"
   ```

3. **æµ‹è¯•æ•°æ®åº“è¿æ¥**

   ```python
   python -c "from data_platform.backend.services.db_manager import db_manager; print(db_manager.list_kb())"
   ```

4. **æµ‹è¯• OpenAI API**

   ```python
   python -c "from langchain.embeddings import OpenAIEmbeddings; emb = OpenAIEmbeddings(); print(emb.embed_query('test')[:5])"
   ```

5. **æ£€æŸ¥ç«¯å£å ç”¨**

   ```bash
   lsof -i:8000  # æŸ¥çœ‹ 8000 ç«¯å£
   ```

### æ¨èèµ„æº

- **LangChain æ–‡æ¡£**: https://python.langchain.com/docs/
- **ChromaDB æ–‡æ¡£**: https://docs.trychroma.com/
- **FastAPI æ–‡æ¡£**: https://fastapi.tiangolo.com/
- **Dify æ–‡æ¡£**: https://docs.dify.ai/

------

## âœ¨ æ€»ç»“

é€šè¿‡ä»¥ä¸Šæ­¥éª¤ï¼Œä½ å°†æ‹¥æœ‰ä¸€ä¸ª**å®Œæ•´çš„ã€å¯æ‰©å±•çš„ä¼ä¸šçº§çŸ¥è¯†ä¸­å°**ï¼š

### æ ¸å¿ƒèƒ½åŠ›

âœ… å¤šçŸ¥è¯†åº“éš”ç¦»ç®¡ç† âœ… è‡ªåŠ¨åŒ–æ–‡æ¡£å¤„ç† âœ… é«˜æ•ˆè¯­ä¹‰æ£€ç´¢ âœ… æŒä¹…åŒ–æ•°æ®å­˜å‚¨ âœ… Dify æ— ç¼å¯¹æ¥ âœ… ç³»ç»Ÿç›‘æ§ä¸å¤‡ä»½

### æŠ€æœ¯æ¶æ„

```
å‰ç«¯ç®¡ç†ç•Œé¢ (React + Ant Design)
         â†“
æ•°æ®ä¸­å° API (FastAPI)
         â†“
LangChain å¤„ç†å±‚
         â†“
å‘é‡æ•°æ®åº“ (ChromaDB/Milvus)
         â†“
Dify æ™ºèƒ½é—®ç­”
```

### å®æ–½ç­–ç•¥

1. **ç¬¬ä¸€å‘¨**ï¼šæ ¸å¿ƒåŠŸèƒ½å¼€å‘ï¼ˆçŸ¥è¯†åº“ã€æ–‡æ¡£å¤„ç†ã€æ£€ç´¢ï¼‰
2. **ç¬¬äºŒå‘¨**ï¼šDify å¯¹æ¥ + é«˜çº§åŠŸèƒ½ï¼ˆç›‘å¬ã€å¤‡ä»½ã€ç›‘æ§ï¼‰
3. **ç¬¬ä¸‰å‘¨**ï¼šå‰ç«¯ç•Œé¢ + éƒ¨ç½²ä¸Šçº¿ï¼ˆå¯é€‰ï¼‰

### å…³é”®å»ºè®®

- ä»ç®€å•åˆ°å¤æ‚ï¼Œé€æ­¥è¿­ä»£
- æ¯ä¸ªé˜¶æ®µéƒ½è¦æµ‹è¯•éªŒè¯
- æ ¹æ®å®é™…éœ€æ±‚è°ƒæ•´åŠŸèƒ½ä¼˜å…ˆçº§
- åšå¥½æ–‡æ¡£å’Œæ—¥å¿—è®°å½•

------

## ğŸš€ å¼€å§‹å®æ–½

ç°åœ¨ä½ å¯ä»¥å¼€å§‹å®æ–½äº†ï¼å»ºè®®æŒ‰ç…§ä»¥ä¸‹é¡ºåºï¼š

1. **ç«‹å³å¼€å§‹**ï¼šStep 1-5ï¼ˆç¯å¢ƒæ­å»ºï¼‰
2. **ç¬¬ä¸€å¤©å®Œæˆ**ï¼šStep 6-8ï¼ˆæ•°æ®åº“ + æ–‡æ¡£å¤„ç†ï¼‰
3. **ç¬¬äºŒå¤©å®Œæˆ**ï¼šStep 9-14ï¼ˆAPI å¼€å‘ + æµ‹è¯•ï¼‰
4. **ç¬¬ä¸‰å¤©å®Œæˆ**ï¼šStep 15-16ï¼ˆDify å¯¹æ¥ï¼‰
5. **åç»­ä¼˜åŒ–**ï¼šStep 17-24ï¼ˆé«˜çº§åŠŸèƒ½ + éƒ¨ç½²ï¼‰

**è®°ä½**ï¼šä¸è¦è¿½æ±‚ä¸€æ¬¡æ€§å®Œç¾ï¼Œå…ˆè®©ç³»ç»Ÿè·‘èµ·æ¥ï¼Œç„¶åå†é€æ­¥ä¼˜åŒ–ï¼

ç¥ä½ å®æ–½é¡ºåˆ©ï¼ğŸ‰

------

## ğŸ“‹ é™„å½•ï¼šå®Œæ•´ä»£ç ä»“åº“ç»“æ„

```
knowledge_platform/
â”œâ”€â”€ README.md                           # é¡¹ç›®è¯´æ˜
â”œâ”€â”€ requirements.txt                    # Python ä¾èµ–
â”œâ”€â”€ .env                                # ç¯å¢ƒå˜é‡
â”œâ”€â”€ .gitignore                          # Git å¿½ç•¥æ–‡ä»¶
â”œâ”€â”€ Dockerfile                          # Docker é•œåƒ
â”œâ”€â”€ docker-compose.yml                  # Docker Compose é…ç½®
â”‚
â”œâ”€â”€ knowledge_bases/                    # çŸ¥è¯†åº“æ•°æ®ï¼ˆè¿è¡Œæ—¶ç”Ÿæˆï¼‰
â”‚   â”œâ”€â”€ crm_system/
â”‚   â”‚   â”œâ”€â”€ config.json
â”‚   â”‚   â”œâ”€â”€ docs/
â”‚   â”‚   â””â”€â”€ db/
â”‚   â””â”€â”€ erp_system/
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ docs/
â”‚       â””â”€â”€ db/
â”‚
â”œâ”€â”€ data_platform/
â”‚   â”œâ”€â”€ backend/
â”‚   â”‚   â”œâ”€â”€ main.py                    # FastAPI å…¥å£ â­
â”‚   â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ kb_management.py       # çŸ¥è¯†åº“ç®¡ç† API
â”‚   â”‚   â”‚   â”œâ”€â”€ document_management.py # æ–‡æ¡£ç®¡ç† API
â”‚   â”‚   â”‚   â”œâ”€â”€ retrieve.py            # æ£€ç´¢ API â­
â”‚   â”‚   â”‚   â”œâ”€â”€ maintenance.py         # ç»´æŠ¤ API
â”‚   â”‚   â”‚   â””â”€â”€ monitoring.py          # ç›‘æ§ API
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â”œâ”€â”€ db_manager.py          # æ•°æ®åº“ç®¡ç† â­
â”‚   â”‚   â”‚   â”œâ”€â”€ document_processor.py  # æ–‡æ¡£å¤„ç† â­
â”‚   â”‚   â”‚   â”œâ”€â”€ retriever.py           # æ£€ç´¢æœåŠ¡ â­
â”‚   â”‚   â”‚   â”œâ”€â”€ watcher.py             # æ–‡ä»¶ç›‘å¬
â”‚   â”‚   â”‚   â””â”€â”€ backup.py              # å¤‡ä»½æœåŠ¡
â”‚   â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ schemas.py             # æ•°æ®æ¨¡å‹ â­
â”‚   â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”‚   â””â”€â”€ settings.py            # é…ç½®ç®¡ç† â­
â”‚   â”‚   â””â”€â”€ utils/
â”‚   â”‚       â”œâ”€â”€ __init__.py
â”‚   â”‚       â””â”€â”€ helpers.py
â”‚   â”‚
â”‚   â””â”€â”€ frontend/                       # å‰ç«¯ï¼ˆå¯é€‰ï¼‰
â”‚       â”œâ”€â”€ package.json
â”‚       â”œâ”€â”€ src/
â”‚       â”‚   â”œâ”€â”€ App.tsx
â”‚       â”‚   â”œâ”€â”€ pages/
â”‚       â”‚   â”œâ”€â”€ components/
â”‚       â”‚   â””â”€â”€ services/
â”‚       â””â”€â”€ public/
â”‚
â”œâ”€â”€ tests/                              # æµ‹è¯•ä»£ç 
â”‚   â”œâ”€â”€ test_db_manager.py
â”‚   â”œâ”€â”€ test_document_processor.py
â”‚   â””â”€â”€ test_api.py
â”‚
â”œâ”€â”€ logs/                               # æ—¥å¿—ï¼ˆè¿è¡Œæ—¶ç”Ÿæˆï¼‰
â”‚   â””â”€â”€ app.log
â”‚
â”œâ”€â”€ backups/                            # å¤‡ä»½ï¼ˆè¿è¡Œæ—¶ç”Ÿæˆï¼‰
â”‚   â””â”€â”€ crm_system_20251030_120000.tar.gz
â”‚
â””â”€â”€ scripts/                            # è„šæœ¬å·¥å…·
    â”œâ”€â”€ init_kb.py                     # åˆå§‹åŒ–çŸ¥è¯†åº“
    â”œâ”€â”€ migrate_data.py                # æ•°æ®è¿ç§»
    â””â”€â”€ cleanup.py                     # æ¸…ç†è„šæœ¬
```

**æ ‡æ³¨ â­ çš„æ˜¯æœ€æ ¸å¿ƒçš„æ–‡ä»¶**ï¼Œè¿™äº›æ˜¯ä½ å®æ–½æ—¶çš„é‡ç‚¹ï¼