# 企业级知识中台完整实施指南

> **目标**：从零搭建 LangChain + Dify 企业级知识中台 **预计时间**：2-3 周（根据功能完整度） **技术栈**：Python 3.10+ / FastAPI / LangChain / ChromaDB / React

------

## 📋 总体实施路线

```
第一周：核心后端开发
├── Day 1-2: 环境搭建 + 数据库设计
├── Day 3-4: 文档处理引擎（LangChain 层）
└── Day 5-7: 数据中台后端 API 开发

第二周：对接与优化
├── Day 8-9: Dify 对接 + 检索接口优化
├── Day 10-11: 多知识库管理 + 版本控制
└── Day 12-14: 监控、日志、备份功能

第三周（可选）：前端开发
├── Day 15-17: 管理界面开发
├── Day 18-19: 部署配置（Docker）
└── Day 20-21: 测试 + 文档编写
```

------

## 🚀 第一阶段：环境搭建与项目初始化

### Step 1: 创建项目目录结构（15分钟）

```bash
# 创建项目根目录
mkdir knowledge_platform && cd knowledge_platform

# 创建目录结构
mkdir -p {knowledge_bases,data_platform/{backend,frontend},logs,backups,tests}

# 创建后端子目录
cd data_platform/backend
mkdir -p {api,services,models,config,utils}
cd ../..

# 查看结构
tree -L 3
```

**预期目录结构**：

```
knowledge_platform/
├── knowledge_bases/          # 知识库存放（运行时创建）
├── data_platform/
│   ├── backend/
│   │   ├── api/             # API 路由
│   │   ├── services/        # 业务逻辑
│   │   ├── models/          # 数据模型
│   │   ├── config/          # 配置文件
│   │   ├── utils/           # 工具函数
│   │   └── main.py          # FastAPI 入口
│   └── frontend/            # 前端代码（第三周开发）
├── logs/                    # 日志目录
├── backups/                 # 备份目录
├── tests/                   # 测试代码
├── requirements.txt         # Python 依赖
├── .env                     # 环境变量
├── .gitignore
└── README.md
```

### Step 2: 创建 Python 虚拟环境（5分钟）

```bash
# 创建虚拟环境
python3.10 -m venv venv

# 激活虚拟环境
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# 验证 Python 版本
python --version  # 应显示 Python 3.10.x
```

### Step 3: 安装核心依赖（10分钟）

创建 `requirements.txt`：

```txt
# FastAPI 框架
fastapi==0.104.1
uvicorn[standard]==0.24.0
python-multipart==0.0.6

# LangChain 生态
langchain==0.1.0
langchain-community==0.0.10
langchain-openai==0.0.2

# 文档处理
unstructured==0.11.0
python-magic==0.4.27
pillow==10.1.0
pypdf==3.17.1
python-docx==1.1.0
openpyxl==3.1.2

# 向量数据库
chromadb==0.4.18

# 工具库
pydantic==2.5.0
pydantic-settings==2.1.0
python-jose[cryptography]==3.3.0
passlib[bcrypt]==1.7.4
python-dotenv==1.0.0
watchdog==3.0.0
psutil==5.9.6

# 可选：其他 embedding 模型
sentence-transformers==2.2.2
```

安装依赖：

```bash
pip install -r requirements.txt

# 验证安装
python -c "import fastapi; import langchain; import chromadb; print('✅ 所有依赖安装成功')"
```

### Step 4: 创建环境变量文件（5分钟）

创建 `.env` 文件：

```bash
# OpenAI API 配置
OPENAI_API_KEY=sk-your-api-key-here
OPENAI_API_BASE=https://api.openai.com/v1  # 或国内代理地址

# 数据库配置
CHROMA_DB_PATH=./knowledge_bases

# 服务配置
API_HOST=0.0.0.0
API_PORT=8000
DEBUG=True

# JWT 认证（可选）
SECRET_KEY=your-secret-key-here
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=1440

# 日志配置
LOG_LEVEL=INFO
LOG_FILE=./logs/app.log
```

创建 `.gitignore`：

```bash
# Python
__pycache__/
*.py[cod]
*$py.class
venv/
.env

# 数据文件
knowledge_bases/
logs/
backups/
*.db
*.sqlite

# IDE
.vscode/
.idea/
*.swp
```

### Step 5: 创建配置管理模块（20分钟）

创建 `data_platform/backend/config/settings.py`：

```python
from pydantic_settings import BaseSettings
from typing import Optional

class Settings(BaseSettings):
    """全局配置"""
    
    # API 配置
    API_HOST: str = "0.0.0.0"
    API_PORT: int = 8000
    DEBUG: bool = True
    
    # OpenAI 配置
    OPENAI_API_KEY: str
    OPENAI_API_BASE: Optional[str] = None
    OPENAI_EMBEDDING_MODEL: str = "text-embedding-3-small"
    
    # 数据库配置
    CHROMA_DB_PATH: str = "./knowledge_bases"
    
    # 认证配置
    SECRET_KEY: str = "change-me-in-production"
    ALGORITHM: str = "HS256"
    ACCESS_TOKEN_EXPIRE_MINUTES: int = 1440
    
    # 文档处理配置
    DEFAULT_CHUNK_SIZE: int = 1000
    DEFAULT_CHUNK_OVERLAP: int = 200
    ALLOWED_FILE_TYPES: list = [".txt", ".md", ".docx", ".pdf", ".html"]
    MAX_FILE_SIZE_MB: int = 50
    
    # 日志配置
    LOG_LEVEL: str = "INFO"
    LOG_FILE: str = "./logs/app.log"
    
    class Config:
        env_file = ".env"

# 全局配置实例
settings = Settings()
```

**测试配置**：

```bash
python -c "from data_platform.backend.config.settings import settings; print(settings.OPENAI_API_KEY[:10] + '...')"
```

------

## 🧱 第二阶段：核心数据模型设计

### Step 6: 创建 Pydantic 数据模型（30分钟）

创建 `data_platform/backend/models/schemas.py`：

```python
from pydantic import BaseModel, Field
from typing import Optional, List, Dict
from datetime import datetime
from enum import Enum

# ==================== 知识库相关 ====================
class KnowledgeBaseCreate(BaseModel):
    """创建知识库请求"""
    kb_id: str = Field(..., description="知识库唯一标识")
    kb_name: str = Field(..., description="知识库名称")
    description: Optional[str] = None
    embedding_model: str = "text-embedding-3-small"
    chunk_size: int = 1000
    chunk_overlap: int = 200

class KnowledgeBaseInfo(BaseModel):
    """知识库信息"""
    kb_id: str
    kb_name: str
    description: Optional[str]
    created_at: str
    doc_count: int = 0
    chunk_count: int = 0
    db_size_mb: float = 0.0

class KnowledgeBaseConfig(BaseModel):
    """知识库配置"""
    kb_id: str
    kb_name: str
    description: str
    created_at: str
    embedding_model: str
    chunk_size: int
    chunk_overlap: int
    db_type: str = "chromadb"
    db_path: str
    allowed_doc_types: List[str]

# ==================== 文档相关 ====================
class DocumentStatus(str, Enum):
    """文档处理状态"""
    PENDING = "pending"
    PROCESSING = "processing"
    COMPLETED = "completed"
    FAILED = "failed"

class DocumentInfo(BaseModel):
    """文档信息"""
    doc_id: str
    kb_id: str
    filename: str
    file_size: int
    status: DocumentStatus
    chunk_count: int = 0
    created_at: str
    processed_at: Optional[str] = None
    error_message: Optional[str] = None

class DocumentMetadata(BaseModel):
    """文档元信息"""
    doc_id: str
    filename: str
    system: str = "unknown"
    doc_type: str = "unknown"
    version: str = "1.0"
    module: str = "general"
    author: Optional[str] = None
    created_at: str

# ==================== 检索相关 ====================
class RetrieveRequest(BaseModel):
    """检索请求"""
    query: str = Field(..., description="查询问题")
    kb_id: str = Field(..., description="知识库ID")
    top_k: int = Field(5, ge=1, le=20, description="返回结果数量")
    min_score: float = Field(0.6, ge=0.0, le=1.0, description="最低相似度阈值")
    filters: Optional[Dict] = Field(None, description="元数据过滤条件")

class ChunkResult(BaseModel):
    """单个检索结果"""
    content: str
    metadata: Dict
    score: float

class RetrieveResponse(BaseModel):
    """检索响应"""
    success: bool
    query: str
    kb_id: str
    chunks: List[ChunkResult]
    total: int

# ==================== 系统相关 ====================
class SystemStats(BaseModel):
    """系统统计"""
    cpu_percent: float
    memory_percent: float
    disk_percent: float
    total_kb: int
    total_docs: int
    total_chunks: int
    timestamp: str

class APIResponse(BaseModel):
    """通用 API 响应"""
    success: bool
    message: str
    data: Optional[Dict] = None
```

------

## 🔧 第三阶段：数据库管理层

### Step 7: 创建数据库管理服务（45分钟）

创建 `data_platform/backend/services/db_manager.py`：

```python
import chromadb
from chromadb.config import Settings as ChromaSettings
from typing import Optional, List, Dict
import os
import json
from datetime import datetime

from ..config.settings import settings

class DatabaseManager:
    """向量数据库管理器"""
    
    def __init__(self):
        self.base_path = settings.CHROMA_DB_PATH
        self._clients = {}  # 缓存不同知识库的 client
    
    def get_client(self, kb_id: str) -> chromadb.ClientAPI:
        """
        获取指定知识库的 ChromaDB 客户端
        """
        if kb_id in self._clients:
            return self._clients[kb_id]
        
        db_path = os.path.join(self.base_path, kb_id, "db")
        
        # 确保目录存在
        os.makedirs(db_path, exist_ok=True)
        
        # 创建持久化客户端
        client = chromadb.PersistentClient(
            path=db_path,
            settings=ChromaSettings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        self._clients[kb_id] = client
        return client
    
    def get_collection(self, kb_id: str, collection_name: Optional[str] = None):
        """
        获取或创建 collection
        """
        client = self.get_client(kb_id)
        
        if collection_name is None:
            collection_name = f"{kb_id}_documents"
        
        collection = client.get_or_create_collection(
            name=collection_name,
            metadata={"kb_id": kb_id}
        )
        
        return collection
    
    def create_kb(self, kb_config: dict) -> bool:
        """
        创建新知识库
        """
        kb_id = kb_config['kb_id']
        kb_path = os.path.join(self.base_path, kb_id)
        
        # 创建目录结构
        os.makedirs(os.path.join(kb_path, "docs"), exist_ok=True)
        os.makedirs(os.path.join(kb_path, "db"), exist_ok=True)
        
        # 保存配置文件
        config_path = os.path.join(kb_path, "config.json")
        with open(config_path, 'w', encoding='utf-8') as f:
            json.dump(kb_config, f, ensure_ascii=False, indent=2)
        
        # 初始化 collection
        self.get_collection(kb_id)
        
        return True
    
    def delete_kb(self, kb_id: str) -> bool:
        """
        删除知识库
        """
        import shutil
        
        kb_path = os.path.join(self.base_path, kb_id)
        
        if os.path.exists(kb_path):
            # 清理客户端缓存
            if kb_id in self._clients:
                del self._clients[kb_id]
            
            # 删除目录
            shutil.rmtree(kb_path)
            return True
        
        return False
    
    def list_kb(self) -> List[Dict]:
        """
        列出所有知识库
        """
        if not os.path.exists(self.base_path):
            return []
        
        kb_list = []
        
        for kb_id in os.listdir(self.base_path):
            kb_path = os.path.join(self.base_path, kb_id)
            config_path = os.path.join(kb_path, "config.json")
            
            if os.path.isdir(kb_path) and os.path.exists(config_path):
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = json.load(f)
                
                # 获取统计信息
                try:
                    collection = self.get_collection(kb_id)
                    chunk_count = collection.count()
                except:
                    chunk_count = 0
                
                kb_list.append({
                    "kb_id": kb_id,
                    "kb_name": config.get('kb_name', kb_id),
                    "description": config.get('description', ''),
                    "chunk_count": chunk_count,
                    "created_at": config.get('created_at', '')
                })
        
        return kb_list
    
    def get_kb_config(self, kb_id: str) -> Optional[Dict]:
        """
        获取知识库配置
        """
        config_path = os.path.join(self.base_path, kb_id, "config.json")
        
        if os.path.exists(config_path):
            with open(config_path, 'r', encoding='utf-8') as f:
                return json.load(f)
        
        return None
    
    def get_kb_stats(self, kb_id: str) -> Dict:
        """
        获取知识库统计信息
        """
        collection = self.get_collection(kb_id)
        
        # chunk 数量
        chunk_count = collection.count()
        
        # 文档数量（通过 doc_id 去重）
        if chunk_count > 0:
            results = collection.get(include=["metadatas"])
            doc_ids = set(m.get('doc_id', '') for m in results['metadatas'])
            doc_count = len(doc_ids)
        else:
            doc_count = 0
        
        # 数据库大小
        db_path = os.path.join(self.base_path, kb_id, "db")
        db_size = self._get_dir_size(db_path) / (1024 * 1024)  # MB
        
        return {
            "kb_id": kb_id,
            "chunk_count": chunk_count,
            "doc_count": doc_count,
            "db_size_mb": round(db_size, 2)
        }
    
    def _get_dir_size(self, path: str) -> int:
        """
        获取目录大小（字节）
        """
        total_size = 0
        for dirpath, dirnames, filenames in os.walk(path):
            for filename in filenames:
                filepath = os.path.join(dirpath, filename)
                total_size += os.path.getsize(filepath)
        return total_size

# 全局数据库管理器实例
db_manager = DatabaseManager()
```

**测试数据库管理器**：

创建 `tests/test_db_manager.py`：

```python
from data_platform.backend.services.db_manager import db_manager
from datetime import datetime

def test_create_kb():
    """测试创建知识库"""
    kb_config = {
        "kb_id": "test_kb",
        "kb_name": "测试知识库",
        "description": "这是一个测试知识库",
        "created_at": datetime.now().isoformat(),
        "embedding_model": "text-embedding-3-small",
        "chunk_size": 1000,
        "chunk_overlap": 200,
        "db_type": "chromadb",
        "db_path": "./knowledge_bases/test_kb/db",
        "allowed_doc_types": [".txt", ".md", ".docx", ".pdf"]
    }
    
    result = db_manager.create_kb(kb_config)
    print(f"✅ 创建知识库: {result}")
    
    # 列出所有知识库
    kb_list = db_manager.list_kb()
    print(f"📚 知识库列表: {kb_list}")
    
    # 获取统计信息
    stats = db_manager.get_kb_stats("test_kb")
    print(f"📊 统计信息: {stats}")

if __name__ == "__main__":
    test_create_kb()
```

运行测试：

```bash
python -m tests.test_db_manager
```

------

## 📄 第四阶段：文档处理引擎（LangChain 层）

### Step 8: 创建文档处理服务（60分钟）

创建 `data_platform/backend/services/document_processor.py`：

```python
from langchain.document_loaders import (
    TextLoader,
    UnstructuredMarkdownLoader,
    UnstructuredWordDocumentLoader,
    PyPDFLoader,
    UnstructuredHTMLLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from typing import List, Dict, Optional
import os
import re
from pathlib import Path
from datetime import datetime
import uuid

from ..config.settings import settings
from .db_manager import db_manager

class DocumentProcessor:
    """文档处理引擎"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            model=settings.OPENAI_EMBEDDING_MODEL,
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_API_BASE
        )
    
    def process_document(
        self, 
        kb_id: str, 
        file_path: str,
        doc_id: Optional[str] = None
    ) -> Dict:
        """
        处理单个文档
        
        Args:
            kb_id: 知识库ID
            file_path: 文件路径
            doc_id: 文档ID（可选，不提供则自动生成）
        
        Returns:
            处理结果字典
        """
        try:
            # 生成文档ID
            if doc_id is None:
                doc_id = str(uuid.uuid4())
            
            # 1. 加载知识库配置
            kb_config = db_manager.get_kb_config(kb_id)
            if not kb_config:
                raise Exception(f"知识库 {kb_id} 不存在")
            
            # 2. 加载文档
            documents = self._load_document(file_path)
            
            if not documents:
                raise Exception("文档加载失败或内容为空")
            
            # 3. 提取元信息
            metadata = self._extract_metadata(file_path, documents[0].page_content)
            metadata['doc_id'] = doc_id
            metadata['kb_id'] = kb_id
            metadata['source'] = file_path
            metadata['processed_at'] = datetime.now().isoformat()
            
            # 4. 文档分块
            chunks = self._split_documents(
                documents,
                chunk_size=kb_config.get('chunk_size', 1000),
                chunk_overlap=kb_config.get('chunk_overlap', 200)
            )
            
            # 5. 向量化并存储
            self._store_chunks(kb_id, chunks, metadata)
            
            return {
                "success": True,
                "doc_id": doc_id,
                "filename": Path(file_path).name,
                "chunk_count": len(chunks),
                "metadata": metadata
            }
        
        except Exception as e:
            return {
                "success": False,
                "doc_id": doc_id,
                "error": str(e)
            }
    
    def _load_document(self, file_path: str) -> List:
        """
        根据文件类型加载文档
        """
        ext = Path(file_path).suffix.lower()
        
        loaders = {
            '.txt': TextLoader,
            '.md': UnstructuredMarkdownLoader,
            '.docx': UnstructuredWordDocumentLoader,
            '.pdf': PyPDFLoader,
            '.html': UnstructuredHTMLLoader,
            '.htm': UnstructuredHTMLLoader
        }
        
        loader_class = loaders.get(ext)
        
        if not loader_class:
            raise Exception(f"不支持的文件类型: {ext}")
        
        try:
            loader = loader_class(file_path)
            documents = loader.load()
            return documents
        except Exception as e:
            raise Exception(f"文档加载失败: {str(e)}")
    
    def _split_documents(
        self, 
        documents: List, 
        chunk_size: int = 1000,
        chunk_overlap: int = 200
    ) -> List:
        """
        文档分块
        """
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=[
                "\n\n",  # 段落
                "\n",    # 行
                "。",    # 中文句号
                "！",    # 感叹号
                "？",    # 问号
                "；",    # 分号
                ".",     # 英文句号
                " ",     # 空格
                ""       # 字符
            ],
            length_function=len
        )
        
        chunks = text_splitter.split_documents(documents)
        return chunks
    
    def _extract_metadata(self, file_path: str, content: str) -> Dict:
        """
        从文件名和内容中提取元信息
        """
        filename = Path(file_path).stem
        
        metadata = {
            "filename": Path(file_path).name,
            "doc_type": "unknown",
            "version": "1.0",
            "system": "unknown",
            "module": "general",
            "author": None
        }
        
        # 从文件名提取（例如：CRM_设计文档_登录模块_v3.0.docx）
        pattern = r'([A-Z]+)_(.+?)_(.+?)_v([\d.]+)'
        match = re.search(pattern, filename)
        
        if match:
            metadata['system'] = match.group(1)
            metadata['doc_type'] = match.group(2)
            metadata['module'] = match.group(3)
            metadata['version'] = match.group(4)
        else:
            # 简单模式：系统_文档类型_版本
            simple_pattern = r'([A-Z]+)_(.+?)_v([\d.]+)'
            simple_match = re.search(simple_pattern, filename)
            if simple_match:
                metadata['system'] = simple_match.group(1)
                metadata['doc_type'] = simple_match.group(2)
                metadata['version'] = simple_match.group(3)
        
        # 从内容提取文档类型
        content_preview = content[:1000]
        
        doc_type_keywords = {
            '需求文档': ['需求', '功能需求', '用户需求', '业务需求'],
            '设计文档': ['设计', '架构设计', '详细设计', '概要设计'],
            '运维文档': ['运维', '部署', '配置', '监控', '故障'],
            'API文档': ['API', '接口', 'endpoint', '请求', '响应']
        }
        
        for doc_type, keywords in doc_type_keywords.items():
            if any(kw in content_preview for kw in keywords):
                metadata['doc_type'] = doc_type
                break
        
        return metadata
    
    def _store_chunks(self, kb_id: str, chunks: List, base_metadata: Dict):
        """
        向量化并存储到数据库
        """
        collection = db_manager.get_collection(kb_id)
        
        for i, chunk in enumerate(chunks):
            # 合并元信息
            chunk_metadata = {
                **base_metadata,
                "chunk_index": i,
                "chunk_total": len(chunks)
            }
            
            # 向量化
            embedding = self.embeddings.embed_query(chunk.page_content)
            
            # 生成唯一 ID
            chunk_id = f"{base_metadata['doc_id']}_chunk_{i}"
            
            # 存储
            collection.add(
                embeddings=[embedding],
                documents=[chunk.page_content],
                metadatas=[chunk_metadata],
                ids=[chunk_id]
            )
    
    def delete_document(self, kb_id: str, doc_id: str) -> bool:
        """
        删除文档的所有 chunks
        """
        try:
            collection = db_manager.get_collection(kb_id)
            
            # 查找该文档的所有 chunks
            results = collection.get(
                where={"doc_id": doc_id}
            )
            
            if results['ids']:
                collection.delete(ids=results['ids'])
                return True
            
            return False
        
        except Exception as e:
            print(f"删除文档失败: {e}")
            return False

# 全局文档处理器实例
doc_processor = DocumentProcessor()
```

**测试文档处理**：

创建 `tests/test_document_processor.py`：

```python
from data_platform.backend.services.document_processor import doc_processor
from data_platform.backend.services.db_manager import db_manager

def test_process_document():
    """测试文档处理"""
    
    # 创建测试文档
    test_file = "./test_doc.txt"
    with open(test_file, 'w', encoding='utf-8') as f:
        f.write("""
# CRM 系统登录模块设计文档 v2.0

## 1. 概述
本文档描述 CRM 系统登录模块的详细设计。

## 2. 功能需求
### 2.1 用户名密码登录
用户可以使用用户名和密码进行登录。

### 2.2 手机验证码登录
用户可以使用手机号接收验证码登录。

## 3. 技术实现
### 3.1 密码加密
使用 bcrypt 算法加密存储密码。

### 3.2 Token 生成
使用 JWT 生成访问令牌。
        """)
    
    # 处理文档
    result = doc_processor.process_document(
        kb_id="test_kb",
        file_path=test_file
    )
    
    print(f"✅ 处理结果: {result}")
    
    # 查看统计
    stats = db_manager.get_kb_stats("test_kb")
    print(f"📊 知识库统计: {stats}")
    
    # 清理测试文件
    import os
    os.remove(test_file)

if __name__ == "__main__":
    test_process_document()
```

------

### 💡 第一阶段总结检查点

到这里，你应该已经完成：

- ✅ 项目目录结构
- ✅ Python 环境与依赖安装
- ✅ 配置管理模块
- ✅ 数据模型定义
- ✅ 数据库管理服务
- ✅ 文档处理引擎

**验证测试**：

```bash
# 运行所有测试
python -m tests.test_db_manager
python -m tests.test_document_processor

# 应该看到：
# ✅ 创建知识库成功
# ✅ 文档处理成功
# 📊 chunk 数量 > 0
```

------

## 🌐 第五阶段：数据中台 API 开发

### Step 9: 创建检索服务（40分钟）

创建 `data_platform/backend/services/retriever.py`：

```python
from typing import List, Dict, Optional
from langchain.embeddings import OpenAIEmbeddings

from ..config.settings import settings
from .db_manager import db_manager

class RetrieverService:
    """检索服务"""
    
    def __init__(self):
        self.embeddings = OpenAIEmbeddings(
            model=settings.OPENAI_EMBEDDING_MODEL,
            openai_api_key=settings.OPENAI_API_KEY,
            openai_api_base=settings.OPENAI_API_BASE
        )
    
    def retrieve(
        self,
        kb_id: str,
        query: str,
        top_k: int = 5,
        min_score: float = 0.6,
        filters: Optional[Dict] = None
    ) -> List[Dict]:
        """
        语义检索
        
        Args:
            kb_id: 知识库ID
            query: 查询问题
            top_k: 返回结果数量
            min_score: 最低相似度阈值
            filters: 元数据过滤条件
        
        Returns:
            检索结果列表
        """
        try:
            # 获取 collection
            collection = db_manager.get_collection(kb_id)
            
            # 向量化查询
            query_embedding = self.embeddings.embed_query(query)
            
            # 检索
            results = collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k * 2,  # 多取一些，后面过滤
                where=filters,
                include=["documents", "metadatas", "distances"]
            )
            
            # 格式化结果
            chunks = []
            
            if results['documents'] and results['documents'][0]:
                for i, doc in enumerate(results['documents'][0]):
                    # ChromaDB 返回的是距离（越小越相似），需要转换为相似度分数
                    distance = results['distances'][0][i]
                    score = 1 / (1 + distance)  # 转换为 0-1 之间的分数
                    
                    if score >= min_score:
                        chunks.append({
                            "content": doc,
                            "metadata": results['metadatas'][0][i],
                            "score": round(score, 4)
                        })
            
            # 按分数排序并限制数量
            chunks.sort(key=lambda x: x['score'], reverse=True)
            
            return chunks[:top_k]
        
        except Exception as e:
            print(f"检索失败: {e}")
            return []
    
    def multi_kb_retrieve(
        self,
        kb_ids: List[str],
        query: str,
        top_k: int = 5,
        merge_strategy: str = "score"
    ) -> List[Dict]:
        """
        多知识库联合检索
        
        Args:
            kb_ids: 知识库ID列表
            query: 查询问题
            top_k: 返回结果数量
            merge_strategy: 合并策略 (score/round_robin/kb_priority)
        
        Returns:
            检索结果列表
        """
        all_results = []
        
        # 从每个知识库检索
        for kb_id in kb_ids:
            kb_results = self.retrieve(
                kb_id=kb_id,
                query=query,
                top_k=top_k
            )
            
            # 添加来源标识
            for result in kb_results:
                result['metadata']['source_kb'] = kb_id
                all_results.append(result)
        
        # 合并策略
        if merge_strategy == "score":
            # 按相似度排序
            all_results.sort(key=lambda x: x['score'], reverse=True)
            return all_results[:top_k]
        
        elif merge_strategy == "round_robin":
            # 轮流取每个知识库的结果
            final_results = []
            kb_indices = {kb_id: 0 for kb_id in kb_ids}
            
            while len(final_results) < top_k and any(kb_indices.values()):
                for kb_id in kb_ids:
                    kb_results = [r for r in all_results if r['metadata']['source_kb'] == kb_id]
                    idx = kb_indices[kb_id]
                    if idx < len(kb_results):
                        final_results.append(kb_results[idx])
                        kb_indices[kb_id] += 1
                        if len(final_results) >= top_k:
                            break
            
            return final_results
        
        else:  # kb_priority
            # 按知识库顺序优先
            final_results = []
            for kb_id in kb_ids:
                kb_results = [r for r in all_results if r['metadata']['source_kb'] == kb_id]
                final_results.extend(kb_results)
                if len(final_results) >= top_k:
                    break
            
            return final_results[:top_k]

# 全局检索服务实例
retriever = RetrieverService()
```

### Step 10: 创建 API 路由 - 知识库管理（30分钟）

创建 `data_platform/backend/api/kb_management.py`：

```python
from fastapi import APIRouter, HTTPException, status
from typing import List

from ..models.schemas import (
    KnowledgeBaseCreate,
    KnowledgeBaseInfo,
    APIResponse
)
from ..services.db_manager import db_manager
from datetime import datetime

router = APIRouter(prefix="/api/kb", tags=["知识库管理"])

@router.post("/create", response_model=APIResponse)
async def create_kb(kb_data: KnowledgeBaseCreate):
    """创建新知识库"""
    try:
        # 检查是否已存在
        existing = db_manager.get_kb_config(kb_data.kb_id)
        if existing:
            raise HTTPException(
                status_code=status.HTTP_400_BAD_REQUEST,
                detail=f"知识库 {kb_data.kb_id} 已存在"
            )
        
        # 构建配置
        kb_config = {
            "kb_id": kb_data.kb_id,
            "kb_name": kb_data.kb_name,
            "description": kb_data.description or "",
            "created_at": datetime.now().isoformat(),
            "embedding_model": kb_data.embedding_model,
            "chunk_size": kb_data.chunk_size,
            "chunk_overlap": kb_data.chunk_overlap,
            "db_type": "chromadb",
            "db_path": f"./knowledge_bases/{kb_data.kb_id}/db",
            "allowed_doc_types": [".txt", ".md", ".docx", ".pdf", ".html"]
        }
        
        # 创建知识库
        success = db_manager.create_kb(kb_config)
        
        if success:
            return APIResponse(
                success=True,
                message=f"知识库 {kb_data.kb_name} 创建成功",
                data={"kb_id": kb_data.kb_id}
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
                detail="知识库创建失败"
            )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/list", response_model=List[KnowledgeBaseInfo])
async def list_kb():
    """获取所有知识库列表"""
    try:
        kb_list = db_manager.list_kb()
        
        result = []
        for kb in kb_list:
            stats = db_manager.get_kb_stats(kb['kb_id'])
            
            result.append(KnowledgeBaseInfo(
                kb_id=kb['kb_id'],
                kb_name=kb['kb_name'],
                description=kb.get('description', ''),
                created_at=kb.get('created_at', ''),
                doc_count=stats.get('doc_count', 0),
                chunk_count=stats.get('chunk_count', 0),
                db_size_mb=stats.get('db_size_mb', 0.0)
            ))
        
        return result
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/{kb_id}/info", response_model=KnowledgeBaseInfo)
async def get_kb_info(kb_id: str):
    """获取指定知识库详情"""
    try:
        config = db_manager.get_kb_config(kb_id)
        if not config:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"知识库 {kb_id} 不存在"
            )
        
        stats = db_manager.get_kb_stats(kb_id)
        
        return KnowledgeBaseInfo(
            kb_id=config['kb_id'],
            kb_name=config['kb_name'],
            description=config.get('description', ''),
            created_at=config.get('created_at', ''),
            doc_count=stats.get('doc_count', 0),
            chunk_count=stats.get('chunk_count', 0),
            db_size_mb=stats.get('db_size_mb', 0.0)
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.delete("/{kb_id}", response_model=APIResponse)
async def delete_kb(kb_id: str):
    """删除知识库"""
    try:
        success = db_manager.delete_kb(kb_id)
        
        if success:
            return APIResponse(
                success=True,
                message=f"知识库 {kb_id} 删除成功"
            )
        else:
            raise HTTPException(
                status_code=status.HTTP_404_NOT_FOUND,
                detail=f"知识库 {kb_id} 不存在"
            )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )

@router.get("/{kb_id}/stats")
async def get_kb_stats(kb_id: str):
    """获取知识库统计信息"""
    try:
        stats = db_manager.get_kb_stats(kb_id)
        return stats
    
    except Exception as e:
        raise HTTPException(
            status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,
            detail=str(e)
        )
```

### Step 11: 创建 API 路由 - 文档管理（40分钟）

创建 `data_platform/backend/api/document_management.py`：

```python
from fastapi import APIRouter, UploadFile, File, HTTPException, BackgroundTasks
from typing import List
import os
import shutil
import uuid

from ..models.schemas import APIResponse, DocumentInfo, DocumentStatus
from ..services.document_processor import doc_processor
from ..services.db_manager import db_manager
from ..config.settings import settings
from datetime import datetime

router = APIRouter(prefix="/api/kb/{kb_id}/documents", tags=["文档管理"])

# 存储文档处理状态
document_status = {}

@router.post("/upload", response_model=APIResponse)
async def upload_documents(
    kb_id: str,
    files: List[UploadFile] = File(...),
    background_tasks: BackgroundTasks = None
):
    """
    上传文档到指定知识库
    支持批量上传，异步处理
    """
    try:
        # 验证知识库是否存在
        config = db_manager.get_kb_config(kb_id)
        if not config:
            raise HTTPException(status_code=404, detail=f"知识库 {kb_id} 不存在")
        
        results = []
        
        for file in files:
            # 验证文件类型
            file_ext = os.path.splitext(file.filename)[1].lower()
            if file_ext not in settings.ALLOWED_FILE_TYPES:
                results.append({
                    "filename": file.filename,
                    "status": "failed",
                    "error": f"不支持的文件类型: {file_ext}"
                })
                continue
            
            # 验证文件大小
            file.file.seek(0, 2)  # 移动到文件末尾
            file_size = file.file.tell()
            file.file.seek(0)  # 重置到开头
            
            if file_size > settings.MAX_FILE_SIZE_MB * 1024 * 1024:
                results.append({
                    "filename": file.filename,
                    "status": "failed",
                    "error": f"文件过大，超过 {settings.MAX_FILE_SIZE_MB}MB"
                })
                continue
            
            # 生成文档 ID
            doc_id = str(uuid.uuid4())
            
            # 保存文件
            docs_dir = os.path.join(settings.CHROMA_DB_PATH, kb_id, "docs")
            os.makedirs(docs_dir, exist_ok=True)
            
            file_path = os.path.join(docs_dir, file.filename)
            
            with open(file_path, "wb") as buffer:
                shutil.copyfileobj(file.file, buffer)
            
            # 初始化状态
            document_status[doc_id] = {
                "doc_id": doc_id,
                "kb_id": kb_id,
                "filename": file.filename,
                "file_size": file_size,
                "status": DocumentStatus.PENDING,
                "created_at": datetime.now().isoformat()
            }
            
            # 添加到后台任务
            background_tasks.add_task(
                process_document_task,
                kb_id=kb_id,
                doc_id=doc_id,
                file_path=file_path
            )
            
            results.append({
                "doc_id": doc_id,
                "filename": file.filename,
                "status": "processing"
            })
        
        return APIResponse(
            success=True,
            message=f"成功上传 {len(results)} 个文件",
            data={"documents": results}
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

def process_document_task(kb_id: str, doc_id: str, file_path: str):
    """
    后台处理文档任务
    """
    try:
        # 更新状态
        document_status[doc_id]["status"] = DocumentStatus.PROCESSING
        
        # 处理文档
        result = doc_processor.process_document(kb_id, file_path, doc_id)
        
        if result["success"]:
            document_status[doc_id].update({
                "status": DocumentStatus.COMPLETED,
                "chunk_count": result["chunk_count"],
                "processed_at": datetime.now().isoformat(),
                "metadata": result.get("metadata", {})
            })
        else:
            document_status[doc_id].update({
                "status": DocumentStatus.FAILED,
                "error_message": result.get("error", "未知错误")
            })
    
    except Exception as e:
        document_status[doc_id].update({
            "status": DocumentStatus.FAILED,
            "error_message": str(e)
        })

@router.get("", response_model=List[DocumentInfo])
async def list_documents(kb_id: str):
    """获取知识库的文档列表"""
    try:
        # 从状态字典获取
        kb_docs = [
            DocumentInfo(**doc) 
            for doc in document_status.values() 
            if doc['kb_id'] == kb_id
        ]
        
        return kb_docs
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.get("/{doc_id}", response_model=DocumentInfo)
async def get_document_info(kb_id: str, doc_id: str):
    """获取文档详情"""
    try:
        if doc_id not in document_status:
            raise HTTPException(status_code=404, detail="文档不存在")
        
        doc = document_status[doc_id]
        
        if doc['kb_id'] != kb_id:
            raise HTTPException(status_code=404, detail="文档不在该知识库中")
        
        return DocumentInfo(**doc)
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.delete("/{doc_id}", response_model=APIResponse)
async def delete_document(kb_id: str, doc_id: str):
    """删除文档"""
    try:
        # 从向量数据库删除
        success = doc_processor.delete_document(kb_id, doc_id)
        
        if success:
            # 删除状态记录
            if doc_id in document_status:
                filename = document_status[doc_id]['filename']
                del document_status[doc_id]
                
                # 删除原始文件
                file_path = os.path.join(
                    settings.CHROMA_DB_PATH, 
                    kb_id, 
                    "docs", 
                    filename
                )
                if os.path.exists(file_path):
                    os.remove(file_path)
            
            return APIResponse(
                success=True,
                message=f"文档 {doc_id} 删除成功"
            )
        else:
            raise HTTPException(status_code=404, detail="文档不存在")
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.put("/{doc_id}/reprocess", response_model=APIResponse)
async def reprocess_document(
    kb_id: str, 
    doc_id: str,
    background_tasks: BackgroundTasks
):
    """重新处理文档"""
    try:
        if doc_id not in document_status:
            raise HTTPException(status_code=404, detail="文档不存在")
        
        doc = document_status[doc_id]
        
        # 先删除旧数据
        doc_processor.delete_document(kb_id, doc_id)
        
        # 重新处理
        file_path = os.path.join(
            settings.CHROMA_DB_PATH,
            kb_id,
            "docs",
            doc['filename']
        )
        
        background_tasks.add_task(
            process_document_task,
            kb_id=kb_id,
            doc_id=doc_id,
            file_path=file_path
        )
        
        return APIResponse(
            success=True,
            message="文档已加入重新处理队列"
        )
    
    except HTTPException:
        raise
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Step 12: 创建 API 路由 - 检索接口（30分钟）

创建 `data_platform/backend/api/retrieve.py`：

```python
from fastapi import APIRouter, HTTPException
from typing import List

from ..models.schemas import RetrieveRequest, RetrieveResponse, ChunkResult
from ..services.retriever import retriever

router = APIRouter(prefix="/api", tags=["检索"])

@router.post("/retrieve", response_model=RetrieveResponse)
async def retrieve_knowledge(request: RetrieveRequest):
    """
    语义检索接口（供 Dify 调用）
    """
    try:
        chunks = retriever.retrieve(
            kb_id=request.kb_id,
            query=request.query,
            top_k=request.top_k,
            min_score=request.min_score,
            filters=request.filters
        )
        
        chunk_results = [ChunkResult(**chunk) for chunk in chunks]
        
        return RetrieveResponse(
            success=True,
            query=request.query,
            kb_id=request.kb_id,
            chunks=chunk_results,
            total=len(chunk_results)
        )
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@router.post("/retrieve/multi")
async def multi_kb_retrieve(
    query: str,
    kb_ids: List[str],
    top_k: int = 5,
    merge_strategy: str = "score"
):
    """
    多知识库联合检索
    """
    try:
        chunks = retriever.multi_kb_retrieve(
            kb_ids=kb_ids,
            query=query,
            top_k=top_k,
            merge_strategy=merge_strategy
        )
        
        return {
            "success": True,
            "query": query,
            "kb_ids": kb_ids,
            "chunks": chunks,
            "total": len(chunks)
        }
    
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))
```

### Step 13: 创建 FastAPI 主入口（20分钟）

创建 `data_platform/backend/main.py`：

```python
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import logging
from datetime import datetime

from .config.settings import settings
from .api import kb_management, document_management, retrieve

# 配置日志
logging.basicConfig(
    level=settings.LOG_LEVEL,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(settings.LOG_FILE),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

# 创建 FastAPI 应用
app = FastAPI(
    title="知识中台 API",
    description="LangChain + Dify 企业级知识中台",
    version="1.0.0"
)

# 配置 CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # 生产环境应该限制具体域名
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# 注册路由
app.include_router(kb_management.router)
app.include_router(document_management.router)
app.include_router(retrieve.router)

@app.get("/")
async def root():
    """健康检查"""
    return {
        "status": "running",
        "service": "知识中台 API",
        "version": "1.0.0",
        "timestamp": datetime.now().isoformat()
    }

@app.get("/health")
async def health_check():
    """健康检查"""
    return {"status": "healthy"}

@app.on_event("startup")
async def startup_event():
    """启动事件"""
    logger.info("🚀 知识中台 API 启动成功")
    logger.info(f"📍 API 地址: http://{settings.API_HOST}:{settings.API_PORT}")
    logger.info(f"📚 API 文档: http://{settings.API_HOST}:{settings.API_PORT}/docs")

@app.on_event("shutdown")
async def shutdown_event():
    """关闭事件"""
    logger.info("👋 知识中台 API 关闭")

if __name__ == "__main__":
    import uvicorn
    
    uvicorn.run(
        "main:app",
        host=settings.API_HOST,
        port=settings.API_PORT,
        reload=settings.DEBUG
    )
```

### Step 14: 启动并测试后端 API（30分钟）

**启动服务**：

```bash
cd data_platform/backend
python main.py

# 或使用 uvicorn
uvicorn main:app --reload --host 0.0.0.0 --port 8000
```

**访问 API 文档**： 浏览器打开 `http://localhost:8000/docs`

**API 测试脚本**：

创建 `tests/test_api.py`：

```python
import requests
import json

BASE_URL = "http://localhost:8000"

def test_create_kb():
    """测试创建知识库"""
    url = f"{BASE_URL}/api/kb/create"
    
    data = {
        "kb_id": "test_crm",
        "kb_name": "CRM系统知识库",
        "description": "包含CRM系统的需求、设计、运维文档",
        "embedding_model": "text-embedding-3-small",
        "chunk_size": 1000,
        "chunk_overlap": 200
    }
    
    response = requests.post(url, json=data)
    print(f"✅ 创建知识库: {response.json()}")
    return response.json()

def test_list_kb():
    """测试获取知识库列表"""
    url = f"{BASE_URL}/api/kb/list"
    
    response = requests.get(url)
    print(f"📚 知识库列表: {json.dumps(response.json(), indent=2, ensure_ascii=False)}")
    return response.json()

def test_upload_document():
    """测试上传文档"""
    url = f"{BASE_URL}/api/kb/test_crm/documents/upload"
    
    # 创建测试文件
    test_content = """
    # CRM系统登录模块设计 v2.0
    
    ## 功能说明
    系统支持三种登录方式：
    1. 用户名密码登录
    2. 手机验证码登录  
    3. 企业微信扫码登录
    """
    
    files = {
        'files': ('CRM_设计文档_登录模块_v2.0.txt', test_content.encode('utf-8'))
    }
    
    response = requests.post(url, files=files)
    print(f"📄 上传文档: {response.json()}")
    return response.json()

def test_retrieve():
    """测试检索"""
    import time
    time.sleep(3)  # 等待文档处理完成
    
    url = f"{BASE_URL}/api/retrieve"
    
    data = {
        "query": "CRM系统有哪些登录方式？",
        "kb_id": "test_crm",
        "top_k": 3
    }
    
    response = requests.post(url, json=data)
    result = response.json()
    
    print(f"\n🔍 检索结果:")
    print(f"问题: {result['query']}")
    print(f"找到 {result['total']} 条相关内容:\n")
    
    for i, chunk in enumerate(result['chunks'], 1):
        print(f"{i}. [相似度: {chunk['score']}]")
        print(f"   内容: {chunk['content'][:100]}...")
        print(f"   元信息: {chunk['metadata']}\n")
    
    return result

if __name__ == "__main__":
    print("=" * 50)
    print("开始测试知识中台 API")
    print("=" * 50)
    
    # 1. 创建知识库
    test_create_kb()
    
    # 2. 查看知识库列表
    test_list_kb()
    
    # 3. 上传文档
    test_upload_document()
    
    # 4. 检索测试
    test_retrieve()
    
    print("\n" + "=" * 50)
    print("✅ 所有测试完成！")
    print("=" * 50)
```

运行测试：

```bash
python -m tests.test_api
```

------

## 💡 第二阶段总结检查点

到这里，你的后端 API 应该已经完全可用：

- ✅ 知识库 CRUD 接口
- ✅ 文档上传与处理
- ✅ 语义检索接口
- ✅ 多知识库联合检索

**验证**：

1. API 文档可访问：http://localhost:8000/docs
2. 所有测试通过
3. 能创建知识库、上传文档、检索内容

------

## 🔗 第六阶段：Dify 对接

### Step 15: 在 Dify 中配置自定义工具（20分钟）

**1. 登录 Dify 控制台**

访问你的 Dify 实例（cloud.dify.ai 或自部署地址）

**2. 创建自定义工具**

进入「工具」→「创建自定义工具」→「API 工具」

**工具配置（JSON 格式）**：

```json
{
  "openapi": "3.0.0",
  "info": {
    "title": "知识中台检索工具",
    "description": "从企业知识中台检索相关文档内容",
    "version": "1.0.0"
  },
  "servers": [
    {
      "url": "http://your-server-ip:8000"
    }
  ],
  "paths": {
    "/api/retrieve": {
      "post": {
        "summary": "语义检索",
        "description": "根据问题检索知识库中的相关内容",
        "operationId": "retrieve_knowledge",
        "requestBody": {
          "required": true,
          "content": {
            "application/json": {
              "schema": {
                "type": "object",
                "properties": {
                  "query": {
                    "type": "string",
                    "description": "用户的问题"
                  },
                  "kb_id": {
                    "type": "string",
                    "description": "知识库ID",
                    "default": "test_crm"
                  },
                  "top_k": {
                    "type": "integer",
                    "description": "返回结果数量",
                    "default": 5
                  }
                },
                "required": ["query", "kb_id"]
              }
            }
          }
        },
        "responses": {
          "200": {
            "description": "检索成功",
            "content": {
              "application/json": {
                "schema": {
                  "type": "object",
                  "properties": {
                    "success": {
                      "type": "boolean"
                    },
                    "chunks": {
                      "type": "array",
                      "items": {
                        "type": "object",
                        "properties": {
                          "content": {
                            "type": "string"
                          },
                          "score": {
                            "type": "number"
                          }
                        }
                      }
                    },
                    "total": {
                      "type": "integer"
                    }
                  }
                }
              }
            }
          }
        }
      }
    }
  }
}
```

**3. 在 Dify Workflow 中使用工具**

创建一个新的 Chatflow 或 Workflow：

```
节点1: 开始节点
  ↓ 接收用户输入
节点2: 工具调用节点（知识中台检索）
  - 工具: knowledge_platform_retrieve
  - 参数配置:
    * query: {{sys.query}}  (用户问题)
    * kb_id: test_crm
    * top_k: 5
  ↓ 输出: chunks (检索结果)
节点3: 代码节点（格式化上下文）
  - 输入: {{retrieve.chunks}}
  - 代码:
    ```python
    def main(chunks):
        context = "\n\n".join([
            f"[相关文档 {i+1}]\n{chunk['content']}"
            for i, chunk in enumerate(chunks)
        ])
        return {
            "context": context
        }
    ```
  ↓ 输出: context
节点4: LLM 节点
  - 模型: GPT-4 / Claude
  - System Prompt:
    "你是一个专业的技术助手。请根据以下知识库内容回答用户问题。
    如果知识库中没有相关信息，请明确告知用户。"
  - User Prompt:
    "知识库内容：
    {{code.context}}
    
    用户问题：{{sys.query}}
    
    请基于以上知识库内容回答问题："
  ↓
节点5: 结束节点
  - 输出: {{llm.text}}
```

**4. 测试 Workflow**

在 Dify 聊天界面测试：

```
用户: CRM系统支持哪些登录方式？

预期流程:
1. 问题发送到知识中台 API
2. 返回相关文档片段
3. LLM 基于文档生成答案
4. 返回给用户：
   "根据知识库，CRM系统支持三种登录方式：
    1. 用户名密码登录
    2. 手机验证码登录
    3. 企业微信扫码登录"
```

### Step 16: 优化检索结果展示（可选，15分钟）

**在 Workflow 中添加引用展示**：

修改 LLM 节点的输出格式：

```python
# 在代码节点中添加
def main(chunks):
    context = "\n\n".join([
        f"[文档{i+1}] {chunk['content']}"
        for i, chunk in enumerate(chunks)
    ])
    
    # 生成引用信息
    references = [
        {
            "index": i+1,
            "filename": chunk['metadata'].get('filename', '未知'),
            "version": chunk['metadata'].get('version', '1.0'),
            "score": chunk['score']
        }
        for i, chunk in enumerate(chunks)
    ]
    
    return {
        "context": context,
        "references": references
    }
```

在最终回复中附加引用：

```
{{llm.text}}

---
📚 参考文档：
{{#each code.references}}
  {{index}}. {{filename}} (v{{version}}) - 相似度: {{score}}
{{/each}}
```

------

## 🔧 第七阶段：高级功能开发

### Step 17: 添加文档监听器（自动化处理）（30分钟）

创建 `data_platform/backend/services/watcher.py`：

```python
from watchdog.observers import Observer
from watchdog.events import FileSystemEventHandler
import time
import os
from pathlib import Path

from ..services.document_processor import doc_processor
from ..config.settings import settings
import logging

logger = logging.getLogger(__name__)

class DocumentWatcher(FileSystemEventHandler):
    """文档监听器"""
    
    def __init__(self, kb_id: str):
        self.kb_id = kb_id
        self.watch_path = os.path.join(settings.CHROMA_DB_PATH, kb_id, "docs")
        self.processing = set()  # 正在处理的文件
    
    def on_created(self, event):
        """文件创建事件"""
        if event.is_directory:
            return
        
        file_path = event.src_path
        
        # 检查文件类型
        if not any(file_path.endswith(ext) for ext in settings.ALLOWED_FILE_TYPES):
            return
        
        # 避免重复处理
        if file_path in self.processing:
            return
        
        self.processing.add(file_path)
        
        logger.info(f"📄 检测到新文档: {file_path}")
        
        # 等待文件写入完成
        time.sleep(2)
        
        try:
            # 处理文档
            result = doc_processor.process_document(
                kb_id=self.kb_id,
                file_path=file_path
            )
            
            if result['success']:
                logger.info(f"✅ 文档处理成功: {result['filename']}, chunks: {result['chunk_count']}")
            else:
                logger.error(f"❌ 文档处理失败: {result.get('error')}")
        
        except Exception as e:
            logger.error(f"❌ 文档处理异常: {e}")
        
        finally:
            self.processing.remove(file_path)
    
    def on_modified(self, event):
        """文件修改事件"""
        if event.is_directory:
            return
        
        logger.info(f"📝 文档已修改: {event.src_path}")
        # 可以选择重新处理文档

class WatcherManager:
    """监听器管理器"""
    
    def __init__(self):
        self.observers = {}  # {kb_id: Observer}
    
    def start_watcher(self, kb_id: str):
        """启动指定知识库的监听"""
        if kb_id in self.observers:
            logger.warning(f"知识库 {kb_id} 的监听器已在运行")
            return
        
        watch_path = os.path.join(settings.CHROMA_DB_PATH, kb_id, "docs")
        
        if not os.path.exists(watch_path):
            os.makedirs(watch_path, exist_ok=True)
        
        event_handler = DocumentWatcher(kb_id)
        observer = Observer()
        observer.schedule(event_handler, watch_path, recursive=False)
        observer.start()
        
        self.observers[kb_id] = observer
        logger.info(f"👀 已启动知识库 {kb_id} 的文档监听")
    
    def stop_watcher(self, kb_id: str):
        """停止指定知识库的监听"""
        if kb_id not in self.observers:
            return
        
        self.observers[kb_id].stop()
        self.observers[kb_id].join()
        del self.observers[kb_id]
        
        logger.info(f"🛑 已停止知识库 {kb_id} 的文档监听")
    
    def start_all_watchers(self):
        """启动所有知识库的监听"""
        from .db_manager import db_manager
        
        kb_list = db_manager.list_kb()
        
        for kb in kb_list:
            self.start_watcher(kb['kb_id'])
        
        logger.info(f"👀 已启动 {len(kb_list)} 个知识库的文档监听")
    
    def stop_all_watchers(self):
        """停止所有监听"""
        for kb_id in list(self.observers.keys()):
            self.stop_watcher(kb_id)

# 全局监听器管理器
watcher_manager = WatcherManager()
```

**在 main.py 中启用监听**：

```python
# 在 main.py 中添加
from .services.watcher import watcher_manager

@app.on_event("startup")
async def startup_event():
    logger.info("🚀 知识中台 API 启动成功")
    
    # 启动所有知识库的文档监听
    watcher_manager.start_all_watchers()
    
    logger.info(f"📍 API 地址: http://{settings.API_HOST}:{settings.API_PORT}")
    logger.info(f"📚 API 文档: http://{settings.API_HOST}:{settings.API_PORT}/docs")

@app.on_event("shutdown")
async def shutdown_event():
    logger.info("👋 关闭文档监听器")
    watcher_manager.stop_all_watchers()
    logger.info("👋 知识中台 API 关闭")
```

**测试自动监听**：

```bash
# 启动服务
python main.py

# 在另一个终端，复制文件到 docs 目录
echo "测试自动处理" > ./knowledge_bases/test_crm/docs/test_auto.txt

# 查看日志，应该看到：
# 📄 检测到新文档: ./knowledge_bases/test_crm/docs/test_auto.txt
# ✅ 文档处理成功: test_auto.txt, chunks: 1
```

### Step 18: 添加数据库备份功能（20分钟）

创建 `data_platform/backend/services/backup.py`：

```python
import os
import shutil
import tarfile
from datetime import datetime
from pathlib import Path

from ..config.settings import settings
import logging

logger = logging.getLogger(__name__)

class BackupService:
    """备份服务"""
    
    def __init__(self):
        self.backup_dir = "./backups"
        os.makedirs(self.backup_dir, exist_ok=True)
    
    def backup_kb(self, kb_id: str) -> dict:
        """备份指定知识库"""
        try:
            kb_path = os.path.join(settings.CHROMA_DB_PATH, kb_id)
            
            if not os.path.exists(kb_path):
                return {
                    "success": False,
                    "error": f"知识库 {kb_id} 不存在"
                }
            
            # 生成备份文件名
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            backup_name = f"{kb_id}_{timestamp}.tar.gz"
            backup_path = os.path.join(self.backup_dir, backup_name)
            
            # 创建压缩包
            with tarfile.open(backup_path, "w:gz") as tar:
                tar.add(kb_path, arcname=kb_id)
            
            # 获取备份文件大小
            backup_size = os.path.getsize(backup_path) / (1024 * 1024)  # MB
            
            logger.info(f"✅ 知识库 {kb_id} 备份成功: {backup_name}")
            
            return {
                "success": True,
                "kb_id": kb_id,
                "backup_file": backup_name,
                "backup_path": backup_path,
                "size_mb": round(backup_size, 2),
                "timestamp": timestamp
            }
        
        except Exception as e:
            logger.error(f"❌ 备份失败: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def backup_all(self) -> dict:
        """备份所有知识库"""
        from .db_manager import db_manager
        
        kb_list = db_manager.list_kb()
        results = []
        
        for kb in kb_list:
            result = self.backup_kb(kb['kb_id'])
            results.append(result)
        
        success_count = sum(1 for r in results if r['success'])
        
        return {
            "success": True,
            "total": len(kb_list),
            "success_count": success_count,
            "results": results
        }
    
    def restore_kb(self, backup_file: str) -> dict:
        """从备份恢复知识库"""
        try:
            backup_path = os.path.join(self.backup_dir, backup_file)
            
            if not os.path.exists(backup_path):
                return {
                    "success": False,
                    "error": "备份文件不存在"
                }
            
            # 解压
            with tarfile.open(backup_path, "r:gz") as tar:
                tar.extractall(path=settings.CHROMA_DB_PATH)
            
            logger.info(f"✅ 从备份恢复成功: {backup_file}")
            
            return {
                "success": True,
                "backup_file": backup_file
            }
        
        except Exception as e:
            logger.error(f"❌ 恢复失败: {e}")
            return {
                "success": False,
                "error": str(e)
            }
    
    def list_backups(self) -> list:
        """列出所有备份"""
        backups = []
        
        for filename in os.listdir(self.backup_dir):
            if filename.endswith(".tar.gz"):
                file_path = os.path.join(self.backup_dir, filename)
                file_size = os.path.getsize(file_path) / (1024 * 1024)
                
                backups.append({
                    "filename": filename,
                    "size_mb": round(file_size, 2),
                    "created_at": datetime.fromtimestamp(
                        os.path.getctime(file_path)
                    ).isoformat()
                })
        
        return sorted(backups, key=lambda x: x['created_at'], reverse=True)
    
    def cleanup_old_backups(self, keep_count: int = 10):
        """清理旧备份，保留最近 N 个"""
        backups = self.list_backups()
        
        if len(backups) <= keep_count:
            return {
                "success": True,
                "message": "备份数量未超过限制",
                "deleted": 0
            }
        
        # 删除旧备份
        to_delete = backups[keep_count:]
        deleted_count = 0
        
        for backup in to_delete:
            file_path = os.path.join(self.backup_dir, backup['filename'])
            try:
                os.remove(file_path)
                deleted_count += 1
            except Exception as e:
                logger.error(f"删除备份失败: {e}")
        
        return {
            "success": True,
            "deleted": deleted_count,
            "kept": keep_count
        }

# 全局备份服务实例
backup_service = BackupService()
```

**添加备份 API**：

创建 `data_platform/backend/api/maintenance.py`：

```python
from fastapi import APIRouter, HTTPException

from ..models.schemas import APIResponse
from ..services.backup import backup_service

router = APIRouter(prefix="/api/maintenance", tags=["维护"])

@router.post("/backup/{kb_id}", response_model=APIResponse)
async def backup_kb(kb_id: str):
    """备份指定知识库"""
    result = backup_service.backup_kb(kb_id)
    
    if result['success']:
        return APIResponse(
            success=True,
            message="备份成功",
            data=result
        )
    else:
        raise HTTPException(status_code=500, detail=result['error'])

@router.post("/backup/all", response_model=APIResponse)
async def backup_all():
    """备份所有知识库"""
    result = backup_service.backup_all()
    return APIResponse(
        success=True,
        message=f"备份完成，成功 {result['success_count']}/{result['total']}",
        data=result
    )

@router.get("/backups")
async def list_backups():
    """列出所有备份"""
    return backup_service.list_backups()

@router.post("/restore/{backup_file}", response_model=APIResponse)
async def restore_backup(backup_file: str):
    """从备份恢复"""
    result = backup_service.restore_kb(backup_file)
    
    if result['success']:
        return APIResponse(
            success=True,
            message="恢复成功",
            data=result
        )
    else:
        raise HTTPException(status_code=500, detail=result['error'])

@router.post("/cleanup", response_model=APIResponse)
async def cleanup_backups(keep_count: int = 10):
    """清理旧备份"""
    result = backup_service.cleanup_old_backups(keep_count)
    return APIResponse(
        success=True,
        message=f"清理完成，删除 {result['deleted']} 个备份",
        data=result
    )
```

在 `main.py` 中注册路由：

```python
from .api import maintenance

app.include_router(maintenance.router)
```

### Step 19: 添加系统监控 API（20分钟）

创建 `data_platform/backend/api/monitoring.py`：

```python
from fastapi import APIRouter
import psutil
from datetime import datetime

from ..services.db_manager import db_manager
from ..models.schemas import SystemStats

router = APIRouter(prefix="/api/system", tags=["系统监控"])

@router.get("/stats", response_model=SystemStats)
async def get_system_stats():
    """获取系统统计信息"""
    
    # CPU 和内存
    cpu_percent = psutil.cpu_percent(interval=1)
    memory = psutil.virtual_memory()
    disk = psutil.disk_usage('/')
    
    # 知识库统计
    kb_list = db_manager.list_kb()
    total_kb = len(kb_list)
    
    total_docs = 0
    total_chunks = 0
    
    for kb in kb_list:
        stats = db_manager.get_kb_stats(kb['kb_id'])
        total_docs += stats['doc_count']
        total_chunks += stats['chunk_count']
    
    return SystemStats(
        cpu_percent=round(cpu_percent, 2),
        memory_percent=round(memory.percent, 2),
        disk_percent=round(disk.percent, 2),
        total_kb=total_kb,
        total_docs=total_docs,
        total_chunks=total_chunks,
        timestamp=datetime.now().isoformat()
    )

@router.get("/health")
async def health_check():
    """健康检查"""
    return {
        "status": "healthy",
        "timestamp": datetime.now().isoformat()
    }
```

在 `main.py` 中注册：

```python
from .api import monitoring

app.include_router(monitoring.router)
```

------

## 📦 第八阶段：Docker 部署

### Step 20: 创建 Dockerfile（15分钟）

在项目根目录创建 `Dockerfile`：

```dockerfile
FROM python:3.10-slim

# 设置工作目录
WORKDIR /app

# 安装系统依赖
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    libmagic1 \
    && rm -rf /var/lib/apt/lists/*

# 复制依赖文件
COPY requirements.txt .

# 安装 Python 依赖
RUN pip install --no-cache-dir -r requirements.txt

# 复制项目文件
COPY data_platform/ ./data_platform/
COPY .env .env

# 创建必要的目录
RUN mkdir -p knowledge_bases logs backups

# 暴露端口
EXPOSE 8000

# 启动命令
CMD ["python", "-m", "uvicorn", "data_platform.backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

### Step 21: 创建 docker-compose.yml（15分钟）

```yaml
version: '3.8'

services:
  knowledge-platform:
    build: .
    container_name: knowledge_platform_api
    ports:
      - "8000:8000"
    volumes:
      - ./knowledge_bases:/app/knowledge_bases
      - ./logs:/app/logs
      - ./backups:/app/backups
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - OPENAI_API_BASE=${OPENAI_API_BASE}
      - API_HOST=0.0.0.0
      - API_PORT=8000
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
```

### Step 22: 部署与测试（10分钟）

```bash
# 构建镜像
docker-compose build

# 启动服务
docker-compose up -d

# 查看日志
docker-compose logs -f

# 测试 API
curl http://localhost:8000/health

# 停止服务
docker-compose down
```

------

## 🎨 第九阶段（可选）：前端管理界面

### Step 23: 初始化前端项目（20分钟）

```bash
cd data_platform/frontend

# 使用 Vite 创建 React 项目
npm create vite@latest . -- --template react-ts

# 安装依赖
npm install

# 安装 UI 组件库和工具
npm install antd axios zustand @ant-design/icons @ant-design/pro-components
npm install recharts dayjs

# 启动开发服务器
npm run dev
```

### Step 24: 创建基础布局（30分钟）

创建 `src/App.tsx`：

```typescript
import { Layout, Menu } from 'antd';
import { DatabaseOutlined, FileTextOutlined, SearchOutlined, SettingOutlined } from '@ant-design/icons';
import { useState } from 'react';

const { Header, Sider, Content } = Layout;

function App() {
  const [collapsed, setCollapsed] = useState(false);
  const [selectedKey, setSelectedKey] = useState('kb');

  const menuItems = [
    {
      key: 'kb',
      icon: <DatabaseOutlined />,
      label: '知识库管理'
    },
    {
      key: 'docs',
      icon: <FileTextOutlined />,
      label: '文档管理'
    },
    {
      key: 'search',
      icon: <SearchOutlined />,
      label: '检索测试'
    },
    {
      key: 'system',
      icon: <SettingOutlined />,
      label: '系统监控'
    }
  ];

  return (
    <Layout style={{ minHeight: '100vh' }}>
      <Sider collapsible collapsed={collapsed} onCollapse={setCollapsed}>
        <div style={{
          height: 64,
          display: 'flex',
          alignItems: 'center',
          justifyContent: 'center',
          color: 'white',
          fontSize: 18,
          fontWeight: 'bold'
        }}>
          {collapsed ? '知识中台' : '企业知识中台'}
        </div>
        <Menu
          theme="dark"
          selectedKeys={[selectedKey]}
          mode="inline"
          items={menuItems}
          onClick={({ key }) => setSelectedKey(key)}
        />
      </Sider>
      <Layout>
        <Header style={{ background: '#fff', padding: '0 24px' }}>
          <h2>知识中台管理系统</h2>
        </Header>
        <Content style={{ margin: '24px 16px', padding: 24, background: '#fff' }}>
          {/* 这里根据 selectedKey 渲染不同的页面 */}
          <div>内容区域 - {selectedKey}</div>
        </Content>
      </Layout>
    </Layout>
  );
}

export default App;
```

由于前端开发内容较多，这里提供核心页面的框架代码。完整的前端实现可以在确认需要后继续补充。

------

## ✅ 完整实施时间表

| 阶段              | 任务                         | 预计时间 | 状态 |
| ----------------- | ---------------------------- | -------- | ---- |
| **第1周**         |                              |          |      |
| Day 1             | 环境搭建 + 项目初始化        | 2小时    | ⬜    |
| Day 2             | 数据模型 + 数据库管理        | 3小时    | ⬜    |
| Day 3-4           | 文档处理引擎开发             | 6小时    | ⬜    |
| Day 5-7           | API 开发（知识库/文档/检索） | 8小时    | ⬜    |
| **第2周**         |                              |          |      |
| Day 8-9           | Dify 对接 + 测试             | 4小时    | ⬜    |
| Day 10-11         | 高级功能（监听/备份）        | 5小时    | ⬜    |
| Day 12-14         | 系统监控 + Docker 部署       | 6小时    | ⬜    |
| **第3周（可选）** |                              |          |      |
| Day 15-17         | 前端界面开发                 | 12小时   | ⬜    |
| Day 18-19         | 联调测试 + Bug 修复          | 8小时    | ⬜    |
| Day 20-21         | 文档编写 + 部署上线          | 6小时    | ⬜    |

------

## 📝 实施检查清单

### 核心功能

- [ ] 知识库创建/删除/列表
- [ ] 文档上传/删除/列表
- [ ] 文档自动解析与分块
- [ ] 向量化与存储
- [ ] 语义检索接口
- [ ] Dify 工具对接

### 高级功能

- [ ] 文档自动监听
- [ ] 多知识库联合检索
- [ ] 数据库备份/恢复
- [ ] 系统监控 API
- [ ] 日志记录

### 部署相关

- [ ] Docker 镜像构建
- [ ] docker-compose 配置
- [ ] 环境变量管理
- [ ] 健康检查

### 文档与测试

- [ ] API 文档（Swagger）
- [ ] 单元测试
- [ ] 集成测试
- [ ] 部署文档
- [ ] 使用手册

------

## 🚨 常见问题与解决方案

### 问题1: OpenAI API 超时

**解决**：

```python
# 在 config/settings.py 中添加
OPENAI_TIMEOUT: int = 60
OPENAI_MAX_RETRIES: int = 3

# 在初始化 embeddings 时使用
embeddings = OpenAIEmbeddings(
    request_timeout=settings.OPENAI_TIMEOUT,
    max_retries=settings.OPENAI_MAX_RETRIES
)
```

### 问题2: 文档解析失败

**解决**：

```python
# 添加异常处理和日志
try:
    loader = UnstructuredFileLoader(file_path)
    documents = loader.load()
except Exception as e:
    logger.error(f"文档加载失败: {file_path}, 错误: {e}")
    # 尝试备用加载器
    try:
        with open(file_path, 'r', encoding='utf-8') as f:
            content = f.read()
            documents = [Document(page_content=content)]
    except:
        raise Exception("文档无法解析")
```

### 问题3: ChromaDB 数据库锁定

**解决**：

```python
# 确保每次只有一个进程访问数据库
# 使用文件锁或修改配置
client = chromadb.PersistentClient(
    path=db_path,
    settings=Settings(
        anonymized_telemetry=False,
        allow_reset=True,
        is_persistent=True
    )
)
```

### 问题4: 内存占用过高

**解决**：

```python
# 批量处理文档时分批提交
BATCH_SIZE = 10

for i in range(0, len(chunks), BATCH_SIZE):
    batch = chunks[i:i+BATCH_SIZE]
    # 处理批次
    collection.add(...)
    # 手动触发垃圾回收
    import gc
    gc.collect()
```

### 问题5: 检索结果不准确

**解决**：

1. 调整 chunk_size（试试 800-1200）
2. 增加 chunk_overlap（试试 150-250）
3. 使用更好的 embedding 模型（text-embedding-3-large）
4. 启用重排序（Rerank）
5. 优化文档结构（清晰的章节）

------

## 📚 下一步优化方向

### 性能优化

1. **缓存机制**
   - Redis 缓存热门查询结果
   - 本地缓存 embedding 结果
2. **异步处理**
   - Celery 任务队列处理大批量文档
   - 异步生成 embedding
3. **数据库升级**
   - 迁移到 Milvus（百万级数据）
   - 使用 Qdrant（高性能过滤）

### 功能增强

1. **智能问答**
   - 多轮对话上下文管理
   - 问题改写与扩展
   - 答案质量评估
2. **知识图谱**
   - 自动提取实体和关系
   - 图谱可视化
   - 基于图谱的推理
3. **版本管理**
   - 文档版本对比
   - 变更历史追踪
   - 版本回滚
4. **权限控制**
   - 用户认证（JWT）
   - 知识库访问控制
   - 操作审计日志

### 用户体验

1. **前端优化**
   - 实时状态更新（WebSocket）
   - 批量操作
   - 拖拽上传
   - 预览与高亮
2. **智能推荐**
   - 相关文档推荐
   - 问题补全
   - 热门查询统计

------

## 🎯 实施建议

### 第一周重点

✅ **先跑通核心流程**

- 创建知识库 → 上传文档 → 检索成功
- 不追求完美，先实现基本功能
- 每天测试，及时发现问题

### 第二周重点

✅ **对接 Dify 并优化**

- 确保 Dify 能正常调用
- 测试真实业务场景
- 根据反馈调整 chunk 策略

### 第三周（可选）

✅ **完善与部署**

- 添加必要的高级功能
- 前端界面（可选）
- 部署到生产环境

------

## 📞 技术支持

### 遇到问题时的调试步骤

1. **检查日志**

   ```bash
   tail -f logs/app.log
   ```

2. **验证配置**

   ```python
   python -c "from data_platform.backend.config.settings import settings; print(settings.dict())"
   ```

3. **测试数据库连接**

   ```python
   python -c "from data_platform.backend.services.db_manager import db_manager; print(db_manager.list_kb())"
   ```

4. **测试 OpenAI API**

   ```python
   python -c "from langchain.embeddings import OpenAIEmbeddings; emb = OpenAIEmbeddings(); print(emb.embed_query('test')[:5])"
   ```

5. **检查端口占用**

   ```bash
   lsof -i:8000  # 查看 8000 端口
   ```

### 推荐资源

- **LangChain 文档**: https://python.langchain.com/docs/
- **ChromaDB 文档**: https://docs.trychroma.com/
- **FastAPI 文档**: https://fastapi.tiangolo.com/
- **Dify 文档**: https://docs.dify.ai/

------

## ✨ 总结

通过以上步骤，你将拥有一个**完整的、可扩展的企业级知识中台**：

### 核心能力

✅ 多知识库隔离管理 ✅ 自动化文档处理 ✅ 高效语义检索 ✅ 持久化数据存储 ✅ Dify 无缝对接 ✅ 系统监控与备份

### 技术架构

```
前端管理界面 (React + Ant Design)
         ↓
数据中台 API (FastAPI)
         ↓
LangChain 处理层
         ↓
向量数据库 (ChromaDB/Milvus)
         ↓
Dify 智能问答
```

### 实施策略

1. **第一周**：核心功能开发（知识库、文档处理、检索）
2. **第二周**：Dify 对接 + 高级功能（监听、备份、监控）
3. **第三周**：前端界面 + 部署上线（可选）

### 关键建议

- 从简单到复杂，逐步迭代
- 每个阶段都要测试验证
- 根据实际需求调整功能优先级
- 做好文档和日志记录

------

## 🚀 开始实施

现在你可以开始实施了！建议按照以下顺序：

1. **立即开始**：Step 1-5（环境搭建）
2. **第一天完成**：Step 6-8（数据库 + 文档处理）
3. **第二天完成**：Step 9-14（API 开发 + 测试）
4. **第三天完成**：Step 15-16（Dify 对接）
5. **后续优化**：Step 17-24（高级功能 + 部署）

**记住**：不要追求一次性完美，先让系统跑起来，然后再逐步优化！

祝你实施顺利！🎉

------

## 📋 附录：完整代码仓库结构

```
knowledge_platform/
├── README.md                           # 项目说明
├── requirements.txt                    # Python 依赖
├── .env                                # 环境变量
├── .gitignore                          # Git 忽略文件
├── Dockerfile                          # Docker 镜像
├── docker-compose.yml                  # Docker Compose 配置
│
├── knowledge_bases/                    # 知识库数据（运行时生成）
│   ├── crm_system/
│   │   ├── config.json
│   │   ├── docs/
│   │   └── db/
│   └── erp_system/
│       ├── config.json
│       ├── docs/
│       └── db/
│
├── data_platform/
│   ├── backend/
│   │   ├── main.py                    # FastAPI 入口 ⭐
│   │   ├── api/
│   │   │   ├── __init__.py
│   │   │   ├── kb_management.py       # 知识库管理 API
│   │   │   ├── document_management.py # 文档管理 API
│   │   │   ├── retrieve.py            # 检索 API ⭐
│   │   │   ├── maintenance.py         # 维护 API
│   │   │   └── monitoring.py          # 监控 API
│   │   ├── services/
│   │   │   ├── __init__.py
│   │   │   ├── db_manager.py          # 数据库管理 ⭐
│   │   │   ├── document_processor.py  # 文档处理 ⭐
│   │   │   ├── retriever.py           # 检索服务 ⭐
│   │   │   ├── watcher.py             # 文件监听
│   │   │   └── backup.py              # 备份服务
│   │   ├── models/
│   │   │   ├── __init__.py
│   │   │   └── schemas.py             # 数据模型 ⭐
│   │   ├── config/
│   │   │   ├── __init__.py
│   │   │   └── settings.py            # 配置管理 ⭐
│   │   └── utils/
│   │       ├── __init__.py
│   │       └── helpers.py
│   │
│   └── frontend/                       # 前端（可选）
│       ├── package.json
│       ├── src/
│       │   ├── App.tsx
│       │   ├── pages/
│       │   ├── components/
│       │   └── services/
│       └── public/
│
├── tests/                              # 测试代码
│   ├── test_db_manager.py
│   ├── test_document_processor.py
│   └── test_api.py
│
├── logs/                               # 日志（运行时生成）
│   └── app.log
│
├── backups/                            # 备份（运行时生成）
│   └── crm_system_20251030_120000.tar.gz
│
└── scripts/                            # 脚本工具
    ├── init_kb.py                     # 初始化知识库
    ├── migrate_data.py                # 数据迁移
    └── cleanup.py                     # 清理脚本
```

**标注 ⭐ 的是最核心的文件**，这些是你实施时的重点！