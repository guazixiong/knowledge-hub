# ğŸš€ ä¼ä¸šçº§çŸ¥è¯†ä¸­å°å®æ–½æ–¹æ¡ˆä¸éªŒæ”¶æ ‡å‡†

åŸºäº LangChain + Dify æ¶æ„çš„ä¼ä¸šçº§çŸ¥è¯†ä¸­å°å®Œæ•´å®æ–½æ–¹æ¡ˆ

## ğŸ“‹ é¡¹ç›®æ¦‚è¿°

### é¡¹ç›®ç›®æ ‡
æ„å»ºä¸€ä¸ªåŸºäº LangChain å’Œ Dify çš„ä¼ä¸šçº§çŸ¥è¯†ä¸­å°è§£å†³æ–¹æ¡ˆï¼Œæ”¯æŒå¤šçŸ¥è¯†åº“ç®¡ç†ã€æ™ºèƒ½æ–‡æ¡£å¤„ç†å’Œè¯­ä¹‰æ£€ç´¢ã€‚

### æŠ€æœ¯æ¶æ„
- **åç«¯**: FastAPI + LangChain + ChromaDB
- **å‰ç«¯**: React + TypeScript + Ant Design
- **AIé›†æˆ**: OpenAI Embeddings + Difyå·¥ä½œæµ
- **éƒ¨ç½²**: Docker + Nginx

### é¢„è®¡å·¥æœŸ
**æ€»å·¥æœŸ**: 8-12å‘¨ï¼ˆæ ¹æ®å›¢é˜Ÿè§„æ¨¡è°ƒæ•´ï¼‰

---

## ğŸ¯ ç¬¬ä¸€é˜¶æ®µï¼šç¯å¢ƒå‡†å¤‡ä¸åŸºç¡€æ¶æ„æ­å»º

### ğŸ“… æ—¶é—´å®‰æ’
**é¢„è®¡å·¥æœŸ**: 1-2å‘¨

### ğŸ¯ ä¸»è¦ä»»åŠ¡

#### 1.1 é¡¹ç›®ç»“æ„åˆ›å»º

```bash
```

#### 1.2 å¼€å‘ç¯å¢ƒé…ç½®
- **Python ç¯å¢ƒ**: 3.10+
- **Node.js ç¯å¢ƒ**: 16+
- **IDE é…ç½®**: VSCode + Python/TypeScript æ’ä»¶
- **Git é…ç½®**: åˆ†æ”¯ç­–ç•¥ã€æäº¤è§„èŒƒ

#### 1.3 ä¾èµ–åŒ…å®‰è£…
```bash
# åç«¯ä¾èµ–
pip install fastapi==0.104.1 uvicorn==0.24.0 
pip install langchain==0.1.0 langchain-community==0.0.10
pip install openai==1.6.1 chromadb==0.4.18
pip install unstructured==0.11.8 python-multipart==0.0.6
pip install aiofiles==23.2.1 pydantic-settings==2.1.0
pip install watchdog==3.0.0 pytest==7.4.3
pip install minio==7.2.0 boto3==1.34.0  # MinIO å¯¹è±¡å­˜å‚¨

# å‰ç«¯ä¾èµ–
cd frontend
npm install react@18.2.0 @types/react@18.2.45
npm install antd@5.12.8 @ant-design/icons@5.2.6
npm install axios@1.6.2 zustand@4.4.7
npm install @types/node@20.10.4 typescript@5.3.3
```

#### 1.4 MinIO å¯¹è±¡å­˜å‚¨éƒ¨ç½²

**ç›®æ ‡**: éƒ¨ç½²æœ¬åœ° MinIO æœåŠ¡å™¨ç”¨äºæ–‡æ¡£æ–‡ä»¶å­˜å‚¨

##### 1.4.1 Docker æ–¹å¼éƒ¨ç½² MinIO (æ¨è)
```bash
# 1. åˆ›å»º MinIO æ•°æ®ç›®å½•
mkdir -p ./data/minio/{data,config}

# 2. ä½¿ç”¨ Docker å¯åŠ¨ MinIO
docker run -d \
  --name minio-server \
  -p 9000:9000 \
  -p 9001:9001 \
  -e "MINIO_ROOT_USER=minioadmin" \
  -e "MINIO_ROOT_PASSWORD=minioadmin123" \
  -v ./data/minio/data:/data \
  -v ./data/minio/config:/root/.minio \
  minio/minio server /data --console-address ":9001"

# 3. éªŒè¯ MinIO æœåŠ¡
curl http://localhost:9000/minio/health/live
```

##### 1.4.2 Docker Compose æ–¹å¼éƒ¨ç½²
åˆ›å»º `docker-compose.minio.yml`:
```yaml
version: '3.8'

services:
  minio:
    image: minio/minio:latest
    container_name: knowledge-hub-minio
    ports:
      - "9000:9000"    # API ç«¯å£
      - "9001:9001"    # Web æ§åˆ¶å°ç«¯å£
    environment:
      MINIO_ROOT_USER: minioadmin
      MINIO_ROOT_PASSWORD: minioadmin123
      MINIO_BROWSER_REDIRECT_URL: http://localhost:9001
    volumes:
      - ./data/minio/data:/data
      - ./data/minio/config:/root/.minio
    command: server /data --console-address ":9001"
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9000/minio/health/live"]
      interval: 30s
      timeout: 20s
      retries: 3
    networks:
      - knowledge-hub-network

networks:
  knowledge-hub-network:
    driver: bridge
```

å¯åŠ¨å‘½ä»¤:
```bash
docker-compose -f docker-compose.minio.yml up -d
```

##### 1.4.3 MinIO å®¢æˆ·ç«¯é…ç½®
```bash
# å®‰è£… MinIO å®¢æˆ·ç«¯ (å¯é€‰)
# Windows
curl -O https://dl.min.io/client/mc/release/windows-amd64/mc.exe

# Linux/Mac
curl -O https://dl.min.io/client/mc/release/linux-amd64/mc
chmod +x mc

# é…ç½® MinIO å®¢æˆ·ç«¯
./mc alias set local http://localhost:9000 minioadmin minioadmin123

# åˆ›å»ºå­˜å‚¨æ¡¶
./mc mb local/knowledge-hub-docs
./mc mb local/knowledge-hub-uploads
```

##### 1.4.4 MinIO Python å®¢æˆ·ç«¯é…ç½®
```python
# backend/core/minio_client.py
from minio import Minio
from minio.error import S3Error
import os
from typing import Optional
import logging

class MinIOClient:
    def __init__(self):
        self.client = Minio(
            endpoint=os.getenv("MINIO_ENDPOINT", "localhost:9000"),
            access_key=os.getenv("MINIO_ACCESS_KEY", "minioadmin"),
            secret_key=os.getenv("MINIO_SECRET_KEY", "minioadmin123"),
            secure=False  # æœ¬åœ°å¼€å‘ç¯å¢ƒä½¿ç”¨ HTTP
        )
        self.bucket_name = os.getenv("MINIO_BUCKET", "knowledge-hub-docs")
        self.logger = logging.getLogger(__name__)
        
        # ç¡®ä¿å­˜å‚¨æ¡¶å­˜åœ¨
        self._ensure_bucket_exists()
    
    def _ensure_bucket_exists(self):
        """ç¡®ä¿å­˜å‚¨æ¡¶å­˜åœ¨"""
        try:
            if not self.client.bucket_exists(self.bucket_name):
                self.client.make_bucket(self.bucket_name)
                self.logger.info(f"Created bucket: {self.bucket_name}")
        except S3Error as e:
            self.logger.error(f"Error creating bucket: {e}")
    
    def upload_file(self, file_path: str, object_name: str) -> bool:
        """ä¸Šä¼ æ–‡ä»¶åˆ° MinIO"""
        try:
            self.client.fput_object(
                bucket_name=self.bucket_name,
                object_name=object_name,
                file_path=file_path
            )
            self.logger.info(f"File uploaded: {object_name}")
            return True
        except S3Error as e:
            self.logger.error(f"Error uploading file: {e}")
            return False
    
    def download_file(self, object_name: str, file_path: str) -> bool:
        """ä» MinIO ä¸‹è½½æ–‡ä»¶"""
        try:
            self.client.fget_object(
                bucket_name=self.bucket_name,
                object_name=object_name,
                file_path=file_path
            )
            return True
        except S3Error as e:
            self.logger.error(f"Error downloading file: {e}")
            return False
    
    def delete_file(self, object_name: str) -> bool:
        """åˆ é™¤ MinIO ä¸­çš„æ–‡ä»¶"""
        try:
            self.client.remove_object(self.bucket_name, object_name)
            return True
        except S3Error as e:
            self.logger.error(f"Error deleting file: {e}")
            return False
    
    def get_file_url(self, object_name: str, expires: int = 3600) -> Optional[str]:
        """è·å–æ–‡ä»¶çš„é¢„ç­¾å URL"""
        try:
            url = self.client.presigned_get_object(
                bucket_name=self.bucket_name,
                object_name=object_name,
                expires=expires
            )
            return url
        except S3Error as e:
            self.logger.error(f"Error generating URL: {e}")
            return None
```

#### 1.5 åŸºç¡€é…ç½®æ–‡ä»¶
- **ç¯å¢ƒå˜é‡é…ç½®** (.env)
- **æ•°æ®åº“é…ç½®** (database.py)
- **æ—¥å¿—é…ç½®** (logging.conf)
- **API æ–‡æ¡£é…ç½®** (FastAPI Swagger)

### âœ… éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|--------|----------|----------|
| é¡¹ç›®ç»“æ„ | æ‰€æœ‰ç›®å½•å’Œæ–‡ä»¶æŒ‰è§„èŒƒåˆ›å»º | æ£€æŸ¥ç›®å½•ç»“æ„å®Œæ•´æ€§ |
| ç¯å¢ƒé…ç½® | Python 3.10+, Node.js 16+ æ­£å¸¸è¿è¡Œ | `python --version`, `node --version` |
| ä¾èµ–å®‰è£… | æ‰€æœ‰ä¾èµ–åŒ…æˆåŠŸå®‰è£…ï¼Œæ— ç‰ˆæœ¬å†²çª | `pip list`, `npm list` |
| åŸºç¡€æœåŠ¡ | FastAPI æœåŠ¡èƒ½æ­£å¸¸å¯åŠ¨ | è®¿é—® http://localhost:8000/docs |
| å‰ç«¯æœåŠ¡ | React å¼€å‘æœåŠ¡å™¨æ­£å¸¸å¯åŠ¨ | è®¿é—® http://localhost:3000 |
| é…ç½®æ–‡ä»¶ | ç¯å¢ƒå˜é‡ã€æ•°æ®åº“é…ç½®æ­£ç¡® | é…ç½®æ–‡ä»¶è¯­æ³•æ£€æŸ¥ |

---

## ğŸ¯ ç¬¬äºŒé˜¶æ®µï¼šæ•°æ®ä¸­å°åç«¯å¼€å‘

### ğŸ“… æ—¶é—´å®‰æ’
**é¢„è®¡å·¥æœŸ**: 2-3å‘¨

### ğŸ¯ ä¸»è¦ä»»åŠ¡

#### å®Œæ•´ API åˆ—è¡¨

```python
# ==================== çŸ¥è¯†åº“ç®¡ç† ====================
POST   /api/kb/create                    # åˆ›å»ºçŸ¥è¯†åº“
GET    /api/kb/list                      # è·å–çŸ¥è¯†åº“åˆ—è¡¨
GET    /api/kb/{kb_id}/info              # è·å–çŸ¥è¯†åº“è¯¦æƒ…
PUT    /api/kb/{kb_id}/update            # æ›´æ–°çŸ¥è¯†åº“é…ç½®
DELETE /api/kb/{kb_id}                   # åˆ é™¤çŸ¥è¯†åº“

# ==================== æ–‡æ¡£ç®¡ç† ====================
POST   /api/kb/{kb_id}/documents/upload  # ä¸Šä¼ æ–‡æ¡£ï¼ˆæ”¯æŒæ‰¹é‡ï¼‰
GET    /api/kb/{kb_id}/documents         # è·å–æ–‡æ¡£åˆ—è¡¨
GET    /api/kb/{kb_id}/documents/{doc_id} # è·å–æ–‡æ¡£è¯¦æƒ…
DELETE /api/kb/{kb_id}/documents/{doc_id} # åˆ é™¤æ–‡æ¡£
PUT    /api/kb/{kb_id}/documents/{doc_id}/reprocess # é‡æ–°å¤„ç†æ–‡æ¡£

# ==================== æ£€ç´¢æ¥å£ï¼ˆä¾› Dify è°ƒç”¨ï¼‰ ====================
POST   /api/retrieve                     # è¯­ä¹‰æ£€ç´¢
POST   /api/retrieve/multi               # å¤šçŸ¥è¯†åº“è”åˆæ£€ç´¢

# ==================== ç›‘æ§ä¸ç»´æŠ¤ ====================
GET    /api/system/stats                 # ç³»ç»Ÿç»Ÿè®¡ä¿¡æ¯
GET    /api/kb/{kb_id}/stats             # çŸ¥è¯†åº“ç»Ÿè®¡
POST   /api/maintenance/backup           # è§¦å‘å¤‡ä»½
POST   /api/maintenance/cleanup          # æ¸…ç†æ—§ç‰ˆæœ¬
```

### âœ… éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|--------|----------|----------|
| API æ¡†æ¶ | FastAPI æœåŠ¡æ­£å¸¸å¯åŠ¨ï¼ŒSwagger æ–‡æ¡£å¯è®¿é—® | è®¿é—® /docs é¡µé¢ |
| çŸ¥è¯†åº“ API | åˆ›å»ºã€æŸ¥è¯¢ã€åˆ é™¤çŸ¥è¯†åº“æ¥å£æ­£å¸¸å·¥ä½œ | Postman/curl æµ‹è¯• |
| æ–‡æ¡£ API | æ–‡æ¡£ä¸Šä¼ ã€åˆ—è¡¨ã€åˆ é™¤æ¥å£æ­£å¸¸å·¥ä½œ | æ–‡ä»¶ä¸Šä¼ æµ‹è¯• |
| æ£€ç´¢ API | æ£€ç´¢æ¥å£è¿”å›æ­£ç¡®æ ¼å¼çš„æ•°æ® | API å“åº”æ ¼å¼éªŒè¯ |
| é”™è¯¯å¤„ç† | å¼‚å¸¸æƒ…å†µè¿”å›åˆé€‚çš„ HTTP çŠ¶æ€ç  | é”™è¯¯åœºæ™¯æµ‹è¯• |
| API æ–‡æ¡£ | æ‰€æœ‰æ¥å£éƒ½æœ‰å®Œæ•´çš„æ–‡æ¡£è¯´æ˜ | Swagger æ–‡æ¡£å®Œæ•´æ€§ |

---

## ğŸ¯ ç¬¬ä¸‰é˜¶æ®µï¼šLangChainå¤„ç†å±‚å¼€å‘

### ğŸ“… æ—¶é—´å®‰æ’
**é¢„è®¡å·¥æœŸ**: 2-3å‘¨

### ğŸ¯ ä¸»è¦ä»»åŠ¡

#### 3.1 æ–‡æ¡£è§£æå™¨å®ç°
```python
# core/langchain_processor.py
from langchain.document_loaders import (
    UnstructuredFileLoader,
    UnstructuredWordDocumentLoader,
    UnstructuredPDFLoader,
    UnstructuredMarkdownLoader
)
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.embeddings import OpenAIEmbeddings
from typing import List, Dict
import os

class DocumentProcessor:
    def __init__(self, config: Dict):
        self.config = config
        self.embeddings = OpenAIEmbeddings(
            model=config.get('embedding_model', 'text-embedding-3-small')
        )
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=config.get('chunk_size', 1000),
            chunk_overlap=config.get('chunk_overlap', 200),
            separators=["\n\n", "\n", "ã€‚", "!", "?", "ï¼›", "â€¦â€¦", "â€¦", " "]
        )
    
    def load_document(self, file_path: str):
        """æ ¹æ®æ–‡ä»¶ç±»å‹é€‰æ‹©åˆé€‚çš„åŠ è½½å™¨"""
        file_ext = os.path.splitext(file_path)[1].lower()
        
        loaders = {
            '.txt': UnstructuredFileLoader,
            '.md': UnstructuredMarkdownLoader,
            '.docx': UnstructuredWordDocumentLoader,
            '.pdf': UnstructuredPDFLoader,
            '.html': UnstructuredFileLoader,
        }
        
        loader_class = loaders.get(file_ext, UnstructuredFileLoader)
        loader = loader_class(file_path)
        return loader.load()
    
    def split_documents(self, documents):
        """æ™ºèƒ½åˆ†å—å¤„ç†"""
        return self.text_splitter.split_documents(documents)
    
    def extract_embeddings(self, texts: List[str]):
        """æ‰¹é‡å‘é‡åŒ–"""
        return self.embeddings.embed_documents(texts)
```

#### 3.2 å…ƒæ•°æ®æå–å™¨
```python
# utils/metadata_extractor.py
import re
from pathlib import Path
from typing import Dict
from datetime import datetime

class MetadataExtractor:
    def __init__(self):
        self.system_patterns = {
            'CRM': r'(?i)(crm|å®¢æˆ·å…³ç³»|å®¢æˆ·ç®¡ç†)',
            'ERP': r'(?i)(erp|ä¼ä¸šèµ„æº|èµ„æºè§„åˆ’)',
            'OA': r'(?i)(oa|åŠå…¬è‡ªåŠ¨åŒ–|ååŒåŠå…¬)',
            'HRM': r'(?i)(hrm|äººåŠ›èµ„æº|äººäº‹ç®¡ç†)'
        }
        
        self.doc_type_patterns = {
            'éœ€æ±‚æ–‡æ¡£': r'(?i)(éœ€æ±‚|requirement|éœ€æ±‚åˆ†æ|ä¸šåŠ¡éœ€æ±‚)',
            'è®¾è®¡æ–‡æ¡£': r'(?i)(è®¾è®¡|design|æ¶æ„è®¾è®¡|è¯¦ç»†è®¾è®¡)',
            'è¿ç»´æ–‡æ¡£': r'(?i)(è¿ç»´|operation|éƒ¨ç½²|ç»´æŠ¤)',
            'ç”¨æˆ·æ‰‹å†Œ': r'(?i)(ç”¨æˆ·æ‰‹å†Œ|ä½¿ç”¨è¯´æ˜|æ“ä½œæŒ‡å—)',
            'æŠ€æœ¯æ–‡æ¡£': r'(?i)(æŠ€æœ¯|technical|å¼€å‘|api)'
        }
    
    def extract_from_filename(self, file_path: str) -> Dict:
        """ä»æ–‡ä»¶åæå–å…ƒæ•°æ®"""
        filename = Path(file_path).stem
        metadata = {
            'filename': filename,
            'file_ext': Path(file_path).suffix,
            'system': 'unknown',
            'doc_type': 'unknown',
            'version': '1.0'
        }
        
        # æå–ç³»ç»Ÿåç§°
        for system, pattern in self.system_patterns.items():
            if re.search(pattern, filename):
                metadata['system'] = system
                break
        
        # æå–æ–‡æ¡£ç±»å‹
        for doc_type, pattern in self.doc_type_patterns.items():
            if re.search(pattern, filename):
                metadata['doc_type'] = doc_type
                break
        
        # æå–ç‰ˆæœ¬å·
        version_match = re.search(r'v?(\d+\.\d+)', filename)
        if version_match:
            metadata['version'] = version_match.group(1)
        
        return metadata
    
    def extract_from_content(self, content: str) -> Dict:
        """ä»æ–‡æ¡£å†…å®¹æå–å…ƒæ•°æ®"""
        metadata = {}
        
        # æå–æ ‡é¢˜
        title_match = re.search(r'^#\s+(.+)$', content, re.MULTILINE)
        if title_match:
            metadata['title'] = title_match.group(1).strip()
        
        # æå–ä½œè€…
        author_match = re.search(r'ä½œè€…[:ï¼š]\s*(.+)', content)
        if author_match:
            metadata['author'] = author_match.group(1).strip()
        
        # æå–æ—¥æœŸ
        date_match = re.search(r'(\d{4}[-/]\d{1,2}[-/]\d{1,2})', content)
        if date_match:
            metadata['date'] = date_match.group(1)
        
        return metadata
```

#### 3.3 æ–‡æ¡£å¤„ç†ä»»åŠ¡é˜Ÿåˆ—
```python
# core/task_processor.py
import asyncio
from typing import Dict, List
import json
import logging

class DocumentTaskProcessor:
    def __init__(self):
        self.task_queue = asyncio.Queue()
        self.processing_tasks = {}
        self.logger = logging.getLogger(__name__)
    
    async def add_task(self, task_data: Dict):
        """æ·»åŠ æ–‡æ¡£å¤„ç†ä»»åŠ¡"""
        await self.task_queue.put(task_data)
        self.logger.info(f"Task added: {task_data['doc_id']}")
    
    async def process_document_task(self, task_data: Dict):
        """å¤„ç†å•ä¸ªæ–‡æ¡£ä»»åŠ¡"""
        doc_id = task_data['doc_id']
        kb_id = task_data['kb_id']
        file_path = task_data['file_path']
        
        try:
            # æ›´æ–°ä»»åŠ¡çŠ¶æ€
            self.processing_tasks[doc_id] = {
                'status': 'processing',
                'progress': 0,
                'message': 'å¼€å§‹å¤„ç†æ–‡æ¡£'
            }
            
            # 1. åŠ è½½æ–‡æ¡£
            processor = DocumentProcessor(task_data['config'])
            documents = processor.load_document(file_path)
            
            self.processing_tasks[doc_id]['progress'] = 25
            self.processing_tasks[doc_id]['message'] = 'æ–‡æ¡£åŠ è½½å®Œæˆ'
            
            # 2. æå–å…ƒæ•°æ®
            extractor = MetadataExtractor()
            base_metadata = extractor.extract_from_filename(file_path)
            content_metadata = extractor.extract_from_content(documents[0].page_content)
            metadata = {**base_metadata, **content_metadata, 'doc_id': doc_id}
            
            self.processing_tasks[doc_id]['progress'] = 50
            self.processing_tasks[doc_id]['message'] = 'å…ƒæ•°æ®æå–å®Œæˆ'
            
            # 3. æ–‡æ¡£åˆ†å—
            chunks = processor.split_documents(documents)
            
            self.processing_tasks[doc_id]['progress'] = 75
            self.processing_tasks[doc_id]['message'] = f'æ–‡æ¡£åˆ†å—å®Œæˆï¼Œå…±{len(chunks)}ä¸ªå—'
            
            # 4. å‘é‡åŒ–å¹¶å­˜å‚¨
            await self.store_chunks(kb_id, doc_id, chunks, metadata, processor)
            
            # 5. å®Œæˆä»»åŠ¡
            self.processing_tasks[doc_id] = {
                'status': 'completed',
                'progress': 100,
                'message': f'å¤„ç†å®Œæˆï¼Œå…±ç”Ÿæˆ{len(chunks)}ä¸ªæ–‡æ¡£å—',
                'chunk_count': len(chunks)
            }
            
            self.logger.info(f"Document {doc_id} processed successfully")
            
        except Exception as e:
            self.processing_tasks[doc_id] = {
                'status': 'failed',
                'progress': 0,
                'message': f'å¤„ç†å¤±è´¥: {str(e)}'
            }
            self.logger.error(f"Error processing document {doc_id}: {e}")
    
    async def store_chunks(self, kb_id: str, doc_id: str, chunks, metadata: Dict, processor):
        """å­˜å‚¨æ–‡æ¡£å—åˆ°å‘é‡æ•°æ®åº“"""
        # å®ç°å‘é‡å­˜å‚¨é€»è¾‘
        pass
    
    def get_task_status(self, doc_id: str) -> Dict:
        """è·å–ä»»åŠ¡çŠ¶æ€"""
        return self.processing_tasks.get(doc_id, {'status': 'not_found'})
```

#### 3.3 MinIO æ–‡ä»¶å­˜å‚¨ API æ¥å£
```python
# api/files.py - MinIO æ–‡ä»¶å­˜å‚¨æ¥å£
from fastapi import APIRouter, UploadFile, File, HTTPException, Query
from fastapi.responses import StreamingResponse
from core.minio_client import MinIOClient
from typing import Optional, List
import uuid
import os
from datetime import datetime

router = APIRouter(prefix="/api/files", tags=["files"])
minio_client = MinIOClient()

@router.post("/upload")
async def upload_file(
    file: UploadFile = File(...),
    kb_id: Optional[str] = None,
    folder: Optional[str] = None
):
    """
    ä¸Šä¼ æ–‡ä»¶åˆ° MinIO å­˜å‚¨
    
    Args:
        file: ä¸Šä¼ çš„æ–‡ä»¶
        kb_id: çŸ¥è¯†åº“IDï¼ˆå¯é€‰ï¼‰
        folder: æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
    
    Returns:
        æ–‡ä»¶ä¿¡æ¯å’Œè®¿é—®URL
    """
    try:
        # ç”Ÿæˆå”¯ä¸€æ–‡ä»¶ID
        file_id = str(uuid.uuid4())
        file_ext = os.path.splitext(file.filename)[1]
        
        # æ„å»ºå¯¹è±¡åç§°
        if folder:
            object_name = f"{folder}/{file_id}{file_ext}"
        elif kb_id:
            object_name = f"kb_{kb_id}/{file_id}{file_ext}"
        else:
            object_name = f"uploads/{file_id}{file_ext}"
        
        # ä¿å­˜ä¸´æ—¶æ–‡ä»¶
        temp_file_path = f"/tmp/{file_id}{file_ext}"
        with open(temp_file_path, "wb") as temp_file:
            content = await file.read()
            temp_file.write(content)
        
        # ä¸Šä¼ åˆ° MinIO
        success = minio_client.upload_file(temp_file_path, object_name)
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        os.remove(temp_file_path)
        
        if not success:
            raise HTTPException(status_code=500, detail="æ–‡ä»¶ä¸Šä¼ å¤±è´¥")
        
        # ç”Ÿæˆé¢„ç­¾åURL
        file_url = minio_client.get_file_url(object_name, expires=3600)
        
        return {
            "success": True,
            "file_id": file_id,
            "filename": file.filename,
            "object_name": object_name,
            "file_url": file_url,
            "size": len(content),
            "upload_time": datetime.now().isoformat(),
            "kb_id": kb_id
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¸Šä¼ å¤±è´¥: {str(e)}")

@router.get("/{file_id}/download")
async def download_file(file_id: str, object_name: str = Query(...)):
    """
    ä¸‹è½½æ–‡ä»¶
    
    Args:
        file_id: æ–‡ä»¶ID
        object_name: MinIOä¸­çš„å¯¹è±¡åç§°
    
    Returns:
        æ–‡ä»¶æµ
    """
    try:
        # ä¸‹è½½åˆ°ä¸´æ—¶æ–‡ä»¶
        temp_file_path = f"/tmp/download_{file_id}"
        success = minio_client.download_file(object_name, temp_file_path)
        
        if not success:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        # è¿”å›æ–‡ä»¶æµ
        def file_generator():
            with open(temp_file_path, "rb") as file:
                while True:
                    chunk = file.read(8192)
                    if not chunk:
                        break
                    yield chunk
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.remove(temp_file_path)
        
        return StreamingResponse(
            file_generator(),
            media_type="application/octet-stream",
            headers={"Content-Disposition": f"attachment; filename={file_id}"}
        )
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"ä¸‹è½½å¤±è´¥: {str(e)}")

@router.get("/{file_id}/url")
async def get_file_url(
    file_id: str,
    object_name: str = Query(...),
    expires: int = Query(3600, description="URLè¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰")
):
    """
    è·å–æ–‡ä»¶çš„é¢„ç­¾åè®¿é—®URL
    
    Args:
        file_id: æ–‡ä»¶ID
        object_name: MinIOä¸­çš„å¯¹è±¡åç§°
        expires: URLè¿‡æœŸæ—¶é—´ï¼ˆç§’ï¼‰
    
    Returns:
        é¢„ç­¾åURL
    """
    try:
        file_url = minio_client.get_file_url(object_name, expires)
        
        if not file_url:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨")
        
        return {
            "success": True,
            "file_id": file_id,
            "file_url": file_url,
            "expires_in": expires
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–URLå¤±è´¥: {str(e)}")

@router.delete("/{file_id}")
async def delete_file(file_id: str, object_name: str = Query(...)):
    """
    åˆ é™¤æ–‡ä»¶
    
    Args:
        file_id: æ–‡ä»¶ID
        object_name: MinIOä¸­çš„å¯¹è±¡åç§°
    
    Returns:
        åˆ é™¤ç»“æœ
    """
    try:
        success = minio_client.delete_file(object_name)
        
        if not success:
            raise HTTPException(status_code=404, detail="æ–‡ä»¶ä¸å­˜åœ¨æˆ–åˆ é™¤å¤±è´¥")
        
        return {
            "success": True,
            "message": f"æ–‡ä»¶ {file_id} åˆ é™¤æˆåŠŸ"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"åˆ é™¤å¤±è´¥: {str(e)}")

@router.get("/list")
async def list_files(
    kb_id: Optional[str] = Query(None, description="çŸ¥è¯†åº“ID"),
    folder: Optional[str] = Query(None, description="æ–‡ä»¶å¤¹è·¯å¾„"),
    limit: int = Query(50, description="è¿”å›æ•°é‡é™åˆ¶")
):
    """
    è·å–æ–‡ä»¶åˆ—è¡¨
    
    Args:
        kb_id: çŸ¥è¯†åº“IDï¼ˆå¯é€‰ï¼‰
        folder: æ–‡ä»¶å¤¹è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        limit: è¿”å›æ•°é‡é™åˆ¶
    
    Returns:
        æ–‡ä»¶åˆ—è¡¨
    """
    try:
        # æ„å»ºå‰ç¼€
        if folder:
            prefix = f"{folder}/"
        elif kb_id:
            prefix = f"kb_{kb_id}/"
        else:
            prefix = ""
        
        # è·å–æ–‡ä»¶åˆ—è¡¨ï¼ˆè¿™é‡Œéœ€è¦æ‰©å±• MinIOClient ç±»ï¼‰
        # ç”±äº minio-py çš„é™åˆ¶ï¼Œè¿™é‡Œæä¾›ä¸€ä¸ªç®€åŒ–çš„å®ç°
        return {
            "success": True,
            "files": [],
            "message": "æ–‡ä»¶åˆ—è¡¨åŠŸèƒ½éœ€è¦è¿›ä¸€æ­¥å®ç°"
        }
        
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"è·å–æ–‡ä»¶åˆ—è¡¨å¤±è´¥: {str(e)}")
```

### âœ… éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|--------|----------|----------|
| æ–‡æ¡£è§£æ | æ”¯æŒ .txt, .md, .docx, .pdf, .html æ ¼å¼ | å„æ ¼å¼æ–‡æ¡£è§£ææµ‹è¯• |
| æ™ºèƒ½åˆ†å— | æŒ‰è¯­ä¹‰è¾¹ç•Œåˆ†å—ï¼Œå—å¤§å°æ§åˆ¶åœ¨è®¾å®šèŒƒå›´ | åˆ†å—ç»“æœæ£€æŸ¥ |
| å…ƒæ•°æ®æå– | è‡ªåŠ¨æå–æ–‡ä»¶åã€å†…å®¹ä¸­çš„ç³»ç»Ÿã€ç±»å‹ã€ç‰ˆæœ¬ä¿¡æ¯ | å…ƒæ•°æ®å‡†ç¡®æ€§éªŒè¯ |
| å‘é‡åŒ– | æ–‡æœ¬æˆåŠŸè½¬æ¢ä¸ºå‘é‡ï¼Œç»´åº¦æ­£ç¡® | å‘é‡ç»´åº¦å’Œæ ¼å¼æ£€æŸ¥ |
| å¼‚æ­¥å¤„ç† | æ–‡æ¡£å¤„ç†ä»»åŠ¡å¼‚æ­¥æ‰§è¡Œï¼ŒçŠ¶æ€å¯æŸ¥è¯¢ | ä»»åŠ¡çŠ¶æ€æŸ¥è¯¢æµ‹è¯• |
| é”™è¯¯å¤„ç† | å¤„ç†å¤±è´¥æ—¶æœ‰è¯¦ç»†é”™è¯¯ä¿¡æ¯ | å¼‚å¸¸æ–‡æ¡£å¤„ç†æµ‹è¯• |
| MinIO æ–‡ä»¶ä¸Šä¼  | æ–‡ä»¶æˆåŠŸä¸Šä¼ åˆ° MinIOï¼Œè¿”å›è®¿é—®URL | æ–‡ä»¶ä¸Šä¼ æµ‹è¯• |
| MinIO æ–‡ä»¶ä¸‹è½½ | èƒ½å¤Ÿé€šè¿‡APIä¸‹è½½å·²ä¸Šä¼ çš„æ–‡ä»¶ | æ–‡ä»¶ä¸‹è½½æµ‹è¯• |
| MinIO æ–‡ä»¶åˆ é™¤ | èƒ½å¤Ÿåˆ é™¤ MinIO ä¸­çš„æ–‡ä»¶ | æ–‡ä»¶åˆ é™¤æµ‹è¯• |
| é¢„ç­¾åURL | èƒ½å¤Ÿç”Ÿæˆæœ‰æ•ˆçš„æ–‡ä»¶è®¿é—®URL | URLè®¿é—®æµ‹è¯• |

---

## ğŸ¯ ç¬¬å››é˜¶æ®µï¼šå‘é‡æ•°æ®åº“é›†æˆ

### ğŸ“… æ—¶é—´å®‰æ’
**é¢„è®¡å·¥æœŸ**: 1-2å‘¨

### ğŸ¯ ä¸»è¦ä»»åŠ¡

#### 4.1 ChromaDB æŒä¹…åŒ–é…ç½®
```python
# core/database.py
import chromadb
from chromadb.config import Settings
from typing import Dict, List, Optional
import os
import json

class VectorDatabase:
    def __init__(self, kb_id: str, config: Dict):
        self.kb_id = kb_id
        self.config = config
        self.db_path = f"./knowledge_bases/{kb_id}/db"
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        os.makedirs(self.db_path, exist_ok=True)
        
        # åˆ›å»ºæŒä¹…åŒ–å®¢æˆ·ç«¯
        self.client = chromadb.PersistentClient(
            path=self.db_path,
            settings=Settings(
                anonymized_telemetry=False,
                allow_reset=True
            )
        )
        
        # è·å–æˆ–åˆ›å»ºé›†åˆ
        self.collection = self.client.get_or_create_collection(
            name=f"{kb_id}_documents",
            metadata={"kb_id": kb_id}
        )
    
    def add_documents(self, documents: List[str], metadatas: List[Dict], 
                     embeddings: List[List[float]], ids: List[str]):
        """æ‰¹é‡æ·»åŠ æ–‡æ¡£"""
        try:
            self.collection.add(
                documents=documents,
                metadatas=metadatas,
                embeddings=embeddings,
                ids=ids
            )
            return True
        except Exception as e:
            print(f"Error adding documents: {e}")
            return False
    
    def query_documents(self, query_embedding: List[float], 
                       n_results: int = 5, where: Optional[Dict] = None):
        """æŸ¥è¯¢ç›¸ä¼¼æ–‡æ¡£"""
        try:
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                where=where,
                include=["documents", "metadatas", "distances"]
            )
            return results
        except Exception as e:
            print(f"Error querying documents: {e}")
            return None
    
    def delete_document(self, doc_id: str):
        """åˆ é™¤æ–‡æ¡£çš„æ‰€æœ‰å—"""
        try:
            # æŸ¥æ‰¾è¯¥æ–‡æ¡£çš„æ‰€æœ‰å—
            results = self.collection.get(
                where={"doc_id": doc_id},
                include=["metadatas"]
            )
            
            if results['ids']:
                self.collection.delete(ids=results['ids'])
                return len(results['ids'])
            return 0
        except Exception as e:
            print(f"Error deleting document: {e}")
            return 0
    
    def get_stats(self) -> Dict:
        """è·å–æ•°æ®åº“ç»Ÿè®¡ä¿¡æ¯"""
        try:
            count = self.collection.count()
            
            # è®¡ç®—æ•°æ®åº“å¤§å°
            db_size = 0
            for root, dirs, files in os.walk(self.db_path):
                for file in files:
                    db_size += os.path.getsize(os.path.join(root, file))
            
            return {
                "total_chunks": count,
                "db_size_bytes": db_size,
                "db_size_mb": round(db_size / (1024 * 1024), 2)
            }
        except Exception as e:
            print(f"Error getting stats: {e}")
            return {"total_chunks": 0, "db_size_bytes": 0, "db_size_mb": 0}
```

#### 4.2 æ•°æ®åº“ç®¡ç†å·¥å…·
```python
# utils/db_manager.py
import os
import shutil
import json
from datetime import datetime
from typing import Dict, List

class DatabaseManager:
    def __init__(self, base_path: str = "./knowledge_bases"):
        self.base_path = base_path
    
    def create_knowledge_base(self, kb_id: str, config: Dict) -> bool:
        """åˆ›å»ºæ–°çš„çŸ¥è¯†åº“"""
        kb_path = f"{self.base_path}/{kb_id}"
        
        try:
            # åˆ›å»ºç›®å½•ç»“æ„
            os.makedirs(f"{kb_path}/docs", exist_ok=True)
            os.makedirs(f"{kb_path}/db", exist_ok=True)
            
            # ä¿å­˜é…ç½®æ–‡ä»¶
            config_data = {
                **config,
                "created_at": datetime.now().isoformat(),
                "db_path": f"{kb_path}/db"
            }
            
            with open(f"{kb_path}/config.json", 'w', encoding='utf-8') as f:
                json.dump(config_data, f, ensure_ascii=False, indent=2)
            
            return True
        except Exception as e:
            print(f"Error creating knowledge base: {e}")
            return False
    
    def delete_knowledge_base(self, kb_id: str) -> bool:
        """åˆ é™¤çŸ¥è¯†åº“"""
        kb_path = f"{self.base_path}/{kb_id}"
        
        try:
            if os.path.exists(kb_path):
                shutil.rmtree(kb_path)
                return True
            return False
        except Exception as e:
            print(f"Error deleting knowledge base: {e}")
            return False
    
    def list_knowledge_bases(self) -> List[Dict]:
        """åˆ—å‡ºæ‰€æœ‰çŸ¥è¯†åº“"""
        knowledge_bases = []
        
        try:
            if not os.path.exists(self.base_path):
                return knowledge_bases
            
            for kb_id in os.listdir(self.base_path):
                kb_path = f"{self.base_path}/{kb_id}"
                config_path = f"{kb_path}/config.json"
                
                if os.path.isdir(kb_path) and os.path.exists(config_path):
                    with open(config_path, 'r', encoding='utf-8') as f:
                        config = json.load(f)
                    
                    # è·å–ç»Ÿè®¡ä¿¡æ¯
                    db = VectorDatabase(kb_id, config)
                    stats = db.get_stats()
                    
                    knowledge_bases.append({
                        "kb_id": kb_id,
                        "kb_name": config.get("kb_name", kb_id),
                        "description": config.get("description", ""),
                        "created_at": config.get("created_at", ""),
                        "doc_count": self._count_documents(kb_path),
                        "chunk_count": stats["total_chunks"],
                        "db_size": stats["db_size_mb"]
                    })
        except Exception as e:
            print(f"Error listing knowledge bases: {e}")
        
        return knowledge_bases
    
    def _count_documents(self, kb_path: str) -> int:
        """ç»Ÿè®¡æ–‡æ¡£æ•°é‡"""
        docs_path = f"{kb_path}/docs"
        if not os.path.exists(docs_path):
            return 0
        
        count = 0
        for root, dirs, files in os.walk(docs_path):
            count += len([f for f in files if not f.startswith('.')])
        
        return count
    
    def backup_knowledge_base(self, kb_id: str, backup_path: str) -> bool:
        """å¤‡ä»½çŸ¥è¯†åº“"""
        kb_path = f"{self.base_path}/{kb_id}"
        
        try:
            if os.path.exists(kb_path):
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_name = f"{kb_id}_backup_{timestamp}"
                full_backup_path = f"{backup_path}/{backup_name}"
                
                shutil.copytree(kb_path, full_backup_path)
                
                # åˆ›å»ºå¤‡ä»½ä¿¡æ¯æ–‡ä»¶
                backup_info = {
                    "kb_id": kb_id,
                    "backup_time": datetime.now().isoformat(),
                    "backup_path": full_backup_path
                }
                
                with open(f"{full_backup_path}/backup_info.json", 'w') as f:
                    json.dump(backup_info, f, indent=2)
                
                return True
            return False
        except Exception as e:
            print(f"Error backing up knowledge base: {e}")
            return False
```

#### 4.3 æ£€ç´¢ä¼˜åŒ–
```python
# core/retriever.py
from typing import List, Dict, Optional
import numpy as np

class KnowledgeRetriever:
    def __init__(self, kb_id: str, config: Dict):
        self.kb_id = kb_id
        self.config = config
        self.db = VectorDatabase(kb_id, config)
        self.embeddings = OpenAIEmbeddings(
            model=config.get('embedding_model', 'text-embedding-3-small')
        )
    
    async def retrieve(self, query: str, top_k: int = 5, 
                      filters: Optional[Dict] = None, 
                      min_score: float = 0.6) -> Dict:
        """è¯­ä¹‰æ£€ç´¢"""
        try:
            # 1. æŸ¥è¯¢å‘é‡åŒ–
            query_embedding = self.embeddings.embed_query(query)
            
            # 2. å‘é‡æ£€ç´¢
            results = self.db.query_documents(
                query_embedding=query_embedding,
                n_results=top_k * 2,  # å¤šæ£€ç´¢ä¸€äº›ï¼Œåç»­è¿‡æ»¤
                where=filters
            )
            
            if not results or not results['documents'][0]:
                return {
                    "success": True,
                    "query": query,
                    "kb_id": self.kb_id,
                    "chunks": [],
                    "total": 0
                }
            
            # 3. ç»“æœå¤„ç†å’Œè¿‡æ»¤
            chunks = []
            for i, (doc, metadata, distance) in enumerate(zip(
                results['documents'][0],
                results['metadatas'][0],
                results['distances'][0]
            )):
                # è½¬æ¢è·ç¦»ä¸ºç›¸ä¼¼åº¦åˆ†æ•° (0-1)
                similarity_score = 1 - distance
                
                if similarity_score >= min_score:
                    chunks.append({
                        "content": doc,
                        "metadata": metadata,
                        "score": round(similarity_score, 4),
                        "rank": len(chunks) + 1
                    })
                
                if len(chunks) >= top_k:
                    break
            
            return {
                "success": True,
                "query": query,
                "kb_id": self.kb_id,
                "chunks": chunks,
                "total": len(chunks)
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": str(e),
                "query": query,
                "kb_id": self.kb_id,
                "chunks": [],
                "total": 0
            }
    
    async def multi_kb_retrieve(self, query: str, kb_ids: List[str], 
                               top_k: int = 5) -> Dict:
        """å¤šçŸ¥è¯†åº“è”åˆæ£€ç´¢"""
        all_chunks = []
        
        for kb_id in kb_ids:
            try:
                # ä¸ºæ¯ä¸ªçŸ¥è¯†åº“åˆ›å»ºæ£€ç´¢å™¨
                retriever = KnowledgeRetriever(kb_id, self.config)
                result = await retriever.retrieve(query, top_k)
                
                if result['success']:
                    for chunk in result['chunks']:
                        chunk['kb_id'] = kb_id
                        all_chunks.append(chunk)
            except Exception as e:
                print(f"Error retrieving from {kb_id}: {e}")
        
        # æŒ‰åˆ†æ•°æ’åºå¹¶å–å‰ top_k
        all_chunks.sort(key=lambda x: x['score'], reverse=True)
        top_chunks = all_chunks[:top_k]
        
        # é‡æ–°æ’åº
        for i, chunk in enumerate(top_chunks):
            chunk['rank'] = i + 1
        
        return {
            "success": True,
            "query": query,
            "kb_ids": kb_ids,
            "chunks": top_chunks,
            "total": len(top_chunks)
        }
```

### âœ… éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|--------|----------|----------|
| æ•°æ®æŒä¹…åŒ– | å‘é‡æ•°æ®å­˜å‚¨åœ¨ç£ç›˜ï¼Œé‡å¯åæ•°æ®ä¸ä¸¢å¤± | æœåŠ¡é‡å¯æµ‹è¯• |
| æ£€ç´¢æ€§èƒ½ | å•æ¬¡æ£€ç´¢å“åº”æ—¶é—´ < 2ç§’ | æ€§èƒ½æµ‹è¯• |
| æ•°æ®ä¸€è‡´æ€§ | æ–‡æ¡£åˆ é™¤åç›¸å…³å‘é‡æ•°æ®ä¹Ÿè¢«åˆ é™¤ | æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥ |
| å¤‡ä»½æ¢å¤ | æ”¯æŒçŸ¥è¯†åº“å¤‡ä»½å’Œæ¢å¤ | å¤‡ä»½æ¢å¤æµ‹è¯• |
| ç»Ÿè®¡ä¿¡æ¯ | å‡†ç¡®æ˜¾ç¤ºæ–‡æ¡£æ•°ã€å—æ•°ã€æ•°æ®åº“å¤§å° | ç»Ÿè®¡æ•°æ®éªŒè¯ |
| å¤šåº“æ£€ç´¢ | æ”¯æŒè·¨çŸ¥è¯†åº“è”åˆæ£€ç´¢ | å¤šåº“æ£€ç´¢æµ‹è¯• |

---

## ğŸ¯ ç¬¬äº”é˜¶æ®µï¼šå‰ç«¯ç®¡ç†ç•Œé¢å¼€å‘

### ğŸ“… æ—¶é—´å®‰æ’
**é¢„è®¡å·¥æœŸ**: 2-3å‘¨

### ğŸ¯ ä¸»è¦ä»»åŠ¡

#### 5.1 React é¡¹ç›®æ­å»º
```bash
# åˆ›å»º React é¡¹ç›®
npx create-react-app frontend --template typescript
cd frontend

# å®‰è£…ä¾èµ–
npm install antd @ant-design/icons
npm install axios zustand
npm install @types/node
npm install react-router-dom
```

#### 5.2 é¡¹ç›®ç»“æ„è®¾è®¡
```

```

#### 5.3 æ ¸å¿ƒç»„ä»¶å®ç°

**çŸ¥è¯†åº“ç®¡ç†é¡µé¢**
```typescript
// pages/KnowledgeBase/index.tsx
import React, { useEffect, useState } from 'react';
import { Table, Button, Modal, Form, Input, message, Space, Popconfirm } from 'antd';
import { PlusOutlined, DeleteOutlined, EditOutlined } from '@ant-design/icons';
import { useKnowledgeBaseStore } from '../../stores/useKnowledgeBase';
import type { KnowledgeBase } from '../../types';

const KnowledgeBasePage: React.FC = () => {
  const [isModalVisible, setIsModalVisible] = useState(false);
  const [form] = Form.useForm();
  const { 
    knowledgeBases, 
    loading, 
    fetchKnowledgeBases, 
    createKnowledgeBase, 
    deleteKnowledgeBase 
  } = useKnowledgeBaseStore();

  useEffect(() => {
    fetchKnowledgeBases();
  }, []);

  const handleCreate = async (values: any) => {
    try {
      await createKnowledgeBase(values);
      message.success('çŸ¥è¯†åº“åˆ›å»ºæˆåŠŸ');
      setIsModalVisible(false);
      form.resetFields();
      fetchKnowledgeBases();
    } catch (error) {
      message.error('åˆ›å»ºå¤±è´¥');
    }
  };

  const handleDelete = async (kbId: string) => {
    try {
      await deleteKnowledgeBase(kbId);
      message.success('çŸ¥è¯†åº“åˆ é™¤æˆåŠŸ');
      fetchKnowledgeBases();
    } catch (error) {
      message.error('åˆ é™¤å¤±è´¥');
    }
  };

  const columns = [
    {
      title: 'çŸ¥è¯†åº“ID',
      dataIndex: 'kb_id',
      key: 'kb_id',
    },
    {
      title: 'çŸ¥è¯†åº“åç§°',
      dataIndex: 'kb_name',
      key: 'kb_name',
    },
    {
      title: 'æè¿°',
      dataIndex: 'description',
      key: 'description',
    },
    {
      title: 'æ–‡æ¡£æ•°',
      dataIndex: 'doc_count',
      key: 'doc_count',
    },
    {
      title: 'æ•°æ®å—æ•°',
      dataIndex: 'chunk_count',
      key: 'chunk_count',
    },
    {
      title: 'æ•°æ®åº“å¤§å°',
      dataIndex: 'db_size',
      key: 'db_size',
      render: (size: number) => `${size} MB`,
    },
    {
      title: 'åˆ›å»ºæ—¶é—´',
      dataIndex: 'created_at',
      key: 'created_at',
      render: (date: string) => new Date(date).toLocaleString(),
    },
    {
      title: 'æ“ä½œ',
      key: 'action',
      render: (_, record: KnowledgeBase) => (
        <Space size="middle">
          <Button 
            type="link" 
            icon={<EditOutlined />}
            onClick={() => {/* ç¼–è¾‘é€»è¾‘ */}}
          >
            ç¼–è¾‘
          </Button>
          <Popconfirm
            title="ç¡®å®šè¦åˆ é™¤è¿™ä¸ªçŸ¥è¯†åº“å—ï¼Ÿ"
            onConfirm={() => handleDelete(record.kb_id)}
            okText="ç¡®å®š"
            cancelText="å–æ¶ˆ"
          >
            <Button 
              type="link" 
              danger 
              icon={<DeleteOutlined />}
            >
              åˆ é™¤
            </Button>
          </Popconfirm>
        </Space>
      ),
    },
  ];

  return (
    <div>
      <div style={{ marginBottom: 16 }}>
        <Button 
          type="primary" 
          icon={<PlusOutlined />}
          onClick={() => setIsModalVisible(true)}
        >
          åˆ›å»ºçŸ¥è¯†åº“
        </Button>
      </div>
      
      <Table 
        columns={columns} 
        dataSource={knowledgeBases}
        rowKey="kb_id"
        loading={loading}
      />

      <Modal
        title="åˆ›å»ºçŸ¥è¯†åº“"
        visible={isModalVisible}
        onCancel={() => setIsModalVisible(false)}
        footer={null}
      >
        <Form
          form={form}
          layout="vertical"
          onFinish={handleCreate}
        >
          <Form.Item
            name="kb_id"
            label="çŸ¥è¯†åº“ID"
            rules={[{ required: true, message: 'è¯·è¾“å…¥çŸ¥è¯†åº“ID' }]}
          >
            <Input placeholder="ä¾‹å¦‚: crm_system" />
          </Form.Item>
          
          <Form.Item
            name="kb_name"
            label="çŸ¥è¯†åº“åç§°"
            rules={[{ required: true, message: 'è¯·è¾“å…¥çŸ¥è¯†åº“åç§°' }]}
          >
            <Input placeholder="ä¾‹å¦‚: CRMç³»ç»ŸçŸ¥è¯†åº“" />
          </Form.Item>
          
          <Form.Item
            name="description"
            label="æè¿°"
          >
            <Input.TextArea placeholder="çŸ¥è¯†åº“æè¿°ä¿¡æ¯" />
          </Form.Item>
          
          <Form.Item>
            <Space>
              <Button type="primary" htmlType="submit">
                åˆ›å»º
              </Button>
              <Button onClick={() => setIsModalVisible(false)}>
                å–æ¶ˆ
              </Button>
            </Space>
          </Form.Item>
        </Form>
      </Modal>
    </div>
  );
};

export default KnowledgeBasePage;
```

**æ–‡æ¡£ç®¡ç†é¡µé¢**
```typescript
// pages/Documents/index.tsx
import React, { useEffect, useState } from 'react';
import { Upload, Table, Button, message, Progress, Tag, Space } from 'antd';
import { InboxOutlined, DeleteOutlined, ReloadOutlined } from '@ant-design/icons';
import { useDocumentStore } from '../../stores/useDocuments';
import { useKnowledgeBaseStore } from '../../stores/useKnowledgeBase';

const { Dragger } = Upload;

const DocumentsPage: React.FC = () => {
  const [selectedKbId, setSelectedKbId] = useState<string>('');
  const { 
    documents, 
    loading, 
    uploadProgress,
    fetchDocuments, 
    uploadDocuments, 
    deleteDocument 
  } = useDocumentStore();
  const { knowledgeBases } = useKnowledgeBaseStore();

  const uploadProps = {
    name: 'files',
    multiple: true,
    accept: '.txt,.md,.docx,.pdf,.html,.pptx',
    beforeUpload: () => false, // é˜»æ­¢è‡ªåŠ¨ä¸Šä¼ 
    onChange: async (info: any) => {
      if (info.fileList.length > 0 && selectedKbId) {
        const files = info.fileList.map((file: any) => file.originFileObj);
        await uploadDocuments(selectedKbId, files);
        message.success('æ–‡æ¡£ä¸Šä¼ æˆåŠŸï¼Œæ­£åœ¨åå°å¤„ç†...');
      }
    },
  };

  const columns = [
    {
      title: 'æ–‡æ¡£åç§°',
      dataIndex: 'filename',
      key: 'filename',
    },
    {
      title: 'æ–‡æ¡£ç±»å‹',
      dataIndex: 'doc_type',
      key: 'doc_type',
      render: (type: string) => <Tag color="blue">{type}</Tag>,
    },
    {
      title: 'å¤„ç†çŠ¶æ€',
      dataIndex: 'status',
      key: 'status',
      render: (status: string) => {
        const colors = {
          'processing': 'orange',
          'completed': 'green',
          'failed': 'red'
        };
        return <Tag color={colors[status as keyof typeof colors]}>{status}</Tag>;
      },
    },
    {
      title: 'æ•°æ®å—æ•°',
      dataIndex: 'chunk_count',
      key: 'chunk_count',
    },
    {
      title: 'ä¸Šä¼ æ—¶é—´',
      dataIndex: 'upload_time',
      key: 'upload_time',
      render: (time: string) => new Date(time).toLocaleString(),
    },
    {
      title: 'æ“ä½œ',
      key: 'action',
      render: (_, record: any) => (
        <Space size="middle">
          {record.status === 'failed' && (
            <Button 
              type="link" 
              icon={<ReloadOutlined />}
              onClick={() => {/* é‡æ–°å¤„ç†é€»è¾‘ */}}
            >
              é‡æ–°å¤„ç†
            </Button>
          )}
          <Button 
            type="link" 
            danger 
            icon={<DeleteOutlined />}
            onClick={() => deleteDocument(selectedKbId, record.doc_id)}
          >
            åˆ é™¤
          </Button>
        </Space>
      ),
    },
  ];

  return (
    <div>
      <div style={{ marginBottom: 16 }}>
        <select 
          value={selectedKbId} 
          onChange={(e) => setSelectedKbId(e.target.value)}
          style={{ marginRight: 16, padding: '4px 8px' }}
        >
          <option value="">é€‰æ‹©çŸ¥è¯†åº“</option>
          {knowledgeBases.map(kb => (
            <option key={kb.kb_id} value={kb.kb_id}>
              {kb.kb_name}
            </option>
          ))}
        </select>
      </div>

      {selectedKbId && (
        <>
          <Dragger {...uploadProps} style={{ marginBottom: 16 }}>
            <p className="ant-upload-drag-icon">
              <InboxOutlined />
            </p>
            <p className="ant-upload-text">ç‚¹å‡»æˆ–æ‹–æ‹½æ–‡ä»¶åˆ°æ­¤åŒºåŸŸä¸Šä¼ </p>
            <p className="ant-upload-hint">
              æ”¯æŒ .txt, .md, .docx, .pdf, .html, .pptx æ ¼å¼
            </p>
          </Dragger>

          {uploadProgress > 0 && uploadProgress < 100 && (
            <Progress percent={uploadProgress} style={{ marginBottom: 16 }} />
          )}

          <Table 
            columns={columns} 
            dataSource={documents}
            rowKey="doc_id"
            loading={loading}
          />
        </>
      )}
    </div>
  );
};

export default DocumentsPage;
```

#### 5.4 çŠ¶æ€ç®¡ç†
```typescript
// stores/useKnowledgeBase.ts
import { create } from 'zustand';
import { knowledgeBaseApi } from '../services/knowledgeBase';
import type { KnowledgeBase } from '../types';

interface KnowledgeBaseState {
  knowledgeBases: KnowledgeBase[];
  currentKb: KnowledgeBase | null;
  loading: boolean;
  fetchKnowledgeBases: () => Promise<void>;
  createKnowledgeBase: (data: any) => Promise<void>;
  deleteKnowledgeBase: (kbId: string) => Promise<void>;
  setCurrentKb: (kb: KnowledgeBase) => void;
}

export const useKnowledgeBaseStore = create<KnowledgeBaseState>((set, get) => ({
  knowledgeBases: [],
  currentKb: null,
  loading: false,

  fetchKnowledgeBases: async () => {
    set({ loading: true });
    try {
      const data = await knowledgeBaseApi.list();
      set({ knowledgeBases: data, loading: false });
    } catch (error) {
      set({ loading: false });
      throw error;
    }
  },

  createKnowledgeBase: async (data) => {
    await knowledgeBaseApi.create(data);
  },

  deleteKnowledgeBase: async (kbId) => {
    await knowledgeBaseApi.delete(kbId);
  },

  setCurrentKb: (kb) => {
    set({ currentKb: kb });
  },
}));
```

### âœ… éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|--------|----------|----------|
| ç•Œé¢å“åº” | é¡µé¢åŠ è½½æ—¶é—´ < 3ç§’ï¼Œæ“ä½œå“åº” < 1ç§’ | æ€§èƒ½æµ‹è¯• |
| åŠŸèƒ½å®Œæ•´ | çŸ¥è¯†åº“CRUDã€æ–‡æ¡£ä¸Šä¼ ã€æ£€ç´¢æµ‹è¯•åŠŸèƒ½æ­£å¸¸ | åŠŸèƒ½æµ‹è¯• |
| ç”¨æˆ·ä½“éªŒ | ç•Œé¢ç¾è§‚ï¼Œæ“ä½œæµç•…ï¼Œé”™è¯¯æç¤ºå‹å¥½ | ç”¨æˆ·ä½“éªŒæµ‹è¯• |
| å…¼å®¹æ€§ | æ”¯æŒä¸»æµæµè§ˆå™¨ï¼ˆChrome, Firefox, Safariï¼‰ | æµè§ˆå™¨å…¼å®¹æ€§æµ‹è¯• |
| å“åº”å¼ | æ”¯æŒä¸åŒå±å¹•å°ºå¯¸ | å“åº”å¼æµ‹è¯• |
| çŠ¶æ€ç®¡ç† | æ•°æ®çŠ¶æ€åŒæ­¥æ­£ç¡®ï¼Œæ— å†…å­˜æ³„æ¼ | çŠ¶æ€ç®¡ç†æµ‹è¯• |

---

## ğŸ¯ ç¬¬å…­é˜¶æ®µï¼šDifyé›†æˆä¸APIå¯¹æ¥

### ğŸ“… æ—¶é—´å®‰æ’
**é¢„è®¡å·¥æœŸ**: 1å‘¨

### ğŸ¯ ä¸»è¦ä»»åŠ¡

#### 6.1 Dify API å·¥å…·é…ç½®
```json
{
  "name": "Knowledge Retrieval",
  "description": "ä»ä¼ä¸šçŸ¥è¯†åº“ä¸­æ£€ç´¢ç›¸å…³ä¿¡æ¯",
  "url": "http://your-domain:8000/api/retrieve",
  "method": "POST",
  "headers": {
    "Content-Type": "application/json"
  },
  "parameters": {
    "query": {
      "type": "string",
      "description": "ç”¨æˆ·æŸ¥è¯¢é—®é¢˜",
      "required": true
    },
    "kb_id": {
      "type": "string", 
      "description": "çŸ¥è¯†åº“ID",
      "required": true,
      "default": "crm_system"
    },
    "top_k": {
      "type": "integer",
      "description": "è¿”å›ç»“æœæ•°é‡",
      "default": 5
    },
    "min_score": {
      "type": "number",
      "description": "æœ€å°ç›¸ä¼¼åº¦åˆ†æ•°",
      "default": 0.6
    }
  }
}
```

#### 6.2 æ£€ç´¢æ¥å£ä¼˜åŒ–
```python
# api/retrieve.py - ä¸º Dify ä¼˜åŒ–çš„æ£€ç´¢æ¥å£
@router.post("/retrieve", response_model=RetrieveResponse)
async def retrieve_for_dify(request: RetrieveRequest):
    """
    ä¸º Dify ä¼˜åŒ–çš„æ£€ç´¢æ¥å£
    è¿”å›æ ¼å¼åŒ–çš„çŸ¥è¯†å†…å®¹
    """
    try:
        # åŠ è½½çŸ¥è¯†åº“é…ç½®
        config = load_kb_config(request.kb_id)
        if not config:
            raise HTTPException(status_code=404, detail=f"Knowledge base {request.kb_id} not found")
        
        # æ‰§è¡Œæ£€ç´¢
        retriever = KnowledgeRetriever(request.kb_id, config)
        result = await retriever.retrieve(
            query=request.query,
            top_k=request.top_k,
            filters=request.filters,
            min_score=request.min_score
        )
        
        if not result['success']:
            raise HTTPException(status_code=500, detail=result.get('error', 'Retrieval failed'))
        
        # æ ¼å¼åŒ–è¿”å›ç»“æœï¼Œä¾¿äº Dify ä½¿ç”¨
        formatted_chunks = []
        for chunk in result['chunks']:
            formatted_chunks.append({
                "content": chunk['content'],
                "source": chunk['metadata'].get('filename', 'unknown'),
                "doc_type": chunk['metadata'].get('doc_type', 'unknown'),
                "system": chunk['metadata'].get('system', 'unknown'),
                "score": chunk['score'],
                "rank": chunk['rank']
            })
        
        # ç”Ÿæˆæ‘˜è¦æ–‡æœ¬ï¼ˆä¾› Dify ç›´æ¥ä½¿ç”¨ï¼‰
        summary_text = generate_summary_text(formatted_chunks, request.query)
        
        return RetrieveResponse(
            success=True,
            query=request.query,
            kb_id=request.kb_id,
            chunks=formatted_chunks,
            total=len(formatted_chunks),
            summary=summary_text  # æ–°å¢æ‘˜è¦å­—æ®µ
        )
        
    except HTTPException:
        raise
    except Exception as e:
        logger.error(f"Retrieval error: {e}")
        raise HTTPException(status_code=500, detail=str(e))

def generate_summary_text(chunks: List[Dict], query: str) -> str:
    """
    ç”Ÿæˆæ£€ç´¢ç»“æœæ‘˜è¦æ–‡æœ¬
    ä¾› Dify å·¥ä½œæµç›´æ¥ä½¿ç”¨
    """
    if not chunks:
        return f"æœªæ‰¾åˆ°ä¸ '{query}' ç›¸å…³çš„ä¿¡æ¯ã€‚"
    
    summary_parts = [f"æ ¹æ®æŸ¥è¯¢ '{query}'ï¼Œæ‰¾åˆ°ä»¥ä¸‹ç›¸å…³ä¿¡æ¯ï¼š\n"]
    
    for i, chunk in enumerate(chunks[:3], 1):  # åªå–å‰3ä¸ªæœ€ç›¸å…³çš„
        summary_parts.append(
            f"{i}. æ¥æºï¼š{chunk['source']} ({chunk['doc_type']})\n"
            f"   å†…å®¹ï¼š{chunk['content'][:200]}...\n"
            f"   ç›¸å…³åº¦ï¼š{chunk['score']:.2f}\n"
        )
    
    if len(chunks) > 3:
        summary_parts.append(f"\nå¦å¤–è¿˜æ‰¾åˆ° {len(chunks) - 3} æ¡ç›¸å…³ä¿¡æ¯ã€‚")
    
    return "\n".join(summary_parts)
```

#### 6.3 Dify å·¥ä½œæµé›†æˆç¤ºä¾‹
```yaml
# Dify å·¥ä½œæµé…ç½®ç¤ºä¾‹
workflow:
  name: "ä¼ä¸šçŸ¥è¯†é—®ç­”"
  description: "åŸºäºä¼ä¸šçŸ¥è¯†åº“çš„æ™ºèƒ½é—®ç­”"
  
  nodes:
    - id: "start"
      type: "start"
      data:
        title: "å¼€å§‹"
        variables:
          - name: "user_query"
            type: "string"
            description: "ç”¨æˆ·é—®é¢˜"
    
    - id: "knowledge_retrieval"
      type: "tool"
      data:
        title: "çŸ¥è¯†æ£€ç´¢"
        tool_name: "Knowledge Retrieval"
        tool_parameters:
          query: "{{#start.user_query#}}"
          kb_id: "crm_system"
          top_k: 5
          min_score: 0.6
    
    - id: "llm_answer"
      type: "llm"
      data:
        title: "ç”Ÿæˆç­”æ¡ˆ"
        model: "gpt-3.5-turbo"
        prompt: |
          ä½ æ˜¯ä¸€ä¸ªä¼ä¸šçŸ¥è¯†åŠ©æ‰‹ã€‚åŸºäºä»¥ä¸‹æ£€ç´¢åˆ°çš„çŸ¥è¯†å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
          
          ç”¨æˆ·é—®é¢˜ï¼š{{#start.user_query#}}
          
          æ£€ç´¢åˆ°çš„çŸ¥è¯†ï¼š
          {{#knowledge_retrieval.summary#}}
          
          è¯·åŸºäºä¸Šè¿°çŸ¥è¯†å†…å®¹ï¼Œç»™å‡ºå‡†ç¡®ã€è¯¦ç»†çš„å›ç­”ã€‚å¦‚æœçŸ¥è¯†å†…å®¹ä¸è¶³ä»¥å›ç­”é—®é¢˜ï¼Œè¯·è¯´æ˜éœ€è¦æ›´å¤šä¿¡æ¯ã€‚
    
    - id: "end"
      type: "end"
      data:
        title: "ç»“æŸ"
        outputs:
          - name: "answer"
            type: "string"
            value: "{{#llm_answer.text#}}"
          - name: "sources"
            type: "array"
            value: "{{#knowledge_retrieval.chunks#}}"

  edges:
    - source: "start"
      target: "knowledge_retrieval"
    - source: "knowledge_retrieval"
      target: "llm_answer"
    - source: "llm_answer"
      target: "end"
```

#### 6.4 é›†æˆæµ‹è¯•å·¥å…·
```python
# tests/test_dify_integration.py
import asyncio
import httpx
import json

class DifyIntegrationTester:
    def __init__(self, api_base_url: str, dify_api_key: str):
        self.api_base_url = api_base_url
        self.dify_api_key = dify_api_key
    
    async def test_retrieve_api(self):
        """æµ‹è¯•æ£€ç´¢ API"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                f"{self.api_base_url}/api/retrieve",
                json={
                    "query": "å¦‚ä½•é…ç½®ç”¨æˆ·æƒé™",
                    "kb_id": "crm_system",
                    "top_k": 5,
                    "min_score": 0.6
                }
            )
            
            assert response.status_code == 200
            data = response.json()
            assert data['success'] == True
            assert len(data['chunks']) > 0
            
            print(f"âœ… æ£€ç´¢æµ‹è¯•é€šè¿‡ï¼Œè¿”å› {data['total']} æ¡ç»“æœ")
            return data
    
    async def test_dify_workflow(self, user_query: str):
        """æµ‹è¯• Dify å·¥ä½œæµ"""
        async with httpx.AsyncClient() as client:
            response = await client.post(
                "https://api.dify.ai/v1/workflows/run",
                headers={
                    "Authorization": f"Bearer {self.dify_api_key}",
                    "Content-Type": "application/json"
                },
                json={
                    "inputs": {
                        "user_query": user_query
                    },
                    "response_mode": "blocking"
                }
            )
            
            assert response.status_code == 200
            data = response.json()
            
            print(f"âœ… Dify å·¥ä½œæµæµ‹è¯•é€šè¿‡")
            print(f"ç”¨æˆ·é—®é¢˜ï¼š{user_query}")
            print(f"AI å›ç­”ï¼š{data['data']['outputs']['answer']}")
            
            return data

# è¿è¡Œæµ‹è¯•
async def run_integration_tests():
    tester = DifyIntegrationTester(
        api_base_url="http://localhost:8000",
        dify_api_key="your-dify-api-key"
    )
    
    # æµ‹è¯•æ£€ç´¢ API
    await tester.test_retrieve_api()
    
    # æµ‹è¯• Dify å·¥ä½œæµ
    test_queries = [
        "å¦‚ä½•é…ç½®ç”¨æˆ·æƒé™ï¼Ÿ",
        "ç³»ç»Ÿç™»å½•æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ",
        "å¦‚ä½•å¤„ç†å®¢æˆ·æŠ•è¯‰ï¼Ÿ"
    ]
    
    for query in test_queries:
        await tester.test_dify_workflow(query)

if __name__ == "__main__":
    asyncio.run(run_integration_tests())
```

### âœ… éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|--------|----------|----------|
| API å¯¹æ¥ | Dify èƒ½æˆåŠŸè°ƒç”¨æ£€ç´¢æ¥å£ | API è°ƒç”¨æµ‹è¯• |
| æ•°æ®æ ¼å¼ | è¿”å›æ•°æ®æ ¼å¼ç¬¦åˆ Dify è¦æ±‚ | æ•°æ®æ ¼å¼éªŒè¯ |
| å·¥ä½œæµé›†æˆ | Dify å·¥ä½œæµèƒ½æ­£å¸¸è¿è¡Œ | å·¥ä½œæµæµ‹è¯• |
| å“åº”æ—¶é—´ | API å“åº”æ—¶é—´ < 3ç§’ | æ€§èƒ½æµ‹è¯• |
| é”™è¯¯å¤„ç† | å¼‚å¸¸æƒ…å†µæœ‰åˆé€‚çš„é”™è¯¯è¿”å› | å¼‚å¸¸æµ‹è¯• |
| æ–‡æ¡£å®Œæ•´ | é›†æˆæ–‡æ¡£å’Œç¤ºä¾‹å®Œæ•´ | æ–‡æ¡£æ£€æŸ¥ |

---

## ğŸ¯ ç¬¬ä¸ƒé˜¶æ®µï¼šç³»ç»Ÿæµ‹è¯•ä¸æ€§èƒ½ä¼˜åŒ–

### ğŸ“… æ—¶é—´å®‰æ’
**é¢„è®¡å·¥æœŸ**: 1-2å‘¨

### ğŸ¯ ä¸»è¦ä»»åŠ¡

#### 7.1 åŠŸèƒ½æµ‹è¯•
```python
# tests/test_functional.py
import pytest
import asyncio
from httpx import AsyncClient
from fastapi.testclient import TestClient
from main import app

class TestKnowledgeBase:
    def setup_class(self):
        self.client = TestClient(app)
    
    def test_create_knowledge_base(self):
        """æµ‹è¯•åˆ›å»ºçŸ¥è¯†åº“"""
        response = self.client.post("/api/kb/create", json={
            "kb_id": "test_kb",
            "kb_name": "æµ‹è¯•çŸ¥è¯†åº“",
            "description": "ç”¨äºæµ‹è¯•çš„çŸ¥è¯†åº“"
        })
        assert response.status_code == 200
        assert response.json()["success"] == True
    
    def test_upload_document(self):
        """æµ‹è¯•æ–‡æ¡£ä¸Šä¼ """
        with open("test_document.txt", "rb") as f:
            response = self.client.post(
                "/api/kb/test_kb/documents/upload",
                files={"files": ("test.txt", f, "text/plain")}
            )
        assert response.status_code == 200
    
    def test_retrieve_knowledge(self):
        """æµ‹è¯•çŸ¥è¯†æ£€ç´¢"""
        response = self.client.post("/api/retrieve", json={
            "query": "æµ‹è¯•æŸ¥è¯¢",
            "kb_id": "test_kb",
            "top_k": 5
        })
        assert response.status_code == 200
        data = response.json()
        assert "chunks" in data
```

#### 7.2 æ€§èƒ½æµ‹è¯•
```python
# tests/test_performance.py
import asyncio
import time
import statistics
from concurrent.futures import ThreadPoolExecutor
import httpx

class PerformanceTest:
    def __init__(self, base_url: str):
        self.base_url = base_url
    
    async def test_concurrent_retrieval(self, concurrent_users: int = 10):
        """æµ‹è¯•å¹¶å‘æ£€ç´¢æ€§èƒ½"""
        async def single_request():
            async with httpx.AsyncClient() as client:
                start_time = time.time()
                response = await client.post(
                    f"{self.base_url}/api/retrieve",
                    json={
                        "query": "æµ‹è¯•æŸ¥è¯¢",
                        "kb_id": "test_kb",
                        "top_k": 5
                    }
                )
                end_time = time.time()
                return end_time - start_time, response.status_code
        
        # å¹¶å‘æµ‹è¯•
        tasks = [single_request() for _ in range(concurrent_users)]
        results = await asyncio.gather(*tasks)
        
        response_times = [r[0] for r in results]
        status_codes = [r[1] for r in results]
        
        # æ€§èƒ½æŒ‡æ ‡
        avg_response_time = statistics.mean(response_times)
        max_response_time = max(response_times)
        success_rate = sum(1 for code in status_codes if code == 200) / len(status_codes)
        
        print(f"å¹¶å‘ç”¨æˆ·æ•°: {concurrent_users}")
        print(f"å¹³å‡å“åº”æ—¶é—´: {avg_response_time:.2f}s")
        print(f"æœ€å¤§å“åº”æ—¶é—´: {max_response_time:.2f}s")
        print(f"æˆåŠŸç‡: {success_rate:.2%}")
        
        # æ€§èƒ½è¦æ±‚éªŒè¯
        assert avg_response_time < 2.0, f"å¹³å‡å“åº”æ—¶é—´è¶…æ ‡: {avg_response_time}s"
        assert success_rate > 0.95, f"æˆåŠŸç‡è¿‡ä½: {success_rate}"
    
    def test_memory_usage(self):
        """æµ‹è¯•å†…å­˜ä½¿ç”¨æƒ…å†µ"""
        import psutil
        import os
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ‰§è¡Œå¤§é‡æ“ä½œ
        for i in range(100):
            # æ¨¡æ‹Ÿæ–‡æ¡£å¤„ç†
            pass
        
        final_memory = process.memory_info().rss / 1024 / 1024  # MB
        memory_increase = final_memory - initial_memory
        
        print(f"åˆå§‹å†…å­˜: {initial_memory:.2f} MB")
        print(f"æœ€ç»ˆå†…å­˜: {final_memory:.2f} MB")
        print(f"å†…å­˜å¢é•¿: {memory_increase:.2f} MB")
        
        # å†…å­˜æ³„æ¼æ£€æŸ¥
        assert memory_increase < 100, f"å†…å­˜å¢é•¿è¿‡å¤š: {memory_increase} MB"
```

#### 7.3 å®‰å…¨æµ‹è¯•
```python
# tests/test_security.py
import requests
import json

class SecurityTest:
    def __init__(self, base_url: str):
        self.base_url = base_url
    
    def test_sql_injection(self):
        """æµ‹è¯• SQL æ³¨å…¥é˜²æŠ¤"""
        malicious_queries = [
            "'; DROP TABLE users; --",
            "' OR '1'='1",
            "admin'--",
            "' UNION SELECT * FROM users --"
        ]
        
        for query in malicious_queries:
            response = requests.post(
                f"{self.base_url}/api/retrieve",
                json={"query": query, "kb_id": "test_kb"}
            )
            # åº”è¯¥æ­£å¸¸å¤„ç†ï¼Œä¸åº”è¯¥å‡ºç°æ•°æ®åº“é”™è¯¯
            assert response.status_code in [200, 400]
    
    def test_xss_protection(self):
        """æµ‹è¯• XSS é˜²æŠ¤"""
        xss_payloads = [
            "<script>alert('xss')</script>",
            "javascript:alert('xss')",
            "<img src=x onerror=alert('xss')>"
        ]
        
        for payload in xss_payloads:
            response = requests.post(
                f"{self.base_url}/api/kb/create",
                json={
                    "kb_id": "test",
                    "kb_name": payload,
                    "description": payload
                }
            )
            # æ£€æŸ¥å“åº”ä¸­æ˜¯å¦åŒ…å«æœªè½¬ä¹‰çš„è„šæœ¬
            if response.status_code == 200:
                assert "<script>" not in response.text
    
    def test_file_upload_security(self):
        """æµ‹è¯•æ–‡ä»¶ä¸Šä¼ å®‰å…¨æ€§"""
        # æµ‹è¯•æ¶æ„æ–‡ä»¶ä¸Šä¼ 
        malicious_files = [
            ("test.exe", b"MZ\x90\x00"),  # å¯æ‰§è¡Œæ–‡ä»¶
            ("test.php", b"<?php system($_GET['cmd']); ?>"),  # PHP è„šæœ¬
            ("../../../etc/passwd", b"root:x:0:0:root:/root:/bin/bash")  # è·¯å¾„éå†
        ]
        
        for filename, content in malicious_files:
            response = requests.post(
                f"{self.base_url}/api/kb/test_kb/documents/upload",
                files={"files": (filename, content, "text/plain")}
            )
            # åº”è¯¥æ‹’ç»æ¶æ„æ–‡ä»¶
            assert response.status_code in [400, 403, 415]
```

#### 7.4 ç›‘æ§é…ç½®
```python
# monitoring/metrics.py
from prometheus_client import Counter, Histogram, Gauge, start_http_server
import time
import functools

# å®šä¹‰ç›‘æ§æŒ‡æ ‡
REQUEST_COUNT = Counter('api_requests_total', 'Total API requests', ['method', 'endpoint', 'status'])
REQUEST_DURATION = Histogram('api_request_duration_seconds', 'API request duration')
ACTIVE_CONNECTIONS = Gauge('active_connections', 'Active connections')
KNOWLEDGE_BASE_COUNT = Gauge('knowledge_bases_total', 'Total knowledge bases')
DOCUMENT_COUNT = Gauge('documents_total', 'Total documents')

def monitor_requests(func):
    """API è¯·æ±‚ç›‘æ§è£…é¥°å™¨"""
    @functools.wraps(func)
    async def wrapper(*args, **kwargs):
        start_time = time.time()
        try:
            result = await func(*args, **kwargs)
            REQUEST_COUNT.labels(method='POST', endpoint=func.__name__, status='success').inc()
            return result
        except Exception as e:
            REQUEST_COUNT.labels(method='POST', endpoint=func.__name__, status='error').inc()
            raise
        finally:
            REQUEST_DURATION.observe(time.time() - start_time)
    
    return wrapper

class SystemMonitor:
    def __init__(self):
        self.start_time = time.time()
    
    def update_metrics(self):
        """æ›´æ–°ç³»ç»ŸæŒ‡æ ‡"""
        # æ›´æ–°çŸ¥è¯†åº“æ•°é‡
        kb_count = len(self.get_knowledge_bases())
        KNOWLEDGE_BASE_COUNT.set(kb_count)
        
        # æ›´æ–°æ–‡æ¡£æ•°é‡
        doc_count = self.get_total_document_count()
        DOCUMENT_COUNT.set(doc_count)
    
    def start_metrics_server(self, port: int = 8001):
        """å¯åŠ¨ç›‘æ§æŒ‡æ ‡æœåŠ¡å™¨"""
        start_http_server(port)
        print(f"Metrics server started on port {port}")
```

### âœ… éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|--------|----------|----------|
| åŠŸèƒ½æµ‹è¯• | æ‰€æœ‰æ ¸å¿ƒåŠŸèƒ½æµ‹è¯•é€šè¿‡ç‡ > 95% | è‡ªåŠ¨åŒ–æµ‹è¯• |
| æ€§èƒ½æµ‹è¯• | å¹¶å‘50ç”¨æˆ·ï¼Œå¹³å‡å“åº”æ—¶é—´ < 2ç§’ | å‹åŠ›æµ‹è¯• |
| å®‰å…¨æµ‹è¯• | é€šè¿‡ SQL æ³¨å…¥ã€XSSã€æ–‡ä»¶ä¸Šä¼ å®‰å…¨æµ‹è¯• | å®‰å…¨æ‰«æ |
| ç›‘æ§ç³»ç»Ÿ | å…³é”®æŒ‡æ ‡ç›‘æ§æ­£å¸¸ï¼Œå‘Šè­¦æœºåˆ¶æœ‰æ•ˆ | ç›‘æ§æµ‹è¯• |
| é”™è¯¯å¤„ç† | å¼‚å¸¸æƒ…å†µæœ‰åˆé€‚çš„é”™è¯¯ç å’Œä¿¡æ¯ | å¼‚å¸¸æµ‹è¯• |
| æ—¥å¿—è®°å½• | å…³é”®æ“ä½œæœ‰å®Œæ•´çš„æ—¥å¿—è®°å½• | æ—¥å¿—æ£€æŸ¥ |

---

## ğŸ¯ ç¬¬å…«é˜¶æ®µï¼šç”Ÿäº§éƒ¨ç½²ä¸è¿ç»´

### ğŸ“… æ—¶é—´å®‰æ’
**é¢„è®¡å·¥æœŸ**: 1-2å‘¨

### ğŸ¯ ä¸»è¦ä»»åŠ¡

#### 8.1 Docker å®¹å™¨åŒ–
```dockerfile
# Dockerfile
FROM python:3.10-slim

WORKDIR /app

# å®‰è£…ç³»ç»Ÿä¾èµ–
RUN apt-get update && apt-get install -y \
    gcc \
    g++ \
    && rm -rf /var/lib/apt/lists/*

# å¤åˆ¶ä¾èµ–æ–‡ä»¶
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶åº”ç”¨ä»£ç 
COPY backend/ ./backend/
COPY knowledge_bases/ ./knowledge_bases/

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV PYTHONPATH=/app/backend
ENV PYTHONUNBUFFERED=1

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "backend.main:app", "--host", "0.0.0.0", "--port", "8000"]
```



```yaml
# docker-compose.yml
version: '3.8'

services:
  knowledge-hub-api:
    build: .
    ports:
      - "8000:8000"
    environment:
      - OPENAI_API_KEY=${OPENAI_API_KEY}
      - DB_PATH=/app/knowledge_bases
    volumes:
      - ./knowledge_bases:/app/knowledge_bases
      - ./logs:/app/logs
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/health"]
      interval: 30s
      timeout: 10s
      retries: 3

  knowledge-hub-frontend:
    build: ./frontend
    ports:
      - "3000:80"
    depends_on:
      - knowledge-hub-api
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
      - ./ssl:/etc/nginx/ssl
    depends_on:
      - knowledge-hub-api
      - knowledge-hub-frontend
    restart: unless-stopped

  prometheus:
    image: prom/prometheus
    ports:
      - "9090:9090"
    volumes:
      - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
    restart: unless-stopped

  grafana:
    image: grafana/grafana
    ports:
      - "3001:3000"
    environment:
      - GF_SECURITY_ADMIN_PASSWORD=admin123
    volumes:
      - grafana-storage:/var/lib/grafana
    restart: unless-stopped

volumes:
  grafana-storage:
```

#### 8.2 ç”Ÿäº§ç¯å¢ƒé…ç½®
```bash
#!/bin/bash
# scripts/deploy.sh - ç”Ÿäº§éƒ¨ç½²è„šæœ¬

set -e

echo "ğŸš€ å¼€å§‹éƒ¨ç½²ä¼ä¸šçº§çŸ¥è¯†ä¸­å°..."

# 1. ç¯å¢ƒæ£€æŸ¥
echo "ğŸ“‹ æ£€æŸ¥éƒ¨ç½²ç¯å¢ƒ..."
if ! command -v docker &> /dev/null; then
    echo "âŒ Docker æœªå®‰è£…"
    exit 1
fi

if ! command -v docker-compose &> /dev/null; then
    echo "âŒ Docker Compose æœªå®‰è£…"
    exit 1
fi

# 2. åˆ›å»ºå¿…è¦ç›®å½•
echo "ğŸ“ åˆ›å»ºç›®å½•ç»“æ„..."
mkdir -p knowledge_bases
mkdir -p logs
mkdir -p ssl
mkdir -p backups

# 3. è®¾ç½®æƒé™
echo "ğŸ” è®¾ç½®æ–‡ä»¶æƒé™..."
chmod 755 knowledge_bases
chmod 755 logs
chmod 600 .env

# 4. æ„å»ºé•œåƒ
echo "ğŸ—ï¸ æ„å»º Docker é•œåƒ..."
docker-compose build --no-cache

# 5. å¯åŠ¨æœåŠ¡
echo "ğŸš€ å¯åŠ¨æœåŠ¡..."
docker-compose up -d

# 6. å¥åº·æ£€æŸ¥
echo "ğŸ¥ ç­‰å¾…æœåŠ¡å¯åŠ¨..."
sleep 30

if curl -f http://localhost:8000/health; then
    echo "âœ… API æœåŠ¡å¯åŠ¨æˆåŠŸ"
else
    echo "âŒ API æœåŠ¡å¯åŠ¨å¤±è´¥"
    docker-compose logs knowledge-hub-api
    exit 1
fi

if curl -f http://localhost:3000; then
    echo "âœ… å‰ç«¯æœåŠ¡å¯åŠ¨æˆåŠŸ"
else
    echo "âŒ å‰ç«¯æœåŠ¡å¯åŠ¨å¤±è´¥"
    docker-compose logs knowledge-hub-frontend
    exit 1
fi

echo "ğŸ‰ éƒ¨ç½²å®Œæˆï¼"
echo "ğŸ“Š è®¿é—®åœ°å€ï¼š"
echo "   - API æ–‡æ¡£: http://localhost:8000/docs"
echo "   - ç®¡ç†ç•Œé¢: http://localhost:3000"
echo "   - ç›‘æ§é¢æ¿: http://localhost:3001"
```

#### 8.3 å¤‡ä»½ç­–ç•¥
```bash
#!/bin/bash
# scripts/backup.sh - æ•°æ®å¤‡ä»½è„šæœ¬

BACKUP_DIR="/opt/backups/knowledge-hub"
DATE=$(date +%Y%m%d_%H%M%S)
BACKUP_NAME="knowledge_hub_backup_${DATE}"

echo "ğŸ“¦ å¼€å§‹å¤‡ä»½çŸ¥è¯†ä¸­å°æ•°æ®..."

# åˆ›å»ºå¤‡ä»½ç›®å½•
mkdir -p ${BACKUP_DIR}/${BACKUP_NAME}

# 1. å¤‡ä»½çŸ¥è¯†åº“æ•°æ®
echo "ğŸ’¾ å¤‡ä»½çŸ¥è¯†åº“æ•°æ®..."
cp -r ./knowledge_bases ${BACKUP_DIR}/${BACKUP_NAME}/

# 2. å¤‡ä»½é…ç½®æ–‡ä»¶
echo "âš™ï¸ å¤‡ä»½é…ç½®æ–‡ä»¶..."
cp .env ${BACKUP_DIR}/${BACKUP_NAME}/
cp docker-compose.yml ${BACKUP_DIR}/${BACKUP_NAME}/

# 3. å¤‡ä»½æ•°æ®åº“
echo "ğŸ—„ï¸ å¤‡ä»½å‘é‡æ•°æ®åº“..."
docker-compose exec knowledge-hub-api python -c "
import os
import shutil
from datetime import datetime

# åˆ›å»ºæ•°æ®åº“å¿«ç…§
for kb_dir in os.listdir('./knowledge_bases'):
    kb_path = f'./knowledge_bases/{kb_dir}'
    if os.path.isdir(kb_path):
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ•°æ®åº“ç‰¹å®šçš„å¤‡ä»½é€»è¾‘
        pass
"

# 4. å‹ç¼©å¤‡ä»½
echo "ğŸ—œï¸ å‹ç¼©å¤‡ä»½æ–‡ä»¶..."
cd ${BACKUP_DIR}
tar -czf ${BACKUP_NAME}.tar.gz ${BACKUP_NAME}
rm -rf ${BACKUP_NAME}

# 5. æ¸…ç†æ—§å¤‡ä»½ï¼ˆä¿ç•™æœ€è¿‘7å¤©ï¼‰
echo "ğŸ§¹ æ¸…ç†æ—§å¤‡ä»½..."
find ${BACKUP_DIR} -name "knowledge_hub_backup_*.tar.gz" -mtime +7 -delete

echo "âœ… å¤‡ä»½å®Œæˆ: ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz"

# 6. ä¸Šä¼ åˆ°äº‘å­˜å‚¨ï¼ˆå¯é€‰ï¼‰
if [ ! -z "$CLOUD_BACKUP_ENABLED" ]; then
    echo "â˜ï¸ ä¸Šä¼ åˆ°äº‘å­˜å‚¨..."
    # è¿™é‡Œæ·»åŠ äº‘å­˜å‚¨ä¸Šä¼ é€»è¾‘
    # aws s3 cp ${BACKUP_DIR}/${BACKUP_NAME}.tar.gz s3://your-backup-bucket/
fi
```

#### 8.4 ç›‘æ§å‘Šè­¦
```yaml
# monitoring/prometheus.yml
global:
  scrape_interval: 15s

scrape_configs:
  - job_name: 'knowledge-hub-api'
    static_configs:
      - targets: ['knowledge-hub-api:8001']
    metrics_path: /metrics
    scrape_interval: 5s

rule_files:
  - "alert_rules.yml"

alerting:
  alertmanagers:
    - static_configs:
        - targets:
          - alertmanager:9093
```

```yaml
# monitoring/alert_rules.yml
groups:
  - name: knowledge-hub-alerts
    rules:
      - alert: HighResponseTime
        expr: api_request_duration_seconds > 5
        for: 2m
        labels:
          severity: warning
        annotations:
          summary: "API å“åº”æ—¶é—´è¿‡é•¿"
          description: "API å“åº”æ—¶é—´è¶…è¿‡ 5 ç§’"

      - alert: HighErrorRate
        expr: rate(api_requests_total{status="error"}[5m]) > 0.1
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "API é”™è¯¯ç‡è¿‡é«˜"
          description: "API é”™è¯¯ç‡è¶…è¿‡ 10%"

      - alert: ServiceDown
        expr: up == 0
        for: 1m
        labels:
          severity: critical
        annotations:
          summary: "æœåŠ¡ä¸å¯ç”¨"
          description: "çŸ¥è¯†ä¸­å°æœåŠ¡å·²åœæ­¢"
```

### âœ… éªŒæ”¶æ ‡å‡†

| éªŒæ”¶é¡¹ | éªŒæ”¶æ ‡å‡† | éªŒè¯æ–¹æ³• |
|--------|----------|----------|
| å®¹å™¨åŒ–éƒ¨ç½² | Docker é•œåƒæ„å»ºæˆåŠŸï¼ŒæœåŠ¡æ­£å¸¸å¯åŠ¨ | éƒ¨ç½²æµ‹è¯• |
| æœåŠ¡å¯ç”¨æ€§ | æœåŠ¡å¯ç”¨æ€§ > 99.5% | å¯ç”¨æ€§ç›‘æ§ |
| æ•°æ®å¤‡ä»½ | è‡ªåŠ¨å¤‡ä»½æ­£å¸¸ï¼Œæ¢å¤æµ‹è¯•æˆåŠŸ | å¤‡ä»½æ¢å¤æµ‹è¯• |
| ç›‘æ§å‘Šè­¦ | å…³é”®æŒ‡æ ‡ç›‘æ§ï¼Œå¼‚å¸¸åŠæ—¶å‘Šè­¦ | å‘Šè­¦æµ‹è¯• |
| å®‰å…¨é…ç½® | HTTPS é…ç½®ï¼Œè®¿é—®æ§åˆ¶æ­£å¸¸ | å®‰å…¨æ£€æŸ¥ |
| æ–‡æ¡£å®Œæ•´ | éƒ¨ç½²æ–‡æ¡£ã€è¿ç»´æ‰‹å†Œå®Œæ•´ | æ–‡æ¡£æ£€æŸ¥ |

---

## ğŸ“Š é¡¹ç›®æ€»ç»“

### ğŸ¯ é¡¹ç›®æˆæœ
1. **å®Œæ•´çš„çŸ¥è¯†ä¸­å°ç³»ç»Ÿ**ï¼šæ”¯æŒå¤šçŸ¥è¯†åº“ç®¡ç†ã€æ™ºèƒ½æ–‡æ¡£å¤„ç†ã€è¯­ä¹‰æ£€ç´¢
2. **ç°ä»£åŒ–æŠ€æœ¯æ¶æ„**ï¼šåŸºäº FastAPI + React + ChromaDB + Dify
3. **ç”Ÿäº§çº§éƒ¨ç½²æ–¹æ¡ˆ**ï¼šDocker å®¹å™¨åŒ–ã€ç›‘æ§å‘Šè­¦ã€å¤‡ä»½æ¢å¤
4. **å®Œå–„çš„æµ‹è¯•ä½“ç³»**ï¼šåŠŸèƒ½æµ‹è¯•ã€æ€§èƒ½æµ‹è¯•ã€å®‰å…¨æµ‹è¯•

### ğŸ“ˆ æŠ€æœ¯æŒ‡æ ‡
- **å¤„ç†èƒ½åŠ›**ï¼šæ”¯æŒ TB çº§æ–‡æ¡£å­˜å‚¨ï¼Œåƒä¸‡çº§å‘é‡æ£€ç´¢
- **å“åº”æ€§èƒ½**ï¼šå¹³å‡æ£€ç´¢å“åº”æ—¶é—´ < 2ç§’
- **å¹¶å‘èƒ½åŠ›**ï¼šæ”¯æŒ 100+ å¹¶å‘ç”¨æˆ·
- **å¯ç”¨æ€§**ï¼šç³»ç»Ÿå¯ç”¨æ€§ > 99.5%

### ğŸ”§ è¿ç»´è¦æ±‚
- **ç¡¬ä»¶é…ç½®**ï¼š8æ ¸CPUï¼Œ32GBå†…å­˜ï¼Œ1TB SSDå­˜å‚¨
- **ç½‘ç»œè¦æ±‚**ï¼šç¨³å®šçš„äº’è”ç½‘è¿æ¥ï¼ˆOpenAI APIï¼‰
- **ç»´æŠ¤å‘¨æœŸ**ï¼šæ¯å‘¨æ•°æ®å¤‡ä»½ï¼Œæ¯æœˆç³»ç»Ÿæ›´æ–°
- **ç›‘æ§æ£€æŸ¥**ï¼š7x24å°æ—¶ç³»ç»Ÿç›‘æ§

### ğŸ“š äº¤ä»˜ç‰©æ¸…å•
1. âœ… å®Œæ•´çš„æºä»£ç 
2. âœ… éƒ¨ç½²è„šæœ¬å’Œé…ç½®æ–‡ä»¶
3. âœ… ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ
4. âœ… è¿ç»´æ“ä½œæ‰‹å†Œ
5. âœ… API æ¥å£æ–‡æ¡£
6. âœ… æµ‹è¯•æŠ¥å‘Š
7. âœ… ç›‘æ§é…ç½®

---

## ğŸ“ æŠ€æœ¯æ”¯æŒ

### ğŸ†˜ å¸¸è§é—®é¢˜
1. **æ–‡æ¡£å¤„ç†å¤±è´¥**ï¼šæ£€æŸ¥æ–‡ä»¶æ ¼å¼å’Œå¤§å°é™åˆ¶
2. **æ£€ç´¢ç»“æœä¸å‡†ç¡®**ï¼šè°ƒæ•´åˆ†å—å¤§å°å’Œç›¸ä¼¼åº¦é˜ˆå€¼
3. **ç³»ç»Ÿå“åº”æ…¢**ï¼šæ£€æŸ¥å‘é‡æ•°æ®åº“ç´¢å¼•å’Œç¡¬ä»¶èµ„æº
4. **Dify é›†æˆé—®é¢˜**ï¼šéªŒè¯ API æ¥å£å’Œå‚æ•°æ ¼å¼

### ğŸ“§ è”ç³»æ–¹å¼
- **æŠ€æœ¯æ”¯æŒ**ï¼štech-support@company.com
- **é¡¹ç›®ç»ç†**ï¼šproject-manager@company.com
- **ç´§æ€¥è”ç³»**ï¼š+86-xxx-xxxx-xxxx

---

*æœ¬æ–‡æ¡£ç‰ˆæœ¬ï¼šv1.0 | æœ€åæ›´æ–°ï¼š2024å¹´1æœˆ*